{
  "hash": "6bb14069c09391adb438f87c26d8b357",
  "result": {
    "markdown": "---\ntitle: \"ETC3250: Tutorial 4 Help Sheet\"\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Flux\nJoin this weeks flux using [this link](https://flux.qa/V3UHTY)\n\n# Exercises\n## Question 1\nIn the lecture, we used bootstrap to examine the significance of the coefficients for the second principal component from the womens' track PCA. Do this computation for PC1. The question for you to answer is: *Can we consider all of the coefficients to be equal?*\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Where to go for information\nCheck [lecture 3 slides 20 to 24](https://iml.numbat.space/week3/slides.html#/bootstrap-15). The code for this question is *very* similar to the code on slide 24.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Interpreting the plot\nWhat does the red dashed line on the plot mean? \n:::\n\n## Question 2\nThe `ggscree` function in the `mulgar` package computes PCA on multivariate standard normal samples, to learn what the largest eigenvalue might be when there the covariance between variables is 0.\n\n### Part A\nWhat is the mean and covariance matrix of a multivariate standard normal distribution?\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Where to go for information\nThis is a pretty basic question about a multivariate normal distribution, I can only suggest checking the wiki page for the [standard normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution) and the [multivariate normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution)\n:::\n\n### Part B\nSimulate a sample of 55 observations from a 7D standard multivariate normal distribution. Compute the sample mean and covariance. (Question: Why 55 observations? Why 7D?)\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Functions for simulation\nTo simulate the data you will need to use `set.seed` and `rmvnorm`. The variables should be independent so you might find the function `diag` useful in generating the variance-covariance matrix.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Functions for computation\nThere are many ways to calcualte the mean and covariance, I suggest using `apply` to calculate the mean, where `x` is your data, `MARGIN` indicates the calculation will be done over columns, and `FUN` can just be `mean` (you dont need the brackets on the function `mean`). You can use the `cov` for the variance-covariance matrix.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Nudge for 55 observations and 7D\nWe are going to compare this simulation to our results from the `track` data. Consider the dimensionality of the data (for *numeric* variables).\n:::\n\n\n### Part C\nCompute PCA on your sample, and note the variance of the first PC. How does this compare with variance of the first PC of the women's track data?\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Where to go for information\nCheck [lecture 2 slide 45](https://iml.numbat.space/week2/slides.html#/compute-pca) for code that computed a PCA.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: The function options\nRemember, your data is *simulated* from a standard normal distribution, so you shouldn't scale or center the data.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Comparison considerations\nLook at how much of the total variance of the track data is covered by PC1 and then look at how much of the total variance of your simulated data is covered by PC1. Is it a valid check to just compare these two numbers? What would be required to make sure PC1 of the track data does not just *happen* to be different to the PC1 of our simulated data for this particular draw?\n:::\n\n## Question 3\nPermutation samples is used to significance assess relationships and importance of variables. Here we will use it to assess the strength of a non-linear relationship. \n\n### Part A\nGenerate a sample of data that has a strong non-linear relationship but no correlation, as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(908)\nn <- 205\ndf <- tibble(x1 = runif(n)-0.5, x2 = x1^2 + rnorm(n)*0.01)\n```\n:::\n\n\nand then use permutation to generate another 19 plots where `x1` is permuted. You can do this with the `nullabor` package as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(912)\ndf_l <- lineup(null_permute('x1'), df)\n```\n:::\n\n\nand make all 20 plots as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df_l, aes(x=x1, y=x2)) + \n  geom_point() + \n  facet_wrap(~.sample)\n```\n:::\n\n\nIs the data plot recognisably different from the plots of permuted data?\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Where to go for information\nCheck [lecture 3 slide 26](https://iml.numbat.space/week3/slides.html#/permutation-13) for information on permutation.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Things to consider\nConsider how permutation works and how the other 19 plots were generated. What kind of relationship should become invisible when variables are permuted. How does that translate to being able to identify your data in this lineup plot? What does that mean about your data?\n:::\n\n### Part B\nRepeat this with a sample simulated with no relationship between the two variables. Can the data be distinguished from the permuted data?\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Functions for simulation\nTo simulate the data you can basically use the code above except instead of setting `x2 = x1^2 + rnorm(n)*0.01` you can remove the dependence by setting `x2 = rnorm(n)*0.1`.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Things to consider\nConsiderations are identical to those provided in the hint for Part A.\n:::\n\n## Question 4\nFor the penguins data, compute 5-fold cross-validation sets, stratified by species. \n\n### Part A\nList the observations in each sample, so that you can see there is no overlap. \n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Where to go for information\nCheck [lecture 3 slide 13 to 16](https://iml.numbat.space/week3/slides.html#/k-fold-cross-validation-14) for information about k-fold cross validation (in this case, k=5). You can find *very* similar code for this section on slide 14\n:::\n\n### Part B\nMake a scatterplot matrix for each fold, coloured by species. Do the samples look similar?\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Consideration for making data\nEach of the lines in Part A should line up with the *index* of one segmentation of the data. Look at the code and work out what it is doing. \n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: More consideration for making data\nWhat is the code required for `p_tidy[row_index,]` to work? Remember, the training set is the remaining data after the test set is removed, and you can remove data through an index by using `p_tidy[-test_index,]`\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: Functions and code for scatterplot matrix\nLook at `ggscatmat` for the scatterplot matrix , you can use `columns` to specify the numeric variables and `color` (American spelling) for the colouring of the points.\n\nYou can just copy and paste this for each scatterplot matrix you need to make.\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n## Hint: More consideration for answering questions\nDoes your data vary between subsets? Is this variation natural?\n:::\n\n## Question 5\nWhat was the easiest part of this tutorial to understand, and what was the hardest? ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}