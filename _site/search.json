[
  {
    "objectID": "posts/FlexiblevsInflexible/index.html",
    "href": "posts/FlexiblevsInflexible/index.html",
    "title": "4 Things We Can Learn About Conspiracy Theories and Model Flexibility",
    "section": "",
    "text": "A few years ago my mum became very susceptible to suggestion, and made friends with a guy who was really good at speaking about nonsense with the authority to make it sound true. Listening to him sent her down a conspiracy theory rabbit hole, of which I had to experience second hand. Our interactions ended up boiling down to mum sending me a 20 minutes Youtube video about aliens building the pyramids, then I would wait the appropriate amount of time and send a text that said “Wow, what an interesting perspective”. I always hoped it would end the conversation and we could talk about something else, but instead it tended to inspire a paragraph long text rant about how the government was hiding free energy from us, and an 11 year old Texan genius had discovered the plot. When I think of flexible methods, I often have flash backs to that period of my life. Not because high degree polynomials were built by ancient aliens or an 11 year old genius but because we can use the pitfalls of conspiracy theories to understand the difference between flexible and inflexible methods.\n\n\nI think of flexibility as the trade off in capturing the “local” and “global” trends in our data. An inflexible model will capture the global trend of the data, but any relationship between our variables is lost. If we instead choose a flexible model, we are focusing on the local trends and giving our model a better chance at capturing variable relationships, at risk to overfit to the sample. Flexibility has key interactions with 4 other elements of our model: the sample size, dimensionality, assumptions about the function, and irreducible error."
  },
  {
    "objectID": "posts/FlexiblevsInflexible/index.html#outrageous-claims-need-outrageous-evidence",
    "href": "posts/FlexiblevsInflexible/index.html#outrageous-claims-need-outrageous-evidence",
    "title": "4 Things We Can Learn About Conspiracy Theories and Model Flexibility",
    "section": "1: Outrageous Claims Need Outrageous Evidence",
    "text": "1: Outrageous Claims Need Outrageous Evidence\nMy mother is a “bit eccentric” to put it mildly. In the last few months, to only name a few things, she has bought a fire truck to start mud-crabbing (pictured below), bought some goats because the garden is a pain to manage, and turned the pool into a “fish Club Med” where she collects wildlife from the local creek and feeds them McDonalds for breakfast. From expulsions to arrest warrants, to the man she drank goon with at the beach who now lives in our house, the stories are endless. Despite this, never in my life had I ever been called a liar for telling them (the first time was at university orientation). People at my school had grown used to it, they had met my family and heard years worth of stories so I had a wealth of evidence to normalise my claims. Strangers didn’t have that, and so they didn’t believe my outrageous (completely true) tales. Similarly in statistics, if we want a complicated model we will need a large sample size to back it up.\n  \n\nWhy Flexible Models Need a Bigger Sample\nIn general, the larger your sample size, the more likely it is you have captured the “true relationship”. If you are increasing the number of parameters to estimate (not literally for non-parametric models but the idea carries on) without increasing the sample size, we are in effect decreasing the “sample size” for each of the estimated values, and thus decreasing the reliability of our model. Placing more weight on all the observations in calculating our estimates, means we are increasing the influence of outliers and unrepresentative samples. We can either have observations contributing to a large area but averaged over many observations, or over a small area where our estimates are averages over fewer observations. For example, if we have 10 observations and predict using the average, each observation contributes to 1/10th of the prediction, if we use 1-Nearest Neighbour, each prediction is only backed up by a single observation (illustrated below). Highly flexible models can be, and sometimes are, the appropriate choice to model a relationship, we just need a large sample to justify it. Outrageous claims need outrageous evidence."
  },
  {
    "objectID": "posts/FlexiblevsInflexible/index.html#the-internet---deliverer-of-facts-and-local-cult-meet-ups",
    "href": "posts/FlexiblevsInflexible/index.html#the-internet---deliverer-of-facts-and-local-cult-meet-ups",
    "title": "4 Things We Can Learn About Conspiracy Theories and Model Flexibility",
    "section": "2: The Internet - Deliverer of Facts and Local Cult Meet Ups",
    "text": "2: The Internet - Deliverer of Facts and Local Cult Meet Ups\nThe introduction of the internet was the age of new information. Conspiracy theories were on their way out, now anyone can use their phone and find the facts in seconds. Or can they? What I unfortunately discovered when mum got involved with conspiracy theories, is that for every website with legitimate information, there are 50 that don’t. The sheer vastness of the internet means that whenever we expand our search for hidden truth, we are just as likely to discover falsities. This is a useful illustration in dimensionality.\n\nFlexible Models Are Hurt More By Additional Parameters\nDimensionality interacts with the flexible vs inflexible models in two ways. The first is that in some occasions adding dimensions can literally be seen as making the model more flexible. Think of adding a squared variable to a linear regression to make it quadratic, we have made the model more flexible by adding a dimension. The second way it interacts with our models, is by increasing the distance between observations, and thus the amount of input space each observations needs to be used for. To get technical, each additional parameter makes the area each observation is responsible for increase exponentially. Just like how increasing flexibility increases the “weight” of observations by localising their impact on the model, dimensionality makes the total “area” bigger, and so it does a similar thing. Sometimes the relationship between our variables needs to be modeled with a highly flexible model, and so we need to keep this interaction between flexibility and dimensionality in mind so the variance doesn’t get out of control."
  },
  {
    "objectID": "posts/FlexiblevsInflexible/index.html#capitalism---the-gateway-conspiracy-to-lizard-people",
    "href": "posts/FlexiblevsInflexible/index.html#capitalism---the-gateway-conspiracy-to-lizard-people",
    "title": "4 Things We Can Learn About Conspiracy Theories and Model Flexibility",
    "section": "3: Capitalism - The Gateway Conspiracy to Lizard People",
    "text": "3: Capitalism - The Gateway Conspiracy to Lizard People\nNobody suddenly wakes up in the morning, looks in the mirror and says to themselves “Yes, today is the day. Today is the day I start believing in the lizard overlords.” I believe the process is more nuanced than that. Just like the “SayNoToPeerPressure” acting troupe who’s dreams I got to watch die in the comfort of my high school gym, I’m about to push the idea of gateways. From my personal experience, the process of becoming involved in conspiracies looks a little something like this:\n  \nMy point is that ideas that hinge on something already well established in society are easier to swallow than those that aren’t. That is not to say entirely new theories must be wrong, but rather that they are harder for people to immediately understand and they are also more likely to be too out there for the general population to get on board with. I think of parametric and non-parametric models in a very similar way to how people think of capitalism vs lizard people conspiracy theories.\n\nNon-Parametric Models Are Usually More Flexible, But Not Always\nParametric models construct our function by assuming its type, and then estimating the best model within this range. Non-parametric models do not make any assumptions about our model’s form, but rather try to fit to the general shape of the data. Parametric and Non-parametric does not directly translate to flexibility; they both have the potential to produce a very flexible or inflexible fit. For example, a constant polynomial and a K-NN model where K=N would both predict the average response (the most inflexible model we can get). Rather, just like dimensionality, non-parametric models can fall into the same pitfalls as flexibility, and so the limits of our dataset should be kept in mind. By their nature, non-parametric models are more susceptible to variance from changes in the sample, as the sample is the only thing the model is using to make its predictions. Therefore, they are more likely to overfitting than parametric models and are usually more difficult to interpret. These features mean that in general non-parametric models are more flexible, simply by their nature, however they are still have the potential to be inflexible."
  },
  {
    "objectID": "posts/FlexiblevsInflexible/index.html#there-are-always-going-to-be-loonies-on-the-internet",
    "href": "posts/FlexiblevsInflexible/index.html#there-are-always-going-to-be-loonies-on-the-internet",
    "title": "4 Things We Can Learn About Conspiracy Theories and Model Flexibility",
    "section": "4: There are Always Going to Be Loonies on the Internet",
    "text": "4: There are Always Going to Be Loonies on the Internet\nWe can all spend our entire lives trying to convince everyone on the internet that they are wrong, but at the end of the day, we live in a complicated world, with complicated people, and there are always going to be loonies on the internet. Rather than dreaming of a world where everyone knows everything all the time, the system should just be to manage the chaos. The important life skill to learn isn’t that everyone needs to be corrected, and to focus on the nutters, but rather enjoy the fact that the majority get most things right, most of the time. Socrates might disagree with my idea on majority votes but you win some, you lose some.\n\nYou Will Always Have Irreducible Error and It’s Size Matters\nObviously we can never have a perfect prediction since we are working with random variables. We can make our models more flexible to try and account for as much of the error as we can, but if we do, we might end up missing the underlying system entirely. No matter how flexible our model is, we will never have perfection thanks to our irreducible error (an attempt at making one is illustrated below). The interaction between flexibility and irreducible error comes from its size. A large irreducible error means the general shape change more drastically between samples, while a small one means our samples will remain consistent. Just like dimensionality, assumptions about our model, and sample size, this is just something that needs to be kept in mind as it has a strong interaction with the flexibility of our model, and the error from variance."
  },
  {
    "objectID": "posts/FlexiblevsInflexible/index.html#to-conclude",
    "href": "posts/FlexiblevsInflexible/index.html#to-conclude",
    "title": "4 Things We Can Learn About Conspiracy Theories and Model Flexibility",
    "section": "To Conclude",
    "text": "To Conclude\nDon’t let your mum hang out with weirdos, and treat conspiracy theories and overly complicated models with scepticism."
  },
  {
    "objectID": "posts/pca/index.html",
    "href": "posts/pca/index.html",
    "title": "Using PCA to Bully My Housemates (Specifically Tom)",
    "section": "",
    "text": "I recently moved into a share house with three of my friends, and while we generally get along pretty well, I would be lying if I said I never fantasised about burning the place down with them all in it. Today, after I woke up to the dishwasher run with less that half a load, I made this passive aggressive drawing and sent it to the group chat. I have great conflict resolution skills.\n\nThe three people I live with all know me, but none of them know each other, and so as the central housemate, I have often wondered if this clear social dynamic appears in our communication (such as messenger data). This is something that could be easily found through a combination of a principal component analysis (PCA) and violating my housemates privacy. Both of which are enjoyable and straightforward. When PCA was introduced to me in uni, I struggled a bit to understand the plots. So, while I’m violating my housemates privacy, I’m also going to go over the ‘gist’ of PCA and unravel the plots that come with it."
  },
  {
    "objectID": "posts/pca/index.html#what-is-pca",
    "href": "posts/pca/index.html#what-is-pca",
    "title": "Using PCA to Bully My Housemates (Specifically Tom)",
    "section": "What Is PCA?",
    "text": "What Is PCA?\n\nThe Theory\nI would have just jumped into a nice example of understanding the plots, but for the sake of completeness I will explain how PCA works. The idea of PCA is to summarise the “information” of a dataset into its principal components (PCs), and then interpret those instead. These PCs are built to be linear combinations of our variables in their most “interesting” direction. Where “interesting” means the direction of most variance. Think of a linear regression but instead of projecting our results onto a line that uses x to capture as much information as possible about y, we are using both variables trying to capture as much information as possible in the x and y direction that has the most variance. Explaining this with words is a bit difficult, so I have drawn a visualisation of this below.\n\nFollowing on from this illustration, an easy way to understand principal components is to shift your current understanding of linear regression (I’m assuming you have some current understanding of linear regression). The variable loadings are similar to variable weights in the regression line. We interpret the loadings as “how much that variable contributes to the PC”. Our prediction for a value in linear regression is its projection onto the regression line (with the error shown in the above illustration in red). When working with PCA, our observation’s values are their projection onto the PC line. It is important to note that the red lines in in the PCA drawing is not error, but rather the “remaining” value that will then be used to build the second PC. This is just a quick overview of what these values represent (if you want something more technical look at a textbook or something, this isn’t a maths class). Now, lets take a quick look at the data we are working with.\n\n\nSharehouse Chat Data\nTo put some faces (albeit badly drawn cartoon ones) to names, here is an illustration of my housemates. I have also added a fun fact (checked by them after a large amount of “is this what you think of me” arguing) to help give an idea of their personalities. I’m basically introducing them like a 2000’s MTV dating show, but hopefully this will age better and be less racist/homophobic/sexist.\n\nThe data we are going to be working with is the Facebook messenger records of the sharehouse group chat. When I downloaded it, there were about about 6000 messages, over 3000 of which were sent by me. I was originally interested in analysing all my messenger data but seeing that number stung enough for me to pretend I didn’t download my other chat files. I’d rather live in ignorance than face the fact that I feel the need to update all my friends on everything I do.\nSo, after a bit of cleaning (removing punctuation, removing stop words, breaking observations up into single words, counting the frequency by person, diving each value by total number of words said in the chat by that person) I have a my dataframe. Each variable is someone who lives in the house, each observation is a word, and the values are how many times that word was said relative to the number of words that person has sent in total. So my value for the word “tom” is how many times I have said “tom” as a fraction of all the words I have sent to the chat. I could skip making the values a “relative” frequency, but then our PCA would likely just tell us that I am absolutely incapable of shutting up, rather than what words typify each speaker. Below is a glimpse at the data that we will run through the PCA.\n\n\n\n\n\nword\nHarriet\nZac\nEm\nTom\n\n\n\n\ntom\n0.0178\n0.0171\n0.0107\n0.0081\n\n\nhouse\n0.0165\n0.0155\n0.0135\n0.0129\n\n\nzac\n0.0149\n0.0078\n0.0088\n0.0064\n\n\nem\n0.0127\n0.0194\n0.0000\n0.0081\n\n\nyeah\n0.0090\n0.0248\n0.0334\n0.0274\n\n\ntime\n0.0091\n0.0148\n0.0102\n0.0113\n\n\n2\n0.0080\n0.0054\n0.0088\n0.0064\n\n\nshit\n0.0080\n0.0062\n0.0060\n0.0000\n\n\nstuff\n0.0074\n0.0023\n0.0037\n0.0064\n\n\npeople\n0.0067\n0.0078\n0.0042\n0.0081\n\n\n\n\n\nNow that we have some data, lets discuss how we interpret the loadings."
  },
  {
    "objectID": "posts/pca/index.html#the-loadings",
    "href": "posts/pca/index.html#the-loadings",
    "title": "Using PCA to Bully My Housemates (Specifically Tom)",
    "section": "The Loadings",
    "text": "The Loadings\n\nThe Theory\nThe loadings have two things about their interpretation that make them a bit tricky to understand: 1. We are plotting what would be on the axis of the plot in our typical scatter plot (the variables) as observations 2. We are using these “observations” to understand the axis (our PCs). I have drawn this relationship below for additional clarity.\n\nNote: these aren’t the actual loading values of PC1 and PC2 from the example below, this is just an illustration\nTo make matters even more complicated, we usually plot our PCA on a biplot with both loadings and observations. We will make and interpret this plot at the end, but since this is an “introduction to understanding PCA plots” we are going to start with only plotting the loadings, and work our way to the biplot.\nTo interpret our loadings we need to keep three things in mind: 1. The principal components summarise information in descending order of importance. This means that each PC will represent a more overt trend in the data than the PC that follow it. 2. The direction of the PCs is the most important take away. If all your loadings are in the same direction then this PC is analysing the ways in which all your variables are the same. If they move in opposite directions, the PC is identifying a juxtaposition. The actual direction of the loading (positive or negative) doesn’t matter too much outside of the loading’s direction relative to the others. This might seem a bit confusing, it will make more sense once we look at the first loading in the example below. 3. The magnitude of the loading is the least important part. If you start getting so detailed that you are thinking deeply about the magnitude, you are likely overcomplicating the problem for yourself. Just pay attention to the loadings that are significantly different from 0 (I marked these using a red line in the example).You can find your significance line as \\(\\frac1{\\sqrt{p}}\\) where p is the number of variables in your PCA (in the example it’s 4). As with anything, this will be easier to understand with an example, so lets just look at what the sharehouse PCA produced.\n\n\nSharehouse Chat Loadings\nTo start off with, we need to use the loadings to interpret the PCs. The first two PC’s capture most of the variance, and so typically we focus on those two, however since we only have 4 variables (and so 4 possible PCs) I might as well do them all.\n\n\n\n\nKeeping in mind what we covered above, we can analyse these plots. As a side note, the order of names (the x-axis of these plots) are arbitrary and organised only to make the words readable, so we only need to interpret the y-axis (the PC loadings). To begin lets start with PC1, the most important PC. Since all the loadings are negative, any persons use of a word will give that word a negative value on the first PC. To put it simply, words we say a lot as a combined group will have a large negative score, and words that we never say will sit around 0. There wont be any positive values on PC1 because each word’s value is the \\(Housemate'sPCLoading\\times{Housemate'sWordFrequency}\\), summed up for all 4 of us. So since none of the words will have a negative frequency that could cancel out the negative loadings word’s wont have positive value on PC1. Here are the 4 loading interpreted in their positive direction:\nPC1: Words None of us say - The overarching ways in which the four of us are similar thanks to generation and circumstances (of living together). This PC will likely contain words people who live together and people our age use. PC2: Words Tom never says - Out of all of us, the most distinct speaker of the group is Tom. PC3: Words that Em uses - Em is the next most distinct. PC4: Words Differentiate Zac and I - Zac and I were on the same side of all the other loadings, and so once all the other sources of variance have been dealt with, this is all that is left. It makes sense, as we are the oldest and closest friends, so our speech is the most similar.\nInterestingly, the loadings captured the underlying dynamics of the group pretty well. Since the PCs are organised such that they explain decreasing variance, this tells us that the overarching patterns of speech between the 4 of us (PC1) is more salient than the difference between Tom’s and the rest of us (PC2) and so on. I have drawn the social circles identified by the PC loadings below, both as an illustration of the analysis, and to personally attack Tom. Using this understanding of our new variables (the PCs) we can interpret our observations, just as we would normal variables.\n\nAnother note I want to make is that I could have set up this data frame so that the words were the variables instead of the sender (I actually did this the first time without thinking). The main problem with this comes in the analysis. If the variables are words and the largest loadings come from “yeah”, “tom” and “house”, it is hard to understand how these words are similar, and how they are different. That analysis is much easier to do on people, because I have prior understanding of the context of those variables."
  },
  {
    "objectID": "posts/pca/index.html#understanding-observations",
    "href": "posts/pca/index.html#understanding-observations",
    "title": "Using PCA to Bully My Housemates (Specifically Tom)",
    "section": "Understanding Observations",
    "text": "Understanding Observations\n\nThe Theory\nUnderstanding the observations is very straight forward once you have the PC interpretations. Usually when analysing our data, the process looks something like this:\n Variable Meaning -> Understand Observations \nFor example, a low time in a 100m sprint can be interpreted as fast. Obviously, PC1 does not have an inherent meaning to us in the same way that the time for a 100m sprint does, but that is what the loading interpretations was for. The process for understanding the data plots in PCA is:\n Construct PCs -> Use loadings to find PC meaning -> Understand Observations \nSo from this we can see that the interpretation of data in PCA vs regular analysis is almost the same, there is just an extra step (which we have already done in our example) that can complicate it a bit. Now that we understand how to interpret the observations in the PCA, let’s apply this to the sharehouse chat data to finish off the analysis.\n\n\nSharehouse Chat Observations\n\n\n\n\nHow do we interpret these plots? Well we need to use our interpretations of the loadings to understand what our axis represent. Since we established that PC1 represents words we all use, the distance below the line indicates how frequently the word is used between us all. For example, “yeah” and “house” are the most used words across the chat. This makes sense as we are pretty informal and all live together. We can do the same thing for PC2, which identified the ways Tom speaks differently. He uses “nbn” a lot because he is the one who set up the internet. “Tom” is a common word for Zac and I, not only because we love to bully our housemate Tom, but because we also have a few mutual friends (and some not friends) called Tom that we talk about in the chat.\nI sent all these plots to the group (I like to keep them informed) and Em said “I’m apparently the only one who laughs in this chat”. Now this brings up an interesting point in how this analysis was run, and it shows how PCA can bring out some patterns that may not be immediately recognisable in the data.\nThe data cleaning will correct for things like capitalisation (so here Here and HERE are all the same word) but if the words differ by letters (here and herez) thanks to typos or spelling mistakes, they are registered as different words. This creates a problem for registering words that I typically use, since: 1) I’m an absolutely abysmal speller and rarely spell a word the same way twice; and 2) I type laugher according to how funny I think something is (haha vs hahahahahahaha) This means, someone like Zac who almost always laughs in the same way with “lmfao”, or Em with “hahaha” and “hahahaha’, have all their chat laughter collected into one observation. Looking through the records I laugh to a similar degree, but almost all of them are recorded as unique words in the frequency count, and thus don’t make it to the analysis. Tom just never laughs at anything."
  },
  {
    "objectID": "posts/pca/index.html#biplot-putting-it-all-together",
    "href": "posts/pca/index.html#biplot-putting-it-all-together",
    "title": "Using PCA to Bully My Housemates (Specifically Tom)",
    "section": "Biplot: Putting It All Together",
    "text": "Biplot: Putting It All Together\nNow these plots only show one principal component each, and also don’t have the loadings on them. I started by separating the elements of the plot, but making several plots when the information could be conveyed with a single plot is tiresome. Now that we understand each of the components by themselves, lets make a biplot to show how this information is usually conveyed all together.\n\n\n\n\nTypically we use the first two principal components when we build the biplot because they contain the most variance, and thus the most information. This final plot is usually how a PCA is presented to us, with the observations and loadings plotted together and each axis representing a principal component. While the plot looks a little different now, the interpretations are still the same, and as a matter of fact understanding the observation is a little easier than before. Since we have the loadings on the plot too, we no longer need to hold the interpretation of the PCs in our mind to understand the observations. On the x axis, the further to the left a word is, the more we all use it, on the y-axis, the further down an observation is, the more Tom specifically uses it. Now we can make analysis of our observations using this combined information, rather than separating it. For example, looking at the biplot we can see that while “tom” is used a LOT in the chat overall, that is largely due to Zac and I, rather than Tom saying his own name.\nThe biplot allows us to summarise most of the information covered in this post in a single figure, and knowing how to interpret it makes your life much easier. That being said, if you have a lot of loadings you might still need to separate the plots as a biplot can get messy and crowded when we have too many."
  },
  {
    "objectID": "posts/pca/index.html#conclusion",
    "href": "posts/pca/index.html#conclusion",
    "title": "Using PCA to Bully My Housemates (Specifically Tom)",
    "section": "Conclusion",
    "text": "Conclusion\nWhile PCA plots can seem confusing at first, once you break them down into their components, they are pretty straight forward to understand. Also Zac said I need to include his twitter handle which is @zaccheus_e so I can direct people to an illiterate and poorly structured rebuttal."
  },
  {
    "objectID": "posts/bootstrapCI/index.html",
    "href": "posts/bootstrapCI/index.html",
    "title": "My Idiot Brain Ruined My School’s NAPLAN average, But Bootstrapping Could Have Saved it",
    "section": "",
    "text": "Creative writing has never been my forte, however, no attempt was worse than my 5th Grade NAPLAN test. My score was so poor I suspect the examiners were concerned I would never turn out to be a functional adult capable of basic literacy. Unfortunately for my school, typical sample metrics like averages and standard deviation can be heavily skewed by a single student who thinks spelling is a waste of time, and writing out the plot of last nights fever dream makes for good literature. This issue for my school could have been fixed if, rather than using typical sample statistics, the NAPLAN examiners employed bootstrapping.\n\nWhat is Bootstrapping?\nBootstrapping is a versatile tool that can be used to find estimates of variance when they aren’t easily found analytically. This could be due to it being an uncommon statistic, or due to sampling from a strange distribution. Essentially, we sample from our dataset, with replacement, to make a new dataset that is the same size, as illustrated below. It works similarly to simulation, and understanding where simulation and bootstrapping diverge make the limitations and applications easier to understand.\n\n\n\n\n\nSimulation and Bootstrapping\nFor those of us in statistics, simulations are a useful tool and not just something your pseudo-intellectual friend brings up after watching a Neil deGrasse Tyson documentary. When an analytical solution is unavailable to us (or if the math isn’t worth it) a simple estimate with a simulation can go a long way. Bootstrapping is closely related to simulation and outlining their similarities and differences makes it easier to understand the main ideas of bootstrapping.\nIf our data creation process was forced into the context of a family tree, simulated data would be the parent of the bootstrap sample. A simulation of a simulation if you will. If we illustrate this, we need to start with our random process, the true data generating process that we assumes exists but can never truly know. This process creates our data (or simulation if we are working artificially) which turn creates our bootstrapped samples.\n\n\n\nFor a simulation, each observation is an independent trial generated from the true data generating process. As we generate more and more of these simulations, their behaviour (and thus statistics) will on average approach the true distribution. So we can average them to make estimates for the underlying process.\n\n\n\nBootstrapping is a little different but follows a similar idea. Instead of the source being a generating function, it is one of the simulations. To replicate the effect of independent trials, we sample with replacement from the data and generate a new dataset. This new dataset is essentially a simulation of that original simulation. Much in the way that simulation takes one step back and looks at the true relationship, bootstrapping takes one step back and can be used to estimate statistics in the data.\n\n\n\n\n\nThe Limitations of Bootstrapping\nBootstrapping is a simple trick that allows us to fabricate new samples from our data to understand the variance of our statistic. This process, however, only allows us to take one step back in the chain of simulation. Bootstrapping from a dataset only estimates the statistics from that dataset, going further and making estimates for the true underlying process would require additional natural data. More data through bootstrapping won’t cut it.\nHere you may throw up your hands at the futility of trying to estimate anything as you stare in the mirror and ask your refection why you chose statistics instead of following your childhood dream of running an ice cream shop. Lucky for us, data analysis evolved from mathematics with the key goal to estimate the impossible, using an age old technique called lowering our standards.\nReally the underlying data generating process is more of an idea than something you can touch. So the limitation isn’t that limiting. If you just change your goal to understanding the dataset, an endeavour that is more practical really, bootstrapping works just fine.\n\n\nEstimating Confidence Intervals\nHow can we use bootstrapping for something helpful, like estimating a confidence interval? Well, the process is surprisingly simple, and we have already done most of the work. After you have taken your bootstrapped samples and estimated your statistic of interest for each, you simply order them and take the values that lie on the percentile boundary. That’s it. You want a 90% confidence interval with 100 bootstrap samples? Well take the 100 bootstrapped sample statistics, order them, and the value range between the 5th and 95th observations is your 90% confidence interval. It’s that simple.\n\n\n\n\n\nUsing my Poor English Grades For an Example\nWhen I was a child, I did so badly in the year 5 NAPLAN writing that the school insisted on moving me from advanced, to remedial English. My paper, as my teacher explained to my mother, seemed to be a summarised plot of the anime “Full Metal Alchemist”, written in one sentence that spanned several pages, with no punctuation, and mostly incorrectly spelled words. This was coming in hot after my older brother, three years my senior, had also failed a large portion of his NAPLAN tests because he was “too busy thinking about his imaginary Pokémon” to answer the questions, and randomly filled in bubbles in the last 5 minutes. We are going to use my attack on the advanced class average as an example in bootstrapping.\nLets say the advanced class and the regular class both have 5 students each (they didn’t but drawing those stick people gets tiring). All these students have an associated test score, and the staff want to compare the two classes. Simply using the mean of the two classes will cause issues, since the mean is susceptible to be skewed by outliers, and my idiot paper is a big outlier.\n\n\n\n\n\n\nThe bootstrap comes to our aid here in two ways. First of all, finding the average of several bootstrapped samples can help with eliminating problems caused out outliers. In this case, since the sample size is only 5, that doesn’t quite work here, but with a sample size that isn’t limited by how many stick figures I can draw, it’s quite useful. Instead, we want to use the median to compare the two classes, since outliers don’t affect it. The problem with the median, is that unlike the mean, it doesn’t have any nice theorems that give us its variance. This is the second way bootstrapping can come to our aid, to create confidence intervals for our median.\nTo create each bootstrap sample, we will randomly sample our data with replacement 5 times. An example of what one of the bootstrapped samples would look like is shown below.\n\n\n\nIf we repeat this 100 times, we get 100 medians, which we can sort in ascending order, and get a confidence interval for the median. Using this method, we have managed to save my classmates from being dragged down by my terrible paper.\n\n\n\n\n\nA Final Note\nIn the end, bootstrapping is a useful and versatile tool, that can help us when we are using less conventional statistics or have an unconventional distribution. Unlike simulation, bootstrapping isn’t generating new data, but rather creating a new simulation from our current data, so the conclusions we can draw aren’t limitless. One place it could be useful, however, is saving the people around me from my moments of stupidity that drag them down to my nearly illiterate level.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "posts/PopulationvsSample/index.html",
    "href": "posts/PopulationvsSample/index.html",
    "title": "Criminal Statistics in Baby Murder Court Cases",
    "section": "",
    "text": "I have had my fair share of run-ins with the legal system. Probably more than someone my age should have. Here are my three favourite interactions so far:\n\nA guy on acid threw a rock through my window at a music festival and they found him an hour later running around a field and screaming “I’m the richest man in the world”\nMy landlord snapped when he was at our house doing maintenance, jumped the fence and assaulted our neighbour who now has a restraining order against him\nMy parents tried to sue my high school after I was expelled for giving a speech about the private school system at a public speaking competition\n\nThe point here isn’t to talk about my interactions with the law, but rather to show that I have seen a police officer at least once, so I’m definitely well versed enough to critique our complicated legal system. So well versed in fact, I’ll do it while explaining the difference between samples and populations. For the sake of keeping it interesting, lets narrow this weeks side topic even even further to “court cases where the mother was convicted of killing her infant baby thanks to faulty science and circumstantial evidence”. At this point I could be spinning a wheel to decide these side stories with how much they actually have in common with machine learning."
  },
  {
    "objectID": "posts/PopulationvsSample/index.html#how-bad-maths-ruined-sally-clarks-life",
    "href": "posts/PopulationvsSample/index.html#how-bad-maths-ruined-sally-clarks-life",
    "title": "Criminal Statistics in Baby Murder Court Cases",
    "section": "How Bad Maths Ruined Sally Clark’s Life",
    "text": "How Bad Maths Ruined Sally Clark’s Life\nIn 1999 Sally Clark was trialled, and convicted for the murder of her two infant sons. Both had died from Sudden Infant Death Syndrome (SIDS), which is basically the medical term for “I don’t know man, sometimes babies just die”. A key piece of evidence for her conviction was the testament of Professor Roy Meadows, who claimed that the chance of two children of an affluent family dying from SIDS was 1 in 73 million. He found this number by squaring the probability that a child in similar circumstances died from SIDS (1 in 8500). There are two major problems with Meadows Law; the prosecutors fallacy and assumption of independence. Meadows law confuses the probability of “cause given effect” with “effect given cause”. The probability that a single baby dies from SIDS is 1 in 85000, but that wasn’t the case here. Sally Clarks kids had all ready died, and so setting the population to be every baby, alive or dead, would be misleading and incorrect. The population for finding out how likely both kids were to have died from SIDS needed to be compared to other causes of death, by using babies that had died as the population. Meadows law is an excellent example in the importance of a correctly defined population and sample."
  },
  {
    "objectID": "posts/PopulationvsSample/index.html#defining-the-population-and-sample",
    "href": "posts/PopulationvsSample/index.html#defining-the-population-and-sample",
    "title": "Criminal Statistics in Baby Murder Court Cases",
    "section": "Defining The Population and Sample",
    "text": "Defining The Population and Sample\nSamples and Populations are two peas in the statistical pod and their relationship is quite simple. Populations define the group we want to understand, and a sample is a random subset of that group which we use it to understand the population. Actually that explanation is a little circular, let me draw it to make it easier to see.\n  \nOk that drawing is literally a cycle. Its just as bad as the explanation. I need to fire my illustrator.\n  \nAnyway, this circular explanation, while slightly annoying, is the essence of the population and sample relationship, and why getting a solid grasp on the difference is difficult. Rather than constantly trying to think about populations and samples in this cyclical way, let’s instead think about the implied population from any given sample. Then we can consider what happens when we try to deviate from it."
  },
  {
    "objectID": "posts/PopulationvsSample/index.html#lindy-chamberlain-aka-the-dingos-got-my-baby-lady",
    "href": "posts/PopulationvsSample/index.html#lindy-chamberlain-aka-the-dingos-got-my-baby-lady",
    "title": "Criminal Statistics in Baby Murder Court Cases",
    "section": "Lindy Chamberlain AKA The “Dingo’s Got My Baby” Lady",
    "text": "Lindy Chamberlain AKA The “Dingo’s Got My Baby” Lady\nThe “Dingo Ate My Baby” court case is a great example of poor assumptions left right and centre. From a dodgy test for blood to calling in dingo experts from London to the media circus inspiring guilt, the whole thing is an embarrassment to the Australian court system. One of the key arguments against Lindy Chamberlain was that “dingos just don’t attack people”. There was no recorded evidence of a dingo ever severely attacking a person, let alone killing one, and they typically avoided conflict with humans. The story goes, while camping at Uluru, a dingo got into Lindy’s tent, grabbed her baby (Azaria), and ran off into the night. This event had multiple witnesses and logistically made sense. The prosecution argued, that in the 15 minutes Lindy was away from the campsite, she cut her baby’s throat in the boot of her car with nail scissors, stashed the body in a camera bag, hid her baby’s jumpsuit in a dingo lair, used a vial of blood to sprinkle some around the tent, and then went back to the group at the campsite. All in 15 minutes and without getting a drop of blood on her. Seem like an insane story? Well just like Sarah Clark, Lindy Chamberlain was found guilty. During the trial the prosecution had two main points that directed the jury towards a guilty verdict (ignoring the media’s influence) which were both a huge miscarriage of justice and a great case study in a poor jump from a sample to a population."
  },
  {
    "objectID": "posts/PopulationvsSample/index.html#prosecution-point-1",
    "href": "posts/PopulationvsSample/index.html#prosecution-point-1",
    "title": "Criminal Statistics in Baby Murder Court Cases",
    "section": "Prosecution Point 1",
    "text": "Prosecution Point 1\n\n“Dingos Just Don’t Attack People, and If They Did, It Wouldn’t Look Like This”\nThe original coronial inquest for the dingo ate my baby case agreed that Azaria was probably taken by dingos. It wasn’t a far fetched idea that the largest predatory mammal in Australia would eat a small weak animal left alone. Dingos were always wild animals, but they had never attacked people before, and this is a large reason that the criminal trial made it’s way to the courts. The Northern Territory police were dissatisfied with the original coronial inquest and had a second inquest done by sending photos of the baby’s jumpsuit to someone in London who said that the incision around the neck of the jumpsuit implied a cut throat. The jumpsuit of Azaria was mostly in tact except for the top button of the jumpsuit. Something many claimed did not line up with a dingo attack. This part of the argument boils down to “Look, we’ve never seen a dingo attack before, BUT IF WE HAD we’re pretty sure it wouldn’t look like this”. Despite only having a sample of dingo behaviour that included no attacking of people, we used it to understand how they would attack people. In other words, the sample size was too small to capture this rare event, but even so it was used to understand a rare event, expanding beyond the limitations of our data.\n\n\nWhere did this Data Come From? Is My Sample Big Enough?\nIf we have a sample, it needs to be large enough to be an accurate representation of whatever population we are trying to understand. How large is large enough? It depends. For a rare event such as a dingo attack (there are surprisingly few, feel free to google it), you need a VERY large sample. With too small of a dataset the estimated probability of a rare event goes from improbable to impossible. Additionally, regardless of the size of our sample, we cannot use it to understand events that it has no representation for. Sample size is one of the most commonly cited problems in research critiques, but the problem goes much deeper than that. Most statistical tests (each of which have their own assumptions) typically rely on the sample to be REPRESENTATIVE of the population. Which means it needs to be large enough to capture the rarities of the data, and be an accurate, random sample of that population. Usually as statisticians we focus on averages, but outliers have their own value in of themselves. There are two main questions we should ask about our sample, and the prosecution should have asked about dingo attacks.\n1. What is this a random sample of?\nIf you have reason to suspect there might be a difference between all dingo attacks, vs the dingo attacks we know about we cannot fairly say we know the real probability of something like a dingo attack on a person. To think about what we have a sample of, we need to ask what might cause bias, correlation between observations, ect.\n  \n2. Is this sample big enough?\nThis is usually important for reliability in our understanding, but that is even more important when trying to figure our if the event we are looking at is impossible or just very rare. Even if our sample is big enough to be representative it also needs to be big enough that we can reliably understand its relationship to other factors."
  },
  {
    "objectID": "posts/PopulationvsSample/index.html#prosecution-point-2",
    "href": "posts/PopulationvsSample/index.html#prosecution-point-2",
    "title": "Criminal Statistics in Baby Murder Court Cases",
    "section": "Prosecution Point 2",
    "text": "Prosecution Point 2\n\n“This is Baby Blood In Your Car… We Think”\nOne of the strongest pieces of evidence (still circumstantial which makes the guilty verdict even more embarrassing) was forensic. Basically a test for fetal hemoglobin in the boot of the Chamberlain’s car had it lighting up as a baby blood fountain. What was not discussed at the time of the trial, and what actually later played a key role in exonerating Lindy Chamberlain, was that this test is not conclusive at all. If you want to use the positive results to directly implicate blood, we need to assume the absence of all the other sources of a positive result. Interestingly, one of the substances that could give this test a positive was copper dust. The Chamberlains lived in Mt Isa, a copper mining town. This case of bad science is a perfect example in drawing the wrong conclusions from a sample because our assumptions of whatever method we were using were not fulfilled.\n\n\nContext is Everything\nEvery time you do any kind of statistical test, you bring with it a suitcase of assumptions. The type of data, the relationship, ect, ect, most of which goes overlooked. To perform a statistical test without all the assumptions met isn’t a crime, but we need to understand what happens to our results when they are not met. If you are sitting there thinking “oh well that’s just for inference, not for prediction, I’m safe” guess again chump. The issues in assumptions of method and model can mess with your predictions as well.\nLets look to epidemiology to understand this further. The most basic model for prediction infection for diseases is the SIR (susceptible, infected, recovered) model. Basically it just says here is the number of people who could catch the disease, here is the number that have it, and here is the number that have previously had it. The model, while useful, makes several assumption about the interaction of people that are not realistic. For example, by assuming everyone is equally likely to interact with each other and equally likely to catch the disease, it ignores the nuance of social groups. The model has shown before that a high infection rate caused by a single susceptible social group, will lead to massive over prediction for the whole population.   \nThis leads me to the third important question you should ask yourself when trying to understand what conclusions you can draw from your sample.\n3. What are the assumptions of my model, and what happens when they are violated?\nThis questions essentially follows after the other two, and is slightly different. Now instead of asking a question about the sample, we are asking a question about the underlying variables. This both gives us the ability to construct models that are more accurate to the problem at hand, and understand the context in which our statistic models might fail. Assuming our current sample’s trends will expand beyond the limitations of the data is a fatal flaw of extrapolation."
  },
  {
    "objectID": "posts/PopulationvsSample/index.html#in-conclusion",
    "href": "posts/PopulationvsSample/index.html#in-conclusion",
    "title": "Criminal Statistics in Baby Murder Court Cases",
    "section": "In Conclusion",
    "text": "In Conclusion\nBoth statisticians and legal prosecutors make mistakes all the time, that being said, sometimes they were avoidable. If we simply take a step back, and assessed the limits of our conclusions we could stop many overconfident predictions, and wrongly accused mothers. Before doing any kind of analysis there are a handful of questions we should ask ourselves. What is my data a sample of? Is my sample large enough to capture what I’m investigating? Is my model making ridiculous assumptions about my variables? Is murder REALLY the most likely reason for a fragile baby to die? Ok, maybe less of the last question for statistics, but the other three for sure."
  },
  {
    "objectID": "posts/bias_variance_flexibility/index.html",
    "href": "posts/bias_variance_flexibility/index.html",
    "title": "A Deep Dive into How Flexibility Affects The Bias and Variance Trade Off",
    "section": "",
    "text": "When we are building a machine learning model you have a choice of a simple, which would be an inflexible, model vs a complicated, or very flexible model. We need to decide how flexible the model should be to work well for future samples. An inflexible model may not reflect a complex underlying process adequately and hence would be biased. A flexible model has the capacity to capture a complex underlying process but the fitted version might change from one sample to another enormously, which is called variance. This difference is illustrated in the figure below.\nWhen we think of how the bias and variance change with flexibility, we typically only look at its behaviour on average. In the plot below, the left side corresponds to an inflexible model and the right side corresponds to a flexible model. We can see that the test error stay slightly above the training as flexibility increases, until the text error shoots up. Visualisations like this are shown frequently in the textbook “An Introduction to Statistical Learning with Applications in R” by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, which largely inspired this blog post. While this explains the behaviour of our test error on average, it doesn’t give a complete understanding of how our test error estimate will act within any individual sample. This is where we find the benefit of understanding the error distribution. The distribution of the test error allows us to not only understand the average behaviour, but also how that behaviour may change from sample to sample."
  },
  {
    "objectID": "posts/bias_variance_flexibility/index.html#flexibilitys-influence-on-test-error",
    "href": "posts/bias_variance_flexibility/index.html#flexibilitys-influence-on-test-error",
    "title": "A Deep Dive into How Flexibility Affects The Bias and Variance Trade Off",
    "section": "Flexibility’s Influence on Test Error",
    "text": "Flexibility’s Influence on Test Error\nWhen changing the flexibility of a model, the test error distribution will go through three phases, that affect both its expected value, and variance.\n\nPhase 1: Decreasing Bias in Model\nWhen our model is biased, we are forcing our data into constraints that don’t reflect the true relationship between the variables. Since we have not captured the true relationship of the parameters, any sample drawn from our population will also have a more complicated relationship than that of our model, and have error from bias. This relationship is illustrated below, where our high error is largely the result of too much bias in the model. Both distributions are similar to each other, but far from zero.\n\n\n\n\n\nPhase 2: Optimal Fit\nIncreasing the flexibility will reduce the bias which will decrease the error. The optimal error will have smaller error for both training and test, but they will both be pretty similar. If you have captured the true relationship of the data with your model (if there is one), the distributions should perfectly overlap. This is unlikely to happen, since your model will always have a bias towards any quirks in your training set, and thus perform better on that set most of the time. So we instead will interpret the optimal fit is when the test error reaches its minimum (before the variance causes the total error to start to increase).\n\n\n\n\n\nPhase 3: Increasing Variance in Model\nAs we start to overfit our model, we introduce more error from variance than we are losing from decreasing bias. This has two effects on the distribution of the estimated test error. First, it causes the distribution to shift upwards as we have once again missed the true relationship in the population. This miss is different from bias however, as we have overfit our model to the specifics of the test set sample, thus new samples drawn from the same population will not have a similar error. This causes the distributions to shift away from each other. Additionally, the variance of the test error estimate will also increase. Overfitting means a higher penalty for samples that just happen to be different from our training set, and a higher reward for those that just happen to have similar quirks. Ultimately that makes the estimates more unreliable, and thus have a higher variance.\n\n\n\n\n\nUnderstanding with an Example\nThis influence from flexibility can best be seen with an example. To illustrate this, we will use the Auto data from the ISLR package, and fit a model to predict mpg using a polynomial of horsepower. If we take a look at the scatterplot of the two variables below, we can see that the linear model might not be flexible enough, but anything more flexible than a polynomial of about 4, will very likely overfit to the training sample. The plot below shows the data with a loess fit.\n\n\n\n\n\n\n\n\n\nWe can see the effect on the distributions using the animated density plot below. Here we have taken 100 different samples, and fit a model that predicts mpg using a polynomial degree of 1 to 15 of horsepower. Here we can see the above hand drawn illustration and interpretation of the variable relationship play out. Initially, increasing the flexibility of our model eliminates bias and causes both distributions to shift down. At polynomial degree 4, they stop at the minimum, and then for polynomial degrees higher than that, variance is introduced, and the test error increases in both expected value and variance.\n\n\n\n\n\n\n\npng"
  },
  {
    "objectID": "posts/bias_variance_flexibility/index.html#sample-to-sample-changes",
    "href": "posts/bias_variance_flexibility/index.html#sample-to-sample-changes",
    "title": "A Deep Dive into How Flexibility Affects The Bias and Variance Trade Off",
    "section": "Sample to Sample Changes",
    "text": "Sample to Sample Changes\nHere it is important to highlight the difference between a population and a sample, so we can better understand how an unfortunate test and training split can hurt error estimates. A population is all the data on what you are trying to make an inference on. For example, if I want to make an inference on the true relationship between mpg and horsepower, the Auto data is a sample of that. Generally we would be interested to make statements for mgp and horsepower for all possible cars, where all possible cars would be our population. If I want to make an inference on the relationship between mpg and horsepower in the Auto dataset (which is a weirdly specific realm to keep your inferences to but each to his own I guess) then this data is the population sample. For our sample to be representative, it needs to both be randomly drawn, and large enough. Unfortunately, even when we draw our samples to be decently large in size, and random, we will still occasionally get some unrepresentative splits. A sample that is unlike the population will bring the validity of any inference we try to make using that sample (including predictive models) into disrepute. Below is an illustration on how the sample will influence the fit among other interpretations.\n\n\n\n\npng\n\n\n\nThat being said, it’s highly unlikely to get a difference that dramatic in an actual sample. In reality, minor, almost invisible to the eye differences in your sample will create large differences in your MSE estimates.\n\nAn Example of Sample Influence on Error\nThe scatterplots below shows two of the training and test sample splits that were used in the phases example. One produced the best test error on the polynomial 15 model (MSE= 105) and the other, the worst (MSE=9837). Is there a remarkable difference?"
  },
  {
    "objectID": "posts/bias_variance_flexibility/index.html#how-our-estimation-method-influences-our-test-error-distribution",
    "href": "posts/bias_variance_flexibility/index.html#how-our-estimation-method-influences-our-test-error-distribution",
    "title": "A Deep Dive into How Flexibility Affects The Bias and Variance Trade Off",
    "section": "How Our Estimation Method Influences Our Test Error Distribution",
    "text": "How Our Estimation Method Influences Our Test Error Distribution\nA glaring issue with our test error estimate is its high variance, which means less certainty in the conclusions we draw from our test estimates. If we want a test error estimation method that is less susceptible to this issue of variance, we could try using a cross validation method. All methods, like the test error shown above, will still follow the general phases caused by increasing flexibility, but some have a lower overall variance (at the cost of more bias).\n\nThe Phases Example Using Cross Validation\nWhen we originally looked at the test error, it was estimated with the validation set approach (test in the plot) for simplicity. Now, let’s redo those distribution estimations of error from the mpg and horsepower models, but also look at the distribution of the 10-fold (k10cv), and 5-fold cross (k5cv) validation methods.\n\n\n\n\n\n\n\npng\n\n\n\nHere we can see the bias variance trade off play out with our estimates of test error, just as they would with our model fit. Our cross-validation methods in order of increasing variance are:\n\n5-fold CV < 10-fold CV < Validation Set Method\n\nThe methods in order of increasing bias are:\n\n10-fold CV < 5-fold CV < Validation Set Method\n\nIn general, the k-fold CV bias and variance depends on the value of k, where LOOCV (k=n) is approximately unbiased."
  },
  {
    "objectID": "posts/bias_variance_flexibility/index.html#to-summarise",
    "href": "posts/bias_variance_flexibility/index.html#to-summarise",
    "title": "A Deep Dive into How Flexibility Affects The Bias and Variance Trade Off",
    "section": "To Summarise…",
    "text": "To Summarise…\nAs the flexibility of our model increases, we know that the estimated model will have a decrease in bias and increase in variance. This change in our model causes both a change in the mean and variance of our estimated test error. A lot of the difference is caused by the increasing impact of our random sample split, however it is not something that is visually noticeable. Like the model, the method of test error estimation also has its own bias and variance trade off, and it can be balanced using cross validation methods.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "posts/LDA/index.html",
    "href": "posts/LDA/index.html",
    "title": "A Future Public Disturbance Explains LDA",
    "section": "",
    "text": "I would like to say that I’m the type of person who would never complain at a restaurant, but deep down I know I’m one bad day away from being the woman in a viral video pissing on the floor of a Starbucks because they didn’t give her soy milk. If you would like to see the correlation that makes me think this, see the figure below:\n\n\n\nDespite this clear character flaw which I refuse to talk about further, I have never written an online review of a restaurant. I came close earlier this year when my sisters and I decided to spend money we didn’t have on some fine dining, thinking it would be a nice night out. Unsurprisingly from the tone of this post, the experience turned out to be a nightmare. When our waiter eventually asked if we were OK I looked at her with a manic glint in my eyes and told her I was silently praying that something from the ceiling would fall and crush me, so I could escape the eternal purgatory of being trapped in this restaurant. Spending all my money to get the opportunity to wait hours for an onslaught of terrible food, was probably an indicator I had already died and this was some personalized version of hell. Here is a snippet of a review I got halfway through writing, and then forgot about until this very moment.\n\n\n\nYou could consider my complaints to be wildly privilege and out of touch, but my dog died a year ago, and I would rather relive that than go to this restaurant again. If you are thinking to yourself that I seem to have unhinged emotional responses to slight upsets, you would be correct. So while we are on the topic of out of touch emotional responses, lets talk about Linear Discriminant Analysis (LDA).\nBack when I learnt about LDA, I had a regularly maintained page in my notes called “LDA is ruining my life”. Every tutorial for about 4 weeks would be me pointing to a section of the page and asking my tutor why all of it worked the way it did, and why I was too stupid to understand it. Ultimately my issue stemmed from one key question: Is LDA a classification or dimension reduction technique; and if its both, how are they related? I never figured it out, and after the exam, I decided it was something for people greater than myself to know. Or I decided it was not my problem any more. The distinction is unimportant. What is important is that a few weeks ago I overheard someone in the department talking about LDA and I had what I can only describe as machine learning based war flashbacks. So, thanks to this conversation, I reopened the page that hurt me to my soul, and made some plots to help me (and by extension you, the person reading this post) finally understand how LDA works. I’m going to break this down into two sections:\n\nHow LDA classification works in the 1-dimensional case\nHow LDA dimension reduction works in the 2-dimensional case and then extends to classification.\n\nFor the running example, we are going to look at some restaurant reviews to maintain the theme of “things Harriet has disproportionate emotional reactions to”. If anyone was looking for a sign that I’m running out of ideas, here it is."
  },
  {
    "objectID": "posts/LDA/index.html#theory",
    "href": "posts/LDA/index.html#theory",
    "title": "A Future Public Disturbance Explains LDA",
    "section": "Theory",
    "text": "Theory\nLDA is pretty straight forward as far as classification models go. Every time we use a classification model, we are implicitly asking “Which of these groups is more probable?”, LDA just does this very literally. If you are unfamiliar with Bayes theorem (that would be alarming but I’m an adult who said “Who’s Cyprus?” to my housemate the other day so I can hardly judge) it looks like this:\n\\[P(Y=k|X=x) = \\frac{P(K=k)P(X=x|K=k)}{P(X=x)}\\]\nIf you don’t have an intuitive sense of Bayes theorem, its actually pretty easy to draw how it works. Lets say we have two groups, and we want to find the probability that an observation belongs to either Class 1 or Class 2 based on our predictors. Since there are two versions of the function (one for Class 1 and one for Class 2), we will have two different possible equations to plot, and so two different densities. To start with, LDA assumes that both classes are normally distributed and have the same variance, so we only need to calculate three things, each of the group means and their shared variance. Once we have these values we can draw the associated normal density, that is \\(P(X=x|K=k)\\) for each value of K.\n\n\n\nThis is already looking pretty good, but what if each class is not equally probable? Well, we can make these densities more accurate by scaling them by their relative class probability, i.e. \\(P(K=k)\\). So lets say that class 2 is much more likely than class 1, then we end up with this:\n\n\n\nThen to finish we just scale both of them down so that they follow the probability axioms. That is, we make sure the probability of belonging to Class 1 + the probability of belonging to Class 2 is not not greater than 1.\n\n\n\nWith that we end up at the equation for Bayes theorem. Unfortunately this theorem does not give us a classification rule, but rather relative probabilities. To make a rule we could calculate this probability for every class, and then classify our observation to whichever class spits out the largest value of this function, but that is a bit tedious. It would be much easier to have a single value where numbers above it are Class 2 and numbers below are Class 1. Lucky for us, the y-axis here gives a probability, which means for values of x where the Class 2 function is higher, Class 2 is more probable, and vice-versa for Class 1. Therefore, the best place to draw our boundary is when the densities overlap, and both classes are equally probable.\n\n\n\nThis the basics how LDA classifies observations. The only thing to note, is that it doesn’t do this on the scale of your X variable. It will scale X by some constant such that the classification bound drawn above is at 0, and this new scale is called the Linear Discriminant. To make this a little easier to understand, I can give a real example of 1-Dimensional LDA classification."
  },
  {
    "objectID": "posts/LDA/index.html#example",
    "href": "posts/LDA/index.html#example",
    "title": "A Future Public Disturbance Explains LDA",
    "section": "Example",
    "text": "Example\nWhile it would be in character for me to sit alone in my room and write hundreds of spiteful restaurant reviews to make a data set, I’m not going to do that. For this analysis I’m going to use a Kaggle data set with about 1000 restaurant reviews, where each observations has two variables; the typed review and a yes/no indicator for whether or not the person liked the food.\nUnfortunately if we want to predict if someone liked the food based off their review, the raw version of this data isn’t going to cut it. We need some comparable variables, which means we need a measure of how positive or negative the review is. For this, I’m going to keep it simple, and use a basic sentiment analysis. For those who don’t know, there are several ways to assess the sentiment of a word, i.e. if it has positive or negative feelings associated with it. There are a number of “lexicons” that are just massive data sets that have a word (such as “hate” or “love”) and an associated score for that words sentiment (-5 or +5). I’m going to use two of these for this example; ‘AFINN’ which gives words a score from -5 to 5, and ‘bing’ which rates words as either positive or negative (simplified to 1 or -1 here). To use these as variables in our example data set, I took the average sentiment of the words in the review for that lexicon. Finally we get a dataset with observations that look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview\nliked\nafinn_sentiment\nbing_sentiment\n\n\n\n\n22\nThere is not a deal good enough that would drag me into that establishment again.\nNo\n0.1333\n0.0667\n\n\n34\nA great way to finish a great.\nYes\n0.8571\n0.2857\n\n\n94\nThis place should honestly be blown up.\nNo\n0.0000\n0.0000\n\n\n103\nTo my disbelief, each dish qualified as the worst version of these foods I have ever tasted.\nNo\n-0.1765\n-0.0588\n\n\n199\nSo they performed.\nYes\n0.0000\n0.0000\n\n\n\n\n\nNote: These reviews are some of my favourites, they are not a random sample.\nRight now, I’m not going to use both of these sentiments. We are still in the one dimensional classification case, so lets stick to the AFINN average. To see how this variable splits the reviews, lets make a density plot of the data similar to the one explained above.\n\n\n\n\n\n\n\n\n\nFrom this plot we can see that these distributions are neither normal, nor have similar variance. You will rarely have data that behaves exactly according to the assumptions. Does this mean we can’t use LDA? Not really, it just means that if (or when) we end up with a boundary that is a bit different to what you would draw yourself, it is probably because of these incorrect assumptions.\nIf we perform LDA on this data, we can find the coefficient of linear discriminants. Earlier I said that LDA will perform classification on its own scale, and this is how we find it. This value is used the same way it would be in a regression function, where it gives us the coefficients in the formula:\n\n\n\n\\[x = 2.929\\times AverageSentiment_{AFINN}\\] Where x is the “linear discriminant” for that review. We can plot this variable (although it is just afinn_sentiment scaled by a constant) with the decision bound at 0 to see what our linear discriminant space looks like:\n\n\n\n\n\n\n\n\n\nIt looks exactly the same except now the border for a 50% threshold is at 0. You may notice this is not where the densities overlap, and that is because LDA has assumed that distribution for “No” reviews is more spread out than it is. While it isn’t perfect, with this example under our belts we can move onto using LDA for dimension reduction."
  },
  {
    "objectID": "posts/LDA/index.html#theory-1",
    "href": "posts/LDA/index.html#theory-1",
    "title": "A Future Public Disturbance Explains LDA",
    "section": "Theory",
    "text": "Theory\nNow we move onto the dimension reduction aspect of LDA. Remember how in the previous example LDA changed the original variable to make its own scale, LD1? Well, that is the dimension reduction part working, although in that case it wasn’t reducing the dimensionality, just scaling it. LDA dimension reduction is actually very similar to Principal Component Analysis (PCA) dimension reduction, in that it uses eigen-decomposition. To avoid going into some complicated details you can find elsewhere, whenever you read eigen-decomposition just think “we found direction of largest variance, represented it as a line, and projected data onto that line” and you know enough to make it through this post.\nIn my last post, I explained eigen-decomposition and how it is utilized in PCA, but it is much easier to understand when you to see it working. The animation below depicts a scatter plot of data, with a line that rotates around its centre. This line represents a potential direction for us to project our data onto (i.e. a 1-Dimensional representation). When we did PCA, we wanted to pick the line that has the highest variance, that is, had the fattest distribution when plotted as a density plot, and this animation lets us roughly see when that point is:\n\n\n\n\n\n\n\n\n\n\n \n\nThe point of this is visually show you the value of eigen decomposition. It gives an instant analytical result for what we could try to find with the animation above.\nHow is this relevant to LDA? Well it also uses an eigen decomposition, but it isn’t as straight forward as the PCA case. Now we have classes that we need to take into account. LDA decomposition works with the goal of minimising the overlap between classes. That is, if we plot the density of the two groups, we want the smallest possible overlap, as shown below.\n\n\n\nThere are two components of the distributions that come into play when picking this direction: 1) Distance of Class Means. The importance of this bit is pretty obvious. If the two groups are further apart then they have less overlap. Regardless of the features of the data, this will always be an important aspect. 1) Class Variance and Covariance. While having data that is far away is nice, if there is another projection that brings the class means closer, but makes the resulting distribution steeper, it will result in less overlap. Finding a balance between these two components is what allows us to maximise the distance between the two groups, and find the best 1-dimensional projection of the classes. The interaction of these two elements may not be easy to see above, but it will make a lot of sense with some animated examples. In these examples I have added a value called “VarDist” in the corner, I will get to the calculation of this value later, but for right now, know that it represents the interaction of these two components and we are looking to maximise it.\nTo start, lets take the variance and covariance of the data completely out of the equation and look at a simple case where our variables have no correlation and our classes are just two bivariate normals with different means.\n\n\n\n\n \n\nThe animation shows two distributions that change in central location, but their shape stays the same. Since the scatter plot shape of each group is essentially circular (due to the 0 correlation of the variables), no matter what direction we project the data into the shape (and therefore variance) will be the same. This means we can ignore the variance and focus on maximising the distance between the means. This is achieved by projecting the data on the line that goes through the two group averages. Moving on from this simple example, lets make things more interesting and look at some data where the variables are correlated.\n\n\n\n\n \n\nNow we can see two forces at play. Just as before, when the line goes through the two means the data is most separated, but this is no longer the only factor we need to consider. The positive correlation means that the direction we project the data onto can now also flatten or steepen the curve. We can no longer use the line that goes through the two means, because if another direction brings the distributions closer, but also significantly decreases the spread, that would be the preferable option. We can see this in example with negative correlation too.\n\n\n\n\n \n\nNow that we have seen how this works intuitively, we can go through how this is calculated. So, how does LDA perform a decomposition that accounts for these two competing elements? It combines two matrices in the eigen-decomposition, the variance covariance matrix and a matrix of the between group averages.\nFirst we want to minimise the within class variance of the projected data. The first important thing to note is that by the assumptions of LDA, all the classes have identical variance-covariance matrices. Therefore to calculate the matrix, we get the variance-covariance matrix of each isolated class, and then average them. The averaging shouldn’t change the values (if your LDA assumptions are correct), it should just makes the estimate more accurate. This is illustrated in the picture below.\n\n\n\nNow that we have this matrix, how do we find the projection that minimises the variance instead of maximises? We just perform an eigen-decomposition on the inverse of the matrix. Now that we have taken care of the spread element of LDA, we can take care of the “separating the means” element. For this we create another matrix for the “between group differences”.\nThis is just a matrix representation of the distance between the classes which is constructed using the method illustrated below. Much like with the eigen-decomposition of the variance-covariance matrix, how this matrix works is not of major significance.\n\n\n\nSince we are trying to maximise this, we do the eigen decomposition on the matrix. Finally, to get the direction we are projecting our data onto, we need to just take the eigen decomposition of the combination of these two matrices, that is the matrix \\(\\Sigma_V^{-1}\\Sigma_B\\).\nIn this case, what is the “VarDist” value that appears in the plots?. Well, when you do an eigen-decomposition it analytically finds the direction that maximises the distance between groups, but instead of solving it analytically, we could also solve it iteratively and just check the product of the group variance and between group difference in the 1 dimensional projection for a series of projections and select the one that maximises this value. That is the value that is shown in the corner of the animated plots and its calculation is shown in the formula below:\n\\[VarDist=({\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2}})^{-1} \\times \\sum_{i=1}^2(\\bar{x_k}-\\bar{x})^2\\]\nWith this information, it should be clear how we get the 1-dimensional representation that best separates the two classes. While a technical understanding is fine, it is easier to see this come together with an example."
  },
  {
    "objectID": "posts/LDA/index.html#example-1",
    "href": "posts/LDA/index.html#example-1",
    "title": "A Future Public Disturbance Explains LDA",
    "section": "Example",
    "text": "Example\nFollowing on from our example before, let’s add in a second variable, the bing sentiment measure. Now that we have two variables we can plot them together on a scatterplot to see their relation.\n\n\n\n\n\n\n\n\n\nThis plot gives us a few things to note. First of all, this is clearly not two bivariate normal densities, as LDA will assume. Again, this just means our results will be slightly off. In addition to this there are a lot of 0 values for both the AFINN and bing sentiments. This can occur in two circumstances: 1) none of the words in the review appear in that lexicon because they are all neutral (“I’m uncertain I could call that food”), or 2) if the sentiment scores of the words in the review cancel each other out (e.g. “The food was so good I felt bad eating it”). Since this only impacts the assumptions of LDA we are going to power through.\nJust like with the classification, we can perform LDA on this dataset and get a formula that calculates the Linear Discriminant for each review. In this example our coefficients give a function that looks like this:\n\n\n\n\\[ x = 0.800\\times AverageSentiment_{AFINN} + 5.576\\times Average Sentiment_{bing}\\] Now, instead of the linear discriminant just scaling the variables so the 50/50 probability split is at 0, it is a linear combination of the two variables. How did it get these coefficients? Using the eigen-decomposition described above. Again, we can calculate the linear discriminant of each review, and in doing so, have a 1-dimensional projection of the data. Now that we have two variables, just like with PCA we can interpret their coefficient values. If the values are on the same scale (or you scaled them prior to performing your analysis) then this is a simple step of comparison. Unfortunately I did not do that (I wanted it to be clear which sentiment was which by the scale or I was being lazy, you pick) so instead we can plot it on the previous scatter plot and comment on the steepness of the slope. Below is a plot of the data with the line it is projected onto, as well as the resulting density.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the slope is not steep, we can see that AFINN sentiment contributes more to the dimension reduction than bing does, and is therefore more important when it comes to separating the groups. This is how we interpret the dimension reduction. You can see that I added the decision boundary in too. Once we have a single dimension that our data exist on, the classification is exactly as it was above, drawing the line where the two densities overlap (or not quite when we violate the assumptions)."
  },
  {
    "objectID": "posts/why_visualise_uncertainty/index.html",
    "href": "posts/why_visualise_uncertainty/index.html",
    "title": "Do You Have a Good Reason to Ignore Uncertainty? Check if I Approve of Your Reason Below",
    "section": "",
    "text": "Recently I went to a conference with the central topic of visualising data. The conference was great and it was the first time i have ever been able to understand most of most of the speeches. Usually I just stare at the speaker’s slides and wonder if I am the only one who has no idea what they are talking about. There was one overwhelming sentiment shared by all the speakers that seemed a little disastrous to my research. Speaker after speaker went to the podium, started their speech, and said they didn’t visualize uncertainty (except keynote speaker but I was late and missed it). As a PhD student whose research centres on visualising uncertainty, this could be seen as a bit of a spanner thrown into my work. I will admit that it became a running joke in my notes, to see how long until the speaker either admitted they don’t visualise uncertainty, or just outright dismissed it as feasible in their work. I started to wonder if living in the woods and carving sticks would be a more fruitful career than what I was currently doing. I have since spoken about this to multiple friends who have admonished those who openly reject visualising uncertainty, but I actually respect the honesty. In trying to improve uncertainty visualisations, I have noticed that uncertainty is rarely visualised and there must be a reason for it that goes beyond “the current methods aren’t good”. We can all sit here and say “we should visualise uncertainty” but the reality is, nobody actually does and everyone has some reasoning for it that outweighs misrepresenting our results. Even if I spend months working on a new way to visualise uncertainty, without understand why nobody does it, my work would be born to live in a frame on my mothers wall, read only by myself and examiners, and used by nobody."
  },
  {
    "objectID": "posts/why_visualise_uncertainty/index.html#authors-dont-want-to-overwhelm-the-audience",
    "href": "posts/why_visualise_uncertainty/index.html#authors-dont-want-to-overwhelm-the-audience",
    "title": "Do You Have a Good Reason to Ignore Uncertainty? Check if I Approve of Your Reason Below",
    "section": "Authors Don’t Want to Overwhelm the Audience",
    "text": "Authors Don’t Want to Overwhelm the Audience\nThe common theme among these reasons seems to be a belief that uncertainty is secondary to estimations. The argument speaks to an unspoken belief of those that work with data, that the uncertainty associated with an estimate (the noise) only exists to hide the estimate itself (the signal). From this view, uncertainty is only seen as additional or hindering information, therefore despite its alleged importance, when simplifying a plot uncertainty the first thing to go.\n\n\n\n\nHow I presume visualisation authors see a distribution\n\n\n\nBelow are some examples of these reasonings.\n\nClutter\n\nReason\nWhen showing a graphic for a short period of time (such as on TV) you can only present one idea per graphic, and uncertainty will clutter the visualisation.\n\n\nRebuttal\nIf you can only show one idea, why not show the uncertainty? For example, something as simple as presenting a range instead of a point estimate would give an idea of central location as well as degree of uncertainty.\nPeople being unable to make quick decision’s using uncertainty may not even be true. A study that got participants to account for uncertainty in bus arrival times showed laypeople have ability to make fast and accurate decisions that accounted for uncertainty that was displayed on a small screen (Kay et al. 2016).\n\n\n\nAudience Understanding\n\nReason\nThe general public does not understand randomness so including uncertainty will only confuse the audience.\n\n\nRebuttal\nThis statement has two halves that are technically true if you constrain it. For example, the statements “the general public does not have a full and detailed understanding of the philosophical arguments behind uncertainty” and “some people struggle with uncertainty” are both true. What does not seem to be true is the statement above.\nFirst, lets address if the general population can understand uncertainty. I think blanket yes/no conclusions about whether or not uncertainty was understood ignore the intricacies of understanding uncertainty that comes through in the research. The general gist seems to be this. Can laypeople interpret uncertainty in a way that is consistent with frequentist philosophy? No [Hoekstra2014]. Can laypeople reliably translate an error bar plot to an equivalent hypothesis test? No (Bella et al. 2005). Can laypeople read an uncertainty plot and make accurate and efficient decisions factoring the uncertainty into their choices? Yes (Kay et al. 2016) (Fernandes et al. 2018). Additionally, while some people may have a lower baseline than the general public, most people get better at understanding uncertainty plots the more they are exposed to them (Kay et al. 2016).\nRefusing to express uncertainty because people don’t understand them, prevents people from improving in their ability to understand the plots, causing those that may not be able to understand uncertainty to continue to be bad at it.\n\n\n\nComplicates Decision Making\n\nReason\nUncertainty confuses people and makes it harder for them to make decisions. Because of cognitive overload we want to be careful about conveying important information.\n\n\nRebuttal\nRemoving uncertainty makes the decision making process easier in the same way presenting no choices does. Artificially. Very often there are ways to simplify other aspects of the design without sacrificing uncertainty information.\nI think this argument also comes from a misunderstanding of who the decision makers should be in some cases. When presenting evacuation information and you are worried about confusing a general audience, the Government may choose to present a simple “yes or no” threshold. Here, it is not that uncertainty was omitted, but rather the decision maker has been changed.\n\n\n\nCommon Knowledge\n\nReason\nThe presence of uncertainty is common knowledge and does not need to be explicitly stated. Sometimes I\n\n\nRebuttal\nHow much uncertainty is the actual issue at hand. A vague reference to some uncertainty is quite frankly nonsense. While authors typically prefer to express uncertainty in vague terms and will use reasons such as the one above to justify it, decision makers prefer uncertainty in precise quantitative terms (Erev and Cohen 1990), (Olson and Budescu 1997).\n\n\n\nPeople Don’t like it\n\nReason\nPeople cannot tolerate uncertainty and it creates negative feelings.\n\n\nRebuttal\nThis is true, however it is not a reason to avoid visualising uncertainty."
  },
  {
    "objectID": "posts/why_visualise_uncertainty/index.html#authors-dont-know-how-to-calculate-uncertainty-or-dont-have-access-to-the-information-to-do-so.",
    "href": "posts/why_visualise_uncertainty/index.html#authors-dont-know-how-to-calculate-uncertainty-or-dont-have-access-to-the-information-to-do-so.",
    "title": "Do You Have a Good Reason to Ignore Uncertainty? Check if I Approve of Your Reason Below",
    "section": "Authors Don’t Know How to Calculate Uncertainty or Don’t have Access to the Information to do so.",
    "text": "Authors Don’t Know How to Calculate Uncertainty or Don’t have Access to the Information to do so.\nThis section is basically a tour of using incompetence to cover up laziness, a fear of being wrong, or a general lack of desire to do something. I know because I do it. I think the term to describe this that the young socially awake kids use is “weaponised incompetence”. It is when someone pretends to be incompetent so they don’t have to do things they don’t want to do (such as visualizing uncertainty). The tactic was used by my asshole housemate to avoid unpacking the dishwasher for over a year. Using incompetence as an excuse to not do something doesn’t fly too well when you remember, you can just learn to do the thing you don’t know how to do. If someone genuinely cannot or is anxious about estimating uncertainty in any capacity, call me crazy, but I think working with uncertainty was not the best choice of career path.\n\n\n\n\nActually maybe my housemate was just a huge asshole\n\n\n\n\nMultiple Sources of Uncertainty\n\nReason\nSometimes there are multiple layers of uncertainty which are too hard to communicate so it is ignored.\n\n\nRebuttal\nI honestly am happy to take an claim of incompetence in good faith. Also, the details of uncertainty and the assumptions we have to make around them can be confusing, so much so that it is the next blog post topic. I don’t think this is a good reason to ignore the uncertainty all together because if something is worth doing, its worth doing badly. Whenever I have no idea what I’m doing I just estimate my uncertainty with bootstrapping or assume a normal distribution. I will probably continue to do this until one of my supervisors reads this post and tells me that is bad practice. In which case I will update this to reflect whatever they suggest.\n\n\n\nPrecision\n\nReason\nFear that uncertainty will imply unwarranted precision in estimates\n\n\nRebuttal\nI think this excuse is interesting because it ignores the entire principal of an estimate. An estimate is an effort to put a number on something uncertain. I suspect that authors see some uncertainty visualization, such as confidence intervals, to be a death sentence if the actual outcome is outside them. An estimate is a ball park, people don’t actually expect the true number to be exactly the estimate, but people DO expect an outcome to be inside a confidence interval. That is the whole point of them. I haven’t suggested a plot in any of the other sections, but I think this is the one case where a specific plot could fix the issue. A hypothetical outcome plot never gives a firm number on the uncertainty but gives people a “vibe” of possible outcomes. This way you can express uncertainty without giving any precision at all, and in some contexts conveys uncertainty better than other methods (Kale et al., n.d.) (Hullman, Resnick, and Adar 2015). You can even get a version that includes at least one highly disastrous outcome so you don’t get fired."
  },
  {
    "objectID": "posts/why_visualise_uncertainty/index.html#authors-dont-want-their-work-to-seem-questionable",
    "href": "posts/why_visualise_uncertainty/index.html#authors-dont-want-their-work-to-seem-questionable",
    "title": "Do You Have a Good Reason to Ignore Uncertainty? Check if I Approve of Your Reason Below",
    "section": "Authors Don’t Want Their Work to Seem Questionable",
    "text": "Authors Don’t Want Their Work to Seem Questionable\nThis sections is basically the “I’m committing fraud but I have convinced myself that I’m not committing fraud” section. This makes me think of the scene from the Big Short, where Steve Carell’s character asks his colleague why real estate agents would openly commit to fraud. His colleague responds by letting him know they are actually bragging.\n\n\n\nPersonally, I think people committing fraud and then justifying it with some obvious backwards logic won’t be fixed with “how can we better educate you on uncertainty” but rather “how can we convince you to go to a therapist so you are better at understanding your own motivations”. Regardless I will address each argument I have come across even though my heart isn’t in it.\n\nExpertise\n\nReason\nSome interviewees claimed that the uncertainty would add little to the plot because the audience knew that the author would only present statistically significant findings and the audience trusts their expertise.One participant in Hullman’s study said “Most people will trust the doctor, not necessarily because the information itself was trustworthy, but because the doctor was.” when referencing why a certain level of expertise allows one to omit uncertainty (Hullman 2020).\n\n\nRebuttal\nI personally wish my doctor would give me probabilities and I have spent hours complaining to friends about the ones that don’t. To make decisions about risk that others are taking on because you are an expert is wildly infantilising. I broke my finger when my usual GP was on leave and had to see someone else who said it was “probably nothing to worry about” and sent me home without an x-ray. I wish he had communicated the amount of uncertainty about that because I had to get a $5000 hand surgery 3 months later to correct his mistake. Because the uncertainty about his decision and the costs associated with an incorrect decision was not communicated to me, I took on a much higher risk than I was comfortable with. Significance is arbitrary, and communicating uncertainty around that value is necessary for the people making the decisions. This is just an anecdote but there is evidence that general audiences feel the same.\nThis excuse is similar to the “complicates decision making” reasoning in that the goal is to take on a god like position that not only calculates the statistical risk, but then decides how much of that risk the stakeholders are willing to take on. Any trustworthiness is artificially created through obfuscation of facts.\n\n\n\nTrust with the Audience\n\nReason\nCommunicating uncertainty will result in people trusting our results less rather than more.\n\n\nRebuttal\nThis just isn’t true. Hullman herself points out in her paper that there was a strange consensus that the participants seemed to think trust was a precursor to displaying uncertainty and not the other way around. I personally suspect people would trust visualisations that express uncertainty more than those that didn’t (and I’m sure there are some papers out there on this that I can’t be bothered to find right now) however I also haven’t seen any evidence to the contrary.\nAdditionally, this reasoning seems to also be used to hide a more sinister motivation. Multiple participants from Hullman’s study seemed to equate “trust” with “not being questioned” two statements that only a total egomaniac would think are equivalent. Many believed that you could only visualise uncertainty after you had established trust with the audience, stating that if you visualise your uncertainty prior to that “someone will inevitably ask, ‘how did you get these numbers?’”. A question I would argue is completely reasonable and justified. I just thing that if you commit fraud in response to your colleagues asking you reasonable questions, your fix is a personality one.\n\n\n\nFraud\n\nReason\nI am not visualising uncertainty on purpose to misrepresent findings. Hullman’s paper quotes an interviewee justifying not showing uncertainty in their visualisations because it hid the signal since the “data wasn’t reliable and uncertainty seemed too big”.\n\n\nRebuttal\nI have literally been asked to commit fraud in interviews, and when I pointed out to the interviewer that they were asking their new employee to commit fraud, I did not get a call back. I don’t know how else you could describe “playing with the numbers until you find one that looks good and showing those”, but apparently they felt like “fraud” was not fitting.\nIn statistics the truth is already blurry, that is the whole point of the field. What people don’t seem to understand is that the methods around statistics need to be pretty rigorous because the final result is slightly blurry. Even innocent selections at an initial stage can unknowingly introduce bias in the final results. The numbers are not ironclad, but highly sensitive to the choices made by the statistician, which is why avoidance of fraud should be a high priority. Going 2km over the speed limit when you are certain of your speed is one thing, going 2km over the speed limit when you have a pretty good reason to suspect your speedometer could already be out by 10km/hour is another.\nUltimately I am not sure how to reason with someone knowingly or unknowingly performing fraud. The issue could be a general issue of a lack of integrity in the community as a whole, and considering someone was willing to ask me to do fraud at the interview stage of a hiring process, I would not be surprised. In general I think the only way to handle cases of outright fraud is to try and incentivise honest and open displays of data. Not only within organisations, but within the data community as a whole."
  },
  {
    "objectID": "posts/why_visualise_uncertainty/index.html#why-do-i-think-people-dont-visualise-uncertainty",
    "href": "posts/why_visualise_uncertainty/index.html#why-do-i-think-people-dont-visualise-uncertainty",
    "title": "Do You Have a Good Reason to Ignore Uncertainty? Check if I Approve of Your Reason Below",
    "section": "Why do I think People Don’t Visualise Uncertainty?",
    "text": "Why do I think People Don’t Visualise Uncertainty?\nI personally think all these excuses are an effort to use someone’s incompetence (the audiences or their own) to justify not having to do something they don’t want to do.\nThis does not mean everyone who doesn’t visualise uncertainty is evil. Widespread issues like this are almost universally created by systematic problems and norms. But the rationale provided by the participants in these studies reek of back justification. Hullman herself notices this, claiming in her paper\n\n“It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty” (Hullman 2020).\n\nI took this as a fancy academic way of saying “I think these people are full of it and are making up random reasons to justify why their actions don’t reflect their beliefs”. This is not to say I have a poor view of the participants in the study. I think they are normal people doing what people do. Rather, I think that discussing the results with an absence of acknowledgement of the human psychology that got us there is disingenuous in of itself. Authors are likely reacting to unstated norms in the field that are so accepted they don’t even question themselves when they create a visualisation that doesn’t include uncertainty.\nFrom personal experience visualisation as a whole seems to be generally looked down upon in science. There is a large focus on facts and much less of a focus on communications. I sometimes wonder if there is an effort to purposely make research harder to understand. I don’t think I am entirely off the mark considering I have many memories of my undergraduate lecturers d gloating to students about high fail rates and difficulty of their course. Obviously there is a prestige to doing something so complicated others struggle to understand it. When you hear that the proof for the Poincare conjecture (the only millennium prize problem to be solved) could only be understood by experts meeting at a conference and understanding the work in groups over several days, it inspires an idea of godlike intelligence. Therefore, if something is hard to understand it is a more advanced idea, and you are smarter for knowing it. Of course, something can be hard to understand because it is poorly communicated, not only because it is difficult, but that seems to be lost on a few researchers. A man asking for directions to the train station in gibberish is also difficult to understand, but he is unlikely to stand in front of a multivariate calculus lecture and brag about still being lost.\nVery often, while reading papers, I am floored by how difficult some academics are to understand. The papers have become more comprehensive as I understood the field more, but even so, many papers still leave me confused. If the research put out by academia is so difficult to comprehend it is even inaccessible to the people in it, I wonder what the point of our research is.\n\n\n\n\nThis is your brain on academic research\n\n\n\nI want to clarify that I don’t think people are avoiding visualising uncertainty because its more prestigious to avoid doing it. However, I do think visualising uncertainty, and visualisation as a whole have become caught up in the scientific quest for prestige through gatekeeping the field with poor communication.\nThere is a common theme in Hullman’s paper of authors seeing uncertainty as a chip in their armour, a possibility to expose something they don’t know and they hide it. Authors don’t think audiences can understand uncertainty, so they make it completely inaccessible. Authors are afraid they don’t know how to compute uncertainty, so instead of doing it badly, they ignore it. Authors are afraid of being questioned when they show the uncertainty, so they hide it. There are a lot of field wide issues that seem to be coming into play when authors are choosing not to visualise uncertainty and it becomes impossible to pin down a single reason. Authors don’t have to give me their honesty, but they do need to give it to themselves, so the next time you sit down to make a visualisation, be honest with yourself about why you are ignoring the uncertainty."
  },
  {
    "objectID": "posts/permutation_variable_importance/index.html",
    "href": "posts/permutation_variable_importance/index.html",
    "title": "Using the Bachelor to Understand Permutation Variable Importance",
    "section": "",
    "text": "The season of the bachelor is upon us, and what better way to celebrate my love of drawn out reality TV, than to use it to explain permutation variable importance in the random forest model. For those who are not familiar, The Bachelor is a dating show where each week female contestants are eliminated when they do not receive a rose during the rose ceremony. The winner is famously difficult to predict, and many complicated factors (screen time, number of dates, ect) mean our variables are ever evolving through the season and difficult to use in analysis. Today we will not be predicting the winner of the bachelor (as fun as it sounds) but rather, we will use The Bachelor as the basis of an example in calculating variable importance.\n\nWhat Matters Most When Choosing A Partner\nAnyone who has viewed the show for many years starts to notice a trend in the girls who always make it to the end of the competition. In the image below I have circled the top six participants from last year’s season.\n\n\n\nNotice anything? The girls at the end of the bachelor are overwhelmingly blonde. Of course regular viewers would notice other things too. Like how every season has a group skydiving date that ends with one of the girls crying, overcoming her fear, and getting extra time with the bachelor (when I type this out the show sounds stupid). However we are going to focus on the hair, specifically how we can find out how important hair colour is in separating the winners from the losers.\n\n\nIntroducing Our Bachelorettes\nFor our illustration, let’s make an example competition that consists of 10 people, broken down into their most arbitrary characteristics: name, hair colour, eye colour, and job.\n\n\n\nObviously the real winner isn’t chosen on these characteristics alone, but this is a fake example and my fake bachelor is a shallow guy. First we give all the girls a final position in the fake competition, and assign them to one of three groups: finalists (top 3), place moderately (middle 4), and losers (bottom 3).\n\n\n\n\n\nA Normal Random Forest Model\nBefore we can even look at variable permutation, we need a random forest model. If you need refreshing on how they work, a random forest model will take B bootstrapped samples, and build a tree for each. Usually, just by chance, about a third of the contestants will not be used to build each tree, these are the out of bag contestants.\n\n\n\nTypically, for more complicated data sets, random forest models use a random subset of all the predictors at each node. However, Since we only have 3 predictors, we will ignore that for this example (it won’t have any major influence on our results). This model will have multiple trees, but for simplicity, we are only going to look at the first tree in depth, which is illustrated below.\n\n\n\nContestants 2,5,7, and 9 are our out of bag contestants and so were not used to build the tree. Running these four contestants through the tree we get our out-of-bag (OOB) error.\n\n\n\nNow at this point we have a bootstrapped sample, a tree, and an OOB error for all of the B trees in our forest (but we have only looked at the first in depth). This is the basis of a typical random forest model, and it is also what we will use as a point of comparison when we permute our variables.\n\n\nPermutation Variable Importance\nTo calculate the importance of a variable (in this case hair), we randomly permute that variable among the observations. This creates a new dataset where all the variables are the same EXCEPT for the one variable we are checking. So for the bachelor example, the girls have all the same characteristics as before except their hair colour is randomised.\n\n\n\nRationally, we can tell that if our Bachie isn’t using hair colour as a key decider for his life partner (as we would hope), randomising that variable would have no effect on the girls position in the competition. People getting divorced over dyed hair is no way for a society to function. Again, we calculate our OOB error, using the tree above and contestants 2,5,7 and 9. However, we now take our predictors from the table with the permuted hair variable.\n\n\n\nThis gives us an OOB error for the version of the bachelor where love is colour blind. The difference between the first OOB error and the OOB error for the permuted observations will give us the importance of hair colour in the first tree. We repeat this calculation for all trees in the forest, and take the average to find the overall variable importance. That in a nutshell is how we calculate the permutation variable importance.\n\n\nFinal Comments Before we Leave the Mansion\nIt easy to see the logic behind this method of calculating variable importance. If we are essentially rolling a dice to decide a variable, it shouldn’t be useful in making predictions. If previously that variable was important, we have caused serious damage to the predictive power of our model. While this isn’t a complete computation in variable importance (since we only calculated it for one tree and one variable), it’s purpose is to take a look under the hood of the process, and, hopefully, into the heart of our bachelor.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Do You Have a Good Reason to Ignore Uncertainty? Check if I Approve of Your Reason Below\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nTrying to Save My Housements (Specifically Tom) from Alcohol Poisoning with Regularisation\n\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2021\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nA Future Public Disturbance Explains LDA\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2021\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nUsing PCA to Bully My Housemates (Specifically Tom)\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2021\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nLearning Boosting Through Me Getting Fired from Tutoring\n\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2021\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nBaby Teeth are Temporary, Model Interpretability is Forever\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nHow a 5th Grader Makes $1500 in a Single Night (and How They Could Make More Using MLE)\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nCriminal Statistics in Baby Murder Court Cases\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\n4 Things We Can Learn About Conspiracy Theories and Model Flexibility\n\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nMario Party: Destroyer of Friendships and Explainer of Convolutional Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nMy Idiot Brain Ruined My School’s NAPLAN average, But Bootstrapping Could Have Saved it\n\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nUsing the Bachelor to Understand Permutation Variable Importance\n\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\n  \n\n\n\n\nA Deep Dive into How Flexibility Affects The Bias and Variance Trade Off\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2020\n\n\nHarriet Mason\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/LIME/index.html",
    "href": "posts/LIME/index.html",
    "title": "Baby Teeth are Temporary, Model Interpretability is Forever",
    "section": "",
    "text": "When I found out baby teeth fall out, I realised the futility brushing them. The teeth are temporary, but those extra 5 minutes of playing Pokemon are forever. So I quit brushing my teeth. This wouldn’t have been too big a problem for a normal kid, but I also refused to drink water. A strangely pervasive problem in our family that started young (my brother was weaned off breast milk using chocolate Breaka) and lived into adulthood. I exclusively drank Golden Circle Raspberry Cordial, called it pink juice, carried it in my drink bottle, and I would sooner collapse from dehydration before I drank anything else. As you can imagine my teeth decayed at an alarming rate. A visit to the dentist in second grade told my parents something they were well aware of. If you let a child make their own terrible health decisions, they will cost you $10k in dental bills because apparently to a child, pain is an illusion. A lesson that should have been no surprise to them since that same year I made Mum let me slip my broken arm out of its cast to do my ballet examination, and I was still annoyed I only got a Merit. I don’t know if all kids are immune to pain and the consequences of their actions, but I certainly was. So for years I had 4 metal crowns, 13 fillings, and a sudden jolt of pain every time I accidentally got aluminium in my mouth. As an adult I leant my lesson and brush my teeth and floss twice a day. I mean I still don’t drink water, I just upgraded from Pink Juice to Pepsi Max. But I still consider a 50% improvement an inspiring story of growth.\nWhat is the point of this story? Is it related to today’s topic or has this blog become a digital diary where I decompress years of a being a child psychopath with irresponsible parents? Both. Although if my parents has a say in this blog they would probably argue they weren’t irresponsible, but rather thought the best way for us to learn was to experience the consequences of our decisions. The problem in my decision making as a child was I had too much of a focus on the long term. While it was true that the teeth were not permanent and would fall out, I still cringe at the idea of biting into metal packaging. Most methods of understanding machine learning models focus on the model as a whole, but in this post we are going to look at the local interpretation. LIME (Localised Interpretable Models) is a model interpretation method that can be applied to any machine learning algorithm, even if its a “black box” method by breaking it into smaller local models that are easy to interpret. To understand the value in this, we need to first look at the flexibility and interpretability trade off."
  },
  {
    "objectID": "posts/LIME/index.html#the-flexibility-and-interpretability-trade-off",
    "href": "posts/LIME/index.html#the-flexibility-and-interpretability-trade-off",
    "title": "Baby Teeth are Temporary, Model Interpretability is Forever",
    "section": "The Flexibility and Interpretability Trade Off",
    "text": "The Flexibility and Interpretability Trade Off\nI have mentioned (at length) the bias and variance trade off that comes into play when considering the flexibility of a model. What I have not mentioned, is the interpretation trade off that happens at the same time. When we “localise” our model by increasing its flexibility, allowing it to better respond to changes in variables, we also “localise” the possible interpretation. This in turn, means that a single interpretation for the entire span of the possible inputs is no longer useful. At the extreme end of this trade off, we have models in which the intermediate steps are almost impossible to understand, called “black box” models. Early statistics courses introduce flexibility with quadratic models, and deal with the trade off by splitting the area of interpretation. Sadly this is not an idea that easily lends itself easily to more complicated models, a problem I have illustrated below.\n  \nAs we start to get into more complicated models our interpretation methods slightly abandon this idea of localising our understanding and instead opt for completely new techniques, like permutation variable importance which I discussed in a previous post. Instead of inventing a new way to understand our models LIME tries to make the interpretation more “localised” in the same way that flexibility “localised” the model itself."
  },
  {
    "objectID": "posts/LIME/index.html#how-does-it-work",
    "href": "posts/LIME/index.html#how-does-it-work",
    "title": "Baby Teeth are Temporary, Model Interpretability is Forever",
    "section": "How does it work?",
    "text": "How does it work?\nThe main idea of LIME is the same main idea of calculus, which is if we zoom in a bunch we can approximate crazy non-linear functions with straight lines. These approximations are pretty good around that point, but get worse the further we move away. The way it works is actually quite simple and can be broken down into a handful a simple steps. 1. Make a localised dataset based on a single observation 2. Build a model on this localised dataset 3. Interpret that model. Some of the technicalities of the process change depending on the type of data we have (tabular, images or text) and I will go through each of them, but in essence, the idea is the same. I’m going to walk through trying to predict cavities based on the three types of data to illustrate the process of LIME, but keep in mind, I’m assuming we already have some trained model that is making prediction, and a set of test observations. Is crushing pain, blackening teeth, or an exclusive diet of raspberry cordial a predictor of cavities? Lets find out.\n\nTabular Data\nThe first method we are going to look at is tabular data. Lets say instead of doing normal dentistry work my dentist wants to predict if I have cavities based on how often I say I brush my teeth, and how much sugar I eat a day. This is a hypothetical world and my hypothetical doctor is omnipotent apparently. He wants to classify his patients into 3 levels based on the financial commitment they are probably about to make to his family practice. He puts my teeth brushing and sugar intake into the model, and it looks like his family should start packing their swimmers, because they are about to go to Hawaii. But how did the model come up with the prediction? In enters, LIME.\nIllustrated below (and explained in this paragraph) is the process LIME will go through to understand this classification. First we select a single point to run our model on, in this case, me or an observation very close to me. Then LIME will generate some “pretend” data around it according to independent Gaussian distributions. As a side note, this means it ignores correlation, and can generate some points that are unlikely to occur in the real data. Then LIME will run our real data point point and all its fake friends through the black box model and find their hypothetical predictions. Similar to LOESS models, the observations are then reweighted based upon their distance to to the initial(only real) data point. Remember, we aren’t trying to understand the model overall, we are only interested in the area surrounding our point. Now, on our fake, weighted data, we train an easy to interpret model. Something like a tree model or linear regression. It doesn’t have to be even slightly similar to the black box model we are analysing, all that matters is that it is a model that is simple, easy to understand and easy to explain.\n  \n\n\nImages\nSo my dentist is rubbing his hands together when my mum brings me in for a check-up. Once again ignoring normal dental procedures (I’m starting to wonder if this man is a dentist or some back alley scam artist my parents dug up to teach me a lesson) the dentist decides to take a normal photo of my teeth and predict the probability I have a cavity. His picture based model also suggests cavities, but once again, how did it make that decision? LIME is back to his rescue.\nOnce again we select some observation from our dataset, in this case, a photo of my sad decaying teeth. Next, following the tabular method, we would want to create a fake dataset of similar observations, but this is where we run into our first problem. Distance is easy to see in tabular data, its our normal run of the mill Euclidean distance. But how do we define distance for pictures? What metric can we use to say how similar two pictures are. This isn’t a question LIME answers, or even tries to answer but the little algorithm that could does it’s best to work through it. On pictures, rather than making our fake observations a sample that is “close” to the observation in distance, it varies the “superpixels” of the image. Superpixels are just a group of pixels that are next to each other and look similar so they are grouped together. for example, if you had a picture of my face; my skin, hair, lips, etc. would each be their own superpixel. To make our new dataset, LIME turns random super pixels off to create our local dataset. i.e. the pixels in that group cease to exist, are changed to 0, become a black hole of lost information in the land of data. Now we have a bunch of pictures that we run through the black box model to get some cavity prediction. Once again a simple model (like a linear regression) is built using the superpixels as inputs and the probability of a cavity as an output. The image is coloured by LIME based on having a positive impact on the classification or a negative impact.\n  \n\n\nText\nFinally after my dentist(?) finishes his dental(?) work, he decides to predict the chance of an expensive revisit based on my conversation with my Mum on our way out. This is a simple classification problem again and the model predicts I will be back with an expensive cavity. Finally, the dentist(??) implements LIME one more time.\nThe method for text is almost identical to the images, only instead of superpixels, it turns words off and on."
  },
  {
    "objectID": "posts/LIME/index.html#limitations-of-the-method",
    "href": "posts/LIME/index.html#limitations-of-the-method",
    "title": "Baby Teeth are Temporary, Model Interpretability is Forever",
    "section": "Limitations of The Method",
    "text": "Limitations of The Method\nThe obvious problem with LIME is the same thing that made it a good idea, and the same reason some people think the earth is flat. If we zoom in too much, we lose sight of the big picture. Since our understanding is limited to single real observation from our dataset, and running it on every observation would be computationally painful, it is at our discretion which observations, and how many observations we run LIME on to understand what is under the hood of a black box model. While I only went through a general understanding of how the process works, there are other posts out there that discuss practical implementation of the model and some of the more technical aspects of how it works which are certainly worth a read."
  },
  {
    "objectID": "posts/LIME/index.html#in-conclusion",
    "href": "posts/LIME/index.html#in-conclusion",
    "title": "Baby Teeth are Temporary, Model Interpretability is Forever",
    "section": "In Conclusion",
    "text": "In Conclusion\nIf something is called “pink juice” it will give you cavities, and if your dentist uses machine learning algorithms instead of normal dental practices, he might not be an actual dentist.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.— title: “Untitled” editor: visual —"
  },
  {
    "objectID": "posts/ConvolutionalNeuralNetwork/index.html",
    "href": "posts/ConvolutionalNeuralNetwork/index.html",
    "title": "Mario Party: Destroyer of Friendships and Explainer of Convolutional Neural Networks",
    "section": "",
    "text": "This is The Blog of a Mario Party Master\nIn pre-COVID times, my friends and I would gather around for a fun activity called “lets ruin our friendship by taking Mario party way too seriously”. The night always starts with laughter and few drinks, and ends with me standing on a chair, pointing at my closest friends, and screaming “I’m going to make you cry tears you thought were reserved for the death of your mother”. Once the moment has passed it seems a little dramatic, but at the time, we all truly believe that the speed you can get virtual candy out of a jar is an appropriate measure of human worth.\n\n\n\n\nThe last thing my friends see before I block them\n\n\n\nThere are several games that cause spite, but one called “Absent Minded”, pictured below, always sends us into an argument. Basically, you have 3 characters, and a slowly appearing image, and you have to find out which character is absent as the pictures become clearer. The faster you correctly identify, the more points you receive. I have never lost the game. Additionally there are 3 levels of this mini game, and so 3 different ways the images are shown to you: Jumbled Pictures, Blurred Pictures, and One At a Time.\n  Example: the “One At a Time” level \nNow, obviously the levels are meant for humans to play, and not for teaching machine learning, but the challenge each level presents gives us an interesting way to view the concepts. The jumbled picture level can show us how our typical machine learning algorithm will view an image. The blurred picture level shows the benefit of using convolutional neural networks, and the one at a time level can go in the trash! Sorry, not every analogy will fit perfectly into a machine learning theory.\n  \n\n\nHow Does The Picture Look to a Computer\nBefore I jump into explaining the concepts, I want to explain how your computer will “see” your image. Statistical models do not have eyes, and so for any picture we want to use, an observation needs to be converted in to a dataset. The process is illustrated below (although the variable size would be each pixel, and not limited by how small I can make my handwriting).\n  \nFirst our image is broken up into its individual pixels. For greyscale they are typically given a single number to represent its “darkness”, and colour images are given three different values for red, green, and blue (RGB). This dataset is what will be used to represent your model (although I will use the image rather than the dataset for visualisations).\n\n\nPart One: The Jumbled Picture Level\n\nTheory: What’s Wrong With What We Already Have\nTechnically, we could use any other machine learning algorithm to classify an image. We can call these “bag of pixel” methods as they don’t consider the pixels location in the image. They essentially cut the image up into pixels, shake them up in a bag, toss them out, and classify based off these values. Ultimately, the problem with any “bag of pixel” model, is that it fails to capture the shape of the image independent of its location. This means only images that have the right features in the right area are correctly classified.\n\n\nAnalogy: What Makes the Jumbled Level Hard\n  \nThe jumbled picture stage is interesting, because we cannot look at the characters as a whole to identify who is present. Since We cannot identify the pictures using the overall shape of the character, we need to look for the presence of independent key features. This reliance on identifiable features in the correct location is also what identifies our ordinary algorithms.\n\n\nPutting Them Together\nIn reality, this jumbling in our pictures would be at a pixel level, not big chunks, but the minigame is more of a point of illustration rather than a technical tool to understand machine learning. Key features being in the right place can be used successfully to identify images, but ultimately we have “zoomed in” too far to see the relationship between pixels. We can conceptualise this further with an example.\nIf we are trying to identify Mario, cap-looking pixels where Mario’s cap should be make it easy. If we have a picture where Mario doesn’t have his cap, that makes it hard. If we have a single picture where Mario is laying down so his cap is where his feet should be, that makes it even worse.\nThis is essentially the problem with our regular Machine learning algorithms. Key features in the right place make classification easy. No key features makes classification hard. Key features in uncommon places will be incorrectly assumed to be something else, and cause misclassification. This is where the benefits of using a convolutional neural network come in.\n\n\n\nPart 2: The Blurry Image Level\n\n\nTheory: How does a Convolutional Neural Network Work?\nBefore we return to explaining machine learning concepts in terms of Mario Party, lets take a step back, and look at how convolutional neural networks work in general. The illustration below is an overview of the structure of a CNN. The information contained in the image undergoes several transformations using layers that can be classified as either “feature extraction”, or “classification”.\n  \n\nFeature Extraction Layers\nFeature extraction is what really sets the CNN apart from other models. These layers make new variables that are more “computer vision friendly”. The process creates nodes that identify certain aspects of an image, such as Yoshi’s Shell or Peach’s hair, and converts them to variables we can use to make predictions. The most common (and technically interesting) layers used in the process are explained below. The “options” are specifically related to building CNN in the R package, Keras.\n\nConvolutional Layers\n  \nThe convolutional layer is what makes a neural network, a convolutional neural network. This layer creates a small window (called a kernel), that travels along the image and looks for a specific feature. The kernel_size option selects the size of the grid that is run along the image. Larger grids can overlook important details, while smaller grids can provide too much information and create noise. Typically the standard is somewhere in the range of a (3x3) grid. This information is taken from image to feature map using a filter. The filter is basically the type of feature we are looking for, when we run the kernel over the image. The number of times we do this, each with a unique filter, is the “depth” of the layer. In Keras, that is given by the filter option. As for which filters it uses, that is trained by the model.\nThe only convolutional layer that takes information from the image is the first one. All the following feature maps are computed on the previous feature maps. The new filter is applied to some combination of the previous layers feature maps and thus more convolutional layers mean variables that represent more intricate features.\n\n\nMax Pooling\n   Max Pooling is a step in our convolutional neural network that is essentially a dimension reduction of our feature maps. Literally just think of it as doing no transformation to the image, other than shrinking it down. As with all dimension reductions, the goal here is to get rid of the pixels that contain noise (e.g. white space) and keep the pixels that identify the image (e.g. Mario’s cap). This layer reduces our chance of overfitting, and thus is a key player in the bias and variance trade off in convolutional neural networks.\n\n\nHow does it work?\nJust like the original image, feature maps can be seen as a grid of pixels. Max pooling sections each feature map into smaller non-overlapping grids, takes the largest value of those pixels, and moves it on to the next layer. The example illustration above is looking for the darkest pixel on a 2x2 grid. Grid size is important, we want to minimise the bias introduced into the model by keeping the grid small, but also eliminate noise and not make the grid so small the layer does nothing.\n\n\nWhy the Maximum?\nMax pooling is a rather counter-intuitive layer, statistically speaking. Through practice, it seems that the maximum is the measure that minimises this information loss, rather than measures of central tendency as we would expect. As to why, the best analogy I’ve seen for the max pooling stage is from the data sceptic podcast. If you are looking for your keys, and everyone in the group says they don’t have them but one person, you aren’t going to take the median or average value. We are not looking for how much the picture looks like Mario’s cap on average, we are looking for any sign of Mario’s cap.\n\n\n\nClassification Layers\n\nDense\nA dense layer allows takes the nodes from the previous convolutional layers, and make a fully connected layer. This essentially takes our feature maps as inputs and runs them through a typically neural network, which we won’t go into detail about here. Our final classification layer is also a dense layer, that outputs the probabilities of each classification option. This is the final output of the model.\n\n\n\nDropout Layers\nUnlike the previous layers, dropout layers can be placed among the feature extraction or classification layers. In fully connected neural networks its use is quite simple; it samples random nodes to remove from the layer, which prevents overfitting. This interpretation does not follow for dropout layers placed among the feature extraction layers (the reason is matrix multiplication but its not worth going into) however it still helps prevent overfitting. Sometimes the number of pooling layers is limited by the resolution of the image (we can only max pool it to be so small) so if we need an additional measure against overfitting, we can include dropout layers.\n\n\n\nAnalogy: The Blurry Image Level\n  \nCircling back to Mario Party, the blurry levels difficulties are different to the jumbled level. Here, we struggle to make out key features, but must use the overall shape and colour to identify the character. As the image becomes clearer, it becomes easier to see, and we are more certain of our answer, however this additional certainty does not come without cost. The longer you wait to select an answer in the minigame, the more likely it is that you lose. This ultimately means that if the differences between characters are too subtle, the amount of time it will take to identify the difference isn’t worth the cost, and we are better off guessing.\n\n\nPutting It Together\nWaiting for certainty in the minigame is similar to overfitting in our convolutional neural networks. The deeper we go, the more likely it is that we overfit, and the more computationally heavy it is. We can add in dropout layers, but eventually there is an upper limit on the certainty we can have in our prediction. Unlike the normal models however, CNNs can capture shape AND the key features, they just need to be deep enough.\n\n\nMario Party and Convolutional Neural Networks: A Neat Analogy or Desperate Stretch to Make Statistics Seem Fun?\nObviously the CNNs have more nuance to them than can be explained using a Mario Party minigame, but it doesn’t do a bad job of giving us a conceptual understanding. Normal algorithms are limited by their inability to identify characters independent of their location, an issue we can circumvent using CNNs. CNNs capture the shape and general features of a character. Although really the most important learning experience from this post should be that if you come to my house to play Mario Party you might end up dead in a ditch somewhere.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "posts/boosting/index.html",
    "href": "posts/boosting/index.html",
    "title": "Learning Boosting Through Me Getting Fired from Tutoring",
    "section": "",
    "text": "I’ve had about… 13 jobs at this point in my life. Among them were jobs like tutoring, nannying, swim teaching, ect. so I have developed had a decent level of experience in teaching kids, specifically teaching them maths. While swim teaching doesn’t seem like it employs a lot of maths, I would play a “who can get closest to the number I’m thinking” game to decide who goes first. I then used it to explain game theory and how to optimise their strategy based on what the other kids would pick if they were acting as rational agents. They didn’t fully understand, but it was a fun exercise.\n\n\n\nI was never a very good tutor because I have a tendency to overcomplicate simple problems, and argue with the student’s teacher or parent. A recurring personality trait that is likely apparent after reading enough posts from this blog. The worst case of this was when I was fired from my tutoring job several years back. But what do my failures as a tutor have to do with boosting?.\n\n\nI have always seen boosting as the one of the most intuitive ensemble methods. For anyone who doesn’t know, an ensemble model combines many individual models to create one aggregate model that tends to have greater accuracy and less variance than any of the individual models. Think of it as the machine learning version of everyone voting to make a decision instead of a single expert making a decision. If we relate them to human studying, boosting is like doing every question at least once and then only revising the questions we previously got wrong. This makes boosting more similar to the way I study, and try to teach my (previous) tutoring students. In machine learning, boosting builds the models sequentially where each new model is built on the residuals of our current model, and only takes a small amount of predictive power from each (known as the learning rate). To see how this in action, lets look at the animation below.\n\n\n\nHere, I have made a boosting model using 50 regression trees that each consist of a single split, and have a learning rate (how much information we take from each tree) of 0.1. The colour represents the value for y. In the background we have the current predicted values for that area, and the actual data we are working with in the foreground. The size of the data represent the current error for that observation. It is pretty apparent that the data points become smaller as the background (predicted value) more closely resembles our training data. Each dashed line indicates the most recent regression tree (or in this case stump) that has been added to the model. Since this is a model that progressively learns, both the error and prediction change as we incorporate more and more models. Now that we have a visual on how boosting works, lets talk about tutoring."
  },
  {
    "objectID": "posts/boosting/index.html#part-1-focusing-on-mistakes",
    "href": "posts/boosting/index.html#part-1-focusing-on-mistakes",
    "title": "Learning Boosting Through Me Getting Fired from Tutoring",
    "section": "Part 1: Focusing on Mistakes",
    "text": "Part 1: Focusing on Mistakes\n\nIf you get 100%, You don’t need tutoring.\nThe interaction that got me fired from my tutoring company was with a kid I’ll call, Riley. After being begged to take him on as a student (he was a 5th grader and I teach high school maths) they sent me the information I needed to teach Riley. The first was an email from his teacher that read like this: Hi Mrs Riley, I’m not sure why you are getting your son tutoring considering he has no trouble in class. I have nothing he needs to work on. Maybe the tutor could teach him next semester’s content, but then he would just be bored in class so I wouldn’t recommend it.” I think, great, not only does this kid not need tutoring, but his parents are actively going against his teachers advice. Not a good sign. Next I read a note from the last tutor. “I just bring a computer game or a worksheet for him to do, and then mark it” Double great. This comment was even worse. I was clear this kid had nothing to learn, so it didn’t matter what the last tutor did with him. A tutoring session of watching a kid do things they already knows how to do with no useful feedback can go completely unnoticed. You get the most “bang for your buck” focusing on your worst areas, as they are both the areas requiring the most improvement, and are forgotten the fastest. I incorporate this attitude to every aspect of my life. You can see how in the visual below.\n\n\n\nIf you are just revising things you already know with 100% accuracy, you are not learning.\n\n\nBuilding Models in the Residual Space\nIf we build an ensemble model that is 50 models, each identical and with perfect predictions, we get the same result as if we made one. This is just wasting computational power much in the same way Riley’s family was wasting money on tutoring. In boosting, since each model is built on the residuals of previous models, it is trying to make sure that it does not repeatedly learn things it already knows. The model focuses on the most common, frequent, and damning errors, and works its way back from that. In the first animation, I let the size represent the errors, but each model is not built using the response variable, it is built using the residuals. Here, using the exact same data and model above, I have instead animated each individual tree as it tries to predict the residuals.\n\n\n\nWe can see that when we start our boosted model, the residuals are essentially our y value (since the initial prediction for the whole area is 0), and as the main model becomes more accurate, the residuals become 0, and new trees don’t have any information to contribute to the model. If the model continued much further, it would just randomly build trees on the irreducible error.\nBy focusing on the residual space, the model ensures that we aren’t wasting computations by relearning something we already know. In a similar way, the best way to learn as a human is not to revise the areas we get 100% in, but rather the areas we are failing in as they offer the most room for improvement."
  },
  {
    "objectID": "posts/boosting/index.html#part-2-the-learning-rate-the-number-of-models-and-the-model-complexity",
    "href": "posts/boosting/index.html#part-2-the-learning-rate-the-number-of-models-and-the-model-complexity",
    "title": "Learning Boosting Through Me Getting Fired from Tutoring",
    "section": "Part 2: The Learning Rate, The Number of Models, and The Model Complexity",
    "text": "Part 2: The Learning Rate, The Number of Models, and The Model Complexity\n\nRash Decisions in Tutoring Is a Dangerously Simple Method\nWhen I arrive at Riley’s house, I explain I don’t have any computer games or worksheets because I disagree with them morally, however I could cover future school work and invent some fun questions. Riley’s mum was not a big fan of my moral plight to take down “big tutoring”. After a brief discussion about how “we are all a little disorganised” which everyone knows is mum code for “you are disorganised”, she sent me home. Later I received a call from my boss about being “ill-prepared” because I should have just brought computer games and worksheets like the last tutor recommended. I explained my side, and by boss was sympathetic, but I never got another tutoring job from them again. Unfortunately, due to Riley’s mum being unsupportive of trying new teaching methods, the best speed at which Riley should cover new content wont be found. He might have learnt better with longer sessions, or with another student, or doing literally anything other than playing computer games. Much in the same way that we can tailor the environment and complexity of a tutoring session, boosting can improve its predictions by changing the learning rate, number of models and the model complexity.\n\n\nTinkering the Complexity of the Boosting Model\nWhen using boosting, we need to be aware of how the learning rate (or shrinkage), the number of models and the model complexity impact our final prediction. The learning rate decides how much “predictive power” we take from each trees. Smaller learning rates need more models to get a working prediction, larger learning rates run the risk of giving too much power to outlier models, and missing minor complexities. The number of models (trees in our example) is just decided in parallel with the learning rate, and is essentially how much computational time we are willing to dedicate to our model. The depth of the tree is similar, in the sense that with enough trees, a stump tree can capture any relationship, however if we don’t have the capacity for enough models, we can increase the complexity of each individual model to add more nuance to the final prediction."
  },
  {
    "objectID": "posts/boosting/index.html#part-3-need-to-know-when-to-quit",
    "href": "posts/boosting/index.html#part-3-need-to-know-when-to-quit",
    "title": "Learning Boosting Through Me Getting Fired from Tutoring",
    "section": "Part 3: Need to Know When to Quit",
    "text": "Part 3: Need to Know When to Quit\n\nOverfitting in Learning\nI know someone has spent too long studying when I see forum posts asking if some obscure topic is going to be on the exam. Once you have run out of things to focus on that are important, you start to focus on the things that are less and less important, until you are sitting awake at night crying about the sheer vastness of knowledge that you could never hope to learn. Knowing when to quit is an important part of life and machine learning. Most people tell other to “try try and try again” my motto is “if you aren’t feeling it, quit”. After several years of tutoring, I was no longer feeling it, and it was time to quit. It turns out repeatedly being told “the continuity of functions doesn’t matter” and “dividing a number by 0 is 0” my soul had been crushed and I wasn’t doing my job properly any more. I had too much baggage and it was time to quit. Just like with tutoring, boosting needs to know when to quit too.\n\n\nBoosting can Overfit\nUnlike in bootstrapping, boosting has the potential to overfit. Since the later predictions are the cumulative prediction of all the models that came before, and the new models are only concerned with what those models got wrong, the overall benefit of each model is less than the model before it. This means that eventually, the tangible benefit of building a new tree becomes zero. Because of this, we always need to be aware of our ensemble complexity and manually set a stopping criteria."
  },
  {
    "objectID": "posts/boosting/index.html#conclusion",
    "href": "posts/boosting/index.html#conclusion",
    "title": "Learning Boosting Through Me Getting Fired from Tutoring",
    "section": "Conclusion",
    "text": "Conclusion\nBoosting employs three techniques that make it similar to effective human learning. First it focuses on mistakes, secondly it is important to tailor the complexity of any one session, and finally it need to be manually stopped or otherwise your model will stare into the abyss of the unknowable in existential dread.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "posts/MLE/index.html",
    "href": "posts/MLE/index.html",
    "title": "How a 5th Grader Makes $1500 in a Single Night (and How They Could Make More Using MLE)",
    "section": "",
    "text": "As a child, your parents are seen as deities that can do no wrong, that is until you are doing a first aid course 10 years later and learn that a broken arm is not an “I’ll take you to the hospital tomorrow if it still hurts” level emergency. Growing up I started to realise my Dad’s life lessons were somewhat unorthodox and below are some of my favourite quotes.\n\n“If you are going to light fires with your brother, make sure you do it by the pool. The ground is paved and if you set something on fire I’d rather it be you than the house”\n“I’m not going to any of your soccer games until you turn 12. I watched your brother play when he was younger. It was very boring and the other kids parents were insufferable”\n“If someone wants you to do something, they will probably pay you for it. So make sure you get paid. Unless I ask, then you need to do it for free.”\n\nThe last quote was probably the worst thing he taught us, at least for his financial security. It meant my siblings and I learnt to squeeze as much money out of my parents as we could. They paid me to cut my hair, get to school on time, go to swimming lessons, nothing was ever done for free. I even haggled my baby teeth with my mum for $50. This idea expanded to school yard, where my peers were much poorer than my parents, but also easier to part from their money. In grade 2 I had a period of selling stickers outside the tuckshop for spare change. The profit system was simple, sell stickers to my peers for a 10000% mark up. This plan was eventually shut down by “the man” i.e. the staff because some parents had complained about their kids not even getting lunch and just buying stickers. My most effective and long lasting money making plan however, was birthday parties. By the end of middle school, I was pulling in about $2000 a party. One of the most important elements in making a birthday profitable was the ratio of kids invited to kids that turn up. At the time I did guesstimation on this ratio, but now we are going to look at it in a more formal manner, using Maximum Likelihood Estimation (MLE)."
  },
  {
    "objectID": "posts/MLE/index.html#step-1-identify-joint-density-function",
    "href": "posts/MLE/index.html#step-1-identify-joint-density-function",
    "title": "How a 5th Grader Makes $1500 in a Single Night (and How They Could Make More Using MLE)",
    "section": "Step 1: Identify Joint Density Function",
    "text": "Step 1: Identify Joint Density Function\nI mentioned in the beginning that the function that can easily cross between the worlds of the outcomes and parameters is the joint density function, but what is it? Basically it takes a bunch of random variables (in our case, students) and says what is the chance the entire group has a specific outcome (in this case, attend or not attend).   \nMoving from each students probability to the joint probability is not simple task. We can either create a model that understands the intricate inter-personal relationships of this second grade class, or we can make 3 assumptions that will greatly simplify the problem. Statisticians generally prefer the method that doest require reading children’s diaries and so we are now going to perform these assumptions on my second grade class.\n  \n\nAssumption 1: Independence\nThe first assumption we are going to make about our students is that they are independent, i.e any particular kid going to my party is not at all influenced by any other student. In doing so we now have a dataset consisting of children who are completely immune to peer pressure, both a school administration and statisticians fantasy. Unfortunately we have also lost some realism along the way. Why do this? Well, right now the only thing we know is each student has a some probability mass function (which we will get to in assumption 3) but no information on how the students interact with each other. What if one student coming means other wont come? or a students coming means another will certainly come? In order to find the probability of this very specific outcome we have ended up with we need information about the variables relationship. Here we can either figure out the complicated interpersonal relationships of the children, or assume they are all independent. With this assumption, the joint PMF is the product of each individual PMF (this is literally the definition of independence). Now our students don’t interact, and we have taken our first step in simplifying our problem.\n  \n\n\nAssumption 2: Identical\nNow our joint PMF is the product of 24 unique PMFs. The problem is, I don’t really care about the individual students (they are all dollar signs to me). I only care about the overall proportion of students. Here we can simplify our problem further by assuming there is some overall class PMF, and every student is just a random variable drawn from that. To use this assumption in our joint density function, we just say the probability of every student coming is the same. Now we have 24 observations drawn from a single distribution, which means we only need a single individual PMF to define the PMF of all the students.\n  \n\n\nAssumption 3: Identify The Distribution For Individual Parameters\nAs a final step, we still need some individual PMF to put in the big product we have created. Since every student either comes or doesn’t come, we can easily say the PMF for each student follows a Bernoulli distribution. Ultimately this step just depends on what outcome you want to measure, and since I only really care about a yes/no outcome, a Bernoulli will do just fine. Now we have a joint PMF to work with."
  },
  {
    "objectID": "posts/MLE/index.html#step-2-make-your-likelihood-function",
    "href": "posts/MLE/index.html#step-2-make-your-likelihood-function",
    "title": "How a 5th Grader Makes $1500 in a Single Night (and How They Could Make More Using MLE)",
    "section": "Step 2: Make Your Likelihood Function",
    "text": "Step 2: Make Your Likelihood Function\nWow what a beautiful joint PDF… What do we do with it? Well I said in the beginning that the function that gives probability of outcomes and the function that gives probability of parameters is the same function just with a different unknown. Here are the two directions we could take with our joint PMF. \nSince in this case our unknown is the parameter, we are going to use the likelihood function. Here we can actually put find the Likelihood function for our particular birthday party results.\n\\[\nL(\\theta)=\\theta^{18}(1-\\theta)^{6}\n\\]\nBut to simplify it here with the outcomes wouldn’t be an accurate representation of how we usually have to conduct MLE. So I’m going to leave in the product notation. Now that we have a function that shows how likely different values of \\(\\theta\\) (the probability a student turns up) are, we need to find its maximum."
  },
  {
    "objectID": "posts/MLE/index.html#step-3-math-time-logs-and-derivatives-and-more-derivatives",
    "href": "posts/MLE/index.html#step-3-math-time-logs-and-derivatives-and-more-derivatives",
    "title": "How a 5th Grader Makes $1500 in a Single Night (and How They Could Make More Using MLE)",
    "section": "Step 3: Math Time : Logs and Derivatives and More Derivatives",
    "text": "Step 3: Math Time : Logs and Derivatives and More Derivatives\nI mostly wanted to focus on the difference between a PMF/PDF and a likelihood function in this post, but for the sake of completeness I’m going to finish the estimation. That being said I’m not going to be very detailed. Our next step in the process is to take the log.\n\nWhy take the log?\nThe answer is really just mathematicians are lazy. From high school you may remember that when you want to find the maximum of a function you take the derivative and set it equal to 0. The thing is, we have a massive product right now, and the product rule is a pain to do. Especially when we have 24 functions multiplied together. Thanks to log laws, taking the log of this nightmare function both doesn’t change the value the maximum is at (thanks to some math stuff I won’t go into) and also means we have to take the derivative of a big sum instead of a big product, which is really easy.\n\\[L(\\theta)=\\prod^{24}_{i=1}\\theta^{x_i}(1-\\theta)^{1-x_i}\\]\nI’m going to do some cosmetic changes before applying the log.\n\\[L(\\theta)= \\theta^{\\sum_{i=1}^{24} x_i}(1-\\theta)^{24-\\sum_{i=1}^{24}x_i}\\]\nThen we have our log-likelihood.\n\\[logL(\\theta)= log(\\theta)\\sum_{i=1}^{24}x_i+ log(1-\\theta)(24-\\sum_{i=1}^{24} x_i)\\]\n\n\nThe first derivative\nNow we take the first derivative. When our likelihood function has a rate of change of 0, it’s about to fall back down. So we take the derivative with respect to the value we want to maximise and find the parameter that is the most likely given our set of outcomes.\n\\[logL'(\\theta) = \\frac1\\theta {\\sum_{i=1}^{24} x_i}- \\frac1{1-\\theta}(24-\\sum_{i=1}^{24} x_i)\\]\nSince the first order condition is that we would like the first derivative to be equal to 0, this is where I put the hat in because this isn’t true in general, only for our estimate.\n\\[\\frac1{\\hat{\\theta}} {\\sum_{i=1}^{24} x_i}- \\frac1{1-\\hat{\\theta}}(24-\\sum_{i=1}^{24} x_i)=0\\]\nWhich we solve to find\n\\[{\\hat{\\theta}} = \\frac1{24}\\sum_{i=1}^{24} x_i\\]\nNow that we have the solutions we can substitute in our values from our sample of party go-ers and get the probability any one person will turn up.\n\\[{\\hat{\\theta}} = 0.75\\]\n\n\nThe second derivative\nThe lazy of us ignore this step, although it is technically still important. I also tend to ignore it, and will do so here for the sake of brevity. Whoops. We already have our estimate, this is more about making sure we have a clean solution. Taking the second derivative ensures our estimate is a maximum, and not some other stationary point."
  },
  {
    "objectID": "posts/MLE/index.html#conclusion",
    "href": "posts/MLE/index.html#conclusion",
    "title": "How a 5th Grader Makes $1500 in a Single Night (and How They Could Make More Using MLE)",
    "section": "Conclusion",
    "text": "Conclusion\nI used all the money I made from birthday parties to buy about $10 000 worth of manga books because I was a huge Weeb. Sadly I ended up donating them all to the school that expelled me in year 11. Turns out being mercenary enough to make buckets of money as a child doesn’t matter if you waste it all on books you are forced to give away when you move to Melbourne because student accommodation doesn’t come with a wall of free library space. I’m sure there is a lesson in here somewhere.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "posts/regularisation/index.html",
    "href": "posts/regularisation/index.html",
    "title": "Trying to Save My Housements (Specifically Tom) from Alcohol Poisoning with Regularisation",
    "section": "",
    "text": "Back when I lived with my sister I barely managed to drink once a month, and health wise, I was living a good life. Unfortunately, my sister decided to get engaged and her fiance was all “its weird that we live with your hermit sister” and “you two have been co-dependent for years its unhealthy and it’s time to stop”. When I moved in with my friends at the beginning of the year I was immediately tossed in to a long strict lock down. I had to deal with living in a house of people I liked, all of which had no schedule and were all bored out of their minds, so, to cut a long story short, we all started drinking a lot. I have since significantly cut back (I have gone back to my original “barely once a month” amount), but during our trial period for high functioning alcoholism, our friend Fynn introduced us to a “guess your uni grades” drinking game. Here is how it works:\n\nGuess your mark for all the units you are taking this semester\nWhen you get your results, calculate the difference between your guess and your actual result. e.g. if you guess 51 for Topology and actually get a 73 because the exam was scaled, you get 22 points.\nTake your points for each unit and sum them up. e.g. If topology got you 22 points, data structures 7 points, and a project unit was 3 points, your total is 32 points.\nIf you did not do a 4 unit work load, you scale your points up and round to the nearest integer to match that. e.g. if you had 32 points and did 3 units, your scaled score is 43.\nThe number of points you have is the number of drinks you have to do.\n\nFynn’s version is proper shots. Unfortunately because the example case was based on my housemate Tom, who apparently has next to no idea how he is doing in his courses, we had to change our rules to shots of mixed drinks. Even with this change we calculated that there was still a chance Tom would be killed by alcohol poisoning. After a 3-1 house vote we agreed we were willing to make that sacrifice for the sanctity of the game. My housemates in order of least to most points were:\n\nZac with 4 drinks\nMe with 13 drinks\nEm with 17 drinks\nTom with 43 drinks\n\n\n\n\nA visualisation of both the guessing and social order of the house. Here, Tom has died of alchohol poisoning.\n\n\nThis game led into a discussion about whose grades are the most difficult to predict. For example, there are things that seem to make guessing easier, such as completely in semester units. While things like my and Tom’s history of high variance puts us at a natural disadvantage. The first step to understanding what affects our changes in grades is to predict them. Figure 1 below gives a basic visualization of the house’s grades. The x axis represents the year and semester, however it is mostly arbitrary.\n\n\n\n\n\n\nThe house’s grades for every semester of uni.\n\n\nLooking at this plot we can visually see some of our personality quirks in the data. This plot makes it rather obvious which semester I had a mental breakdown and was diagnosed with ADHD. Em’s consistently high marks show the benefit of grade based anxiety, and the slight drop at the end shows the trade off that occurs when you start looking after your mental health. Zac’s grades all sit around the same range, because despite being a reliably high achiever, he over commits to extra-curricula and often hands assignments in several days late, which essentially stops him from getting higher than an 80. Tom has no reason for being the way he is.\nWe want to try and improve our predictions by building a model that forecasts next semesters results. Fynn’s house had a total of 69 drinks, while we had 77 and losing to another household (especially one as inferior as Fynn’s) is a blight on our competitive record. The problem with building a model is that there is very little data here, especially when compared to the number of variables at hand. This means even something as simple as a linear regression will have too much flexibility and will likely over fit, so to fix this, we need to turn to regularisation."
  },
  {
    "objectID": "posts/regularisation/index.html#what-is-regularisation",
    "href": "posts/regularisation/index.html#what-is-regularisation",
    "title": "Trying to Save My Housements (Specifically Tom) from Alcohol Poisoning with Regularisation",
    "section": "What is regularisation?",
    "text": "What is regularisation?\nRegularisation is essentially a systematic way to reduce the variance and increase the bias of a model and improve the overall error through the bias variance trade off. There are quite a few regularisation methods, but I’m not going to go through all of them here. Rather I have summarised three of the more common techniques below.\n\nSubset selection: This technique selects a subset of the predictors to use in the regression. There are three common types of subset selection: forward subset selection, backward subset selection, and best subset selection. Forward subset selection starts with the null model and, at each step, adds the variable that reduces the test error the most, until the model is at a point where the addition of new variables don’t improve the the test error. Backward subset selection does the same but in reverse, it starts with the model containing all the variables and removes predictors until the test error does not improve. Best subset selection makes every possible model (the power set of the predictors) and chooses the one with the minimum error, however this can also over fit and is often computationally infeasible.\nPCA regression: You may remember principal component analysis (PCA) from one of my previous posts as an unsupervised learning method, but it can also be used as a regularisation technique. Using the principal components (which are just linear transformations of the original predictors) as predictors in an OLS regression can reduce the variance of the model.\nShrinkage methods: these methods make a new error function that is the sum of the residual sum of squares (RSS) and a penalty term and selects coefficients by minimising this new error function. The two main methods are lasso, which minimises the function \\(RSS + \\lambda\\sum_{j=1}^{p}|\\beta_j|\\), and ridge which minimises the function \\(RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\\), where \\(\\lambda\\) is a tuning parameter. These additional penalties force the coefficient estimates to be closer to 0.\n\nThe method used in the example, and the main focus of the rest of this post, will be the shrinkage methods, as they have the most interesting theory and haven’t been explained previously on the blog. Now that we have seen how we perform regularisation, this still leaves the question why it works. There are two main benefits to regularisation, lower error and better model interpretability. I will explain how each of them work below.\nThe first reason to use regularisation is to reduce the variance of our model. Often, we do this implicitly by choosing a simple model due to a lack of data. For example, if we had built a neural network and found that the model had too much variance, we could instead build a random forest as a less flexible alternative. Regularisation is used when our model is already as simple as it can be, e.g. a linear regression or LDA in the case of classification, and it still has too much variance. We can’t get more data, and to remove a level of flexibility from a linear regression would be to predict the average (a constant). Regularization allows us to reduce this error from variance by further restricting the model parameters and thus allowing a model that is even more inflexible than a normal least squares linear regression.\nThe second reason to use regularisation is to improve the interpretability of the model. A large number of unnecessary variables not only introduces error, but also complicate the model. The benefit of using regularisation to improve model interpretability stems from the idea that there are signal and noise variables and we want to keep the signal variables while removing the noise variables. Regularisation removes predictors that have a spurious relationship to the response variable and leave us with less coefficients to interpret."
  },
  {
    "objectID": "posts/regularisation/index.html#example-do-the-grades-need-a-regularised-model",
    "href": "posts/regularisation/index.html#example-do-the-grades-need-a-regularised-model",
    "title": "Trying to Save My Housements (Specifically Tom) from Alcohol Poisoning with Regularisation",
    "section": "Example: Do the grades need a regularised model?",
    "text": "Example: Do the grades need a regularised model?\n\n\n\n\n\n\nTechnically we don’t need a reason to run a regularised model, it is just another method we can use to balance the bias and variance trade off, but in cases where there is a very small amount of data it is more useful to do than not. In this example we want to predict the grade of each unit in the up coming semester using several factors, such as student, department, whether the unit was in semester 1 or 2 (I suspect we do worse in semester 2 due to burn out), the level of the unit (most of us should do better in first year units), whether the unit was online, etc. There are also several interaction terms that could be useful, for example an interaction term between the Harriet student dummy and the online semesters would capture the later jump in my grades. There are obviously more interesting (and useful) variables we could include, such as whether we needed to move house during semester, if we went on a holidays during midsemester break, if we were seeing a therapist, etc. These variables would likely produce a better prediction and more easily interpreted coefficients, however I’m going to keep the model simple and leave it as is. Once we have our data set we can immediately see two reasons to use a regularised model over a normal OLS.\nFirst of all, the matrix is full rank, that is, we have variables that are a linear combination of other variables in the data set. For example, Tom and I are the only two students who take maths units (MTH), so with the student other department variables, the MTH variable becomes obsolete. There are several other variables with this issue. I’m not sure which variables are better to keep (department or student) and this issue will likely get worse as I add interaction terms.\nSecond of all, with such a small data set, any model with more than a handful of predictors will have a large amount of variance. Figure 2, below, shows the test and training error of a simple linear model that’s flexibility has been increased (somewhat arbitrarily) with the addition of new predictors. In this plot, a 0 in flexibility indicates a model that predicts the mean, while an 8 indicates a model that contains all the predictors in the data set as well as every every interaction term. This plot only shows the change in mean squared error (MSE) over a single sample of the data. To see the MSE of the model over several samples (and properly assess the variance of each model) we should do some resampling.\n\n\n\nThe trainning and test error compared with model complexity.\n\n\nFigure 3 shows the density of the test and training error of 50 samples of:\n\na basic linear model which predicts the mean of the training set for all test observations\na simple linear model which is an OLS model with only a handful of predictors I selected\na complex linear model which is an OLS model with every predictor and their interaction terms.\n\nThis gives us a cross validated version of Figure 2, and confirms what the previous plot indicated. First of all, it shows, a basic model has slightly too much bias because the training and test error are, on average, higher than the error of the simple model. It also shows that the complex model has over fit the data, given its consistently low training error and high unpredictable test error. We need a model that is somewhere between the complex “every variable” model and a constant. To find this model, we will use regularisation, specifically a shrinkage method.\n\n\n\nDensity plots of the training and test error of three linear models that differ in flexibility."
  },
  {
    "objectID": "posts/regularisation/index.html#shrinkage-methods",
    "href": "posts/regularisation/index.html#shrinkage-methods",
    "title": "Trying to Save My Housements (Specifically Tom) from Alcohol Poisoning with Regularisation",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nThe most common regularisation methods are ridge and lasso regressions. Lasso and Ridge follow the same general idea, which is to put additional restrictions on the coefficients of a linear regression, they only slightly differ on how they go about it. Lasso, will minimise \\(RSS + \\lambda\\sum_{j=1}^{p}|\\beta_j|\\), and ridge will minimiser \\(RSS + \\lambda\\sum_{j=1}^{p}\\beta_j^2\\). The turning parameter \\(\\lambda\\) decides how much the penalty term influences the final coefficient values. A large value of \\(\\lambda\\) means the penalty term outweighs the RSS and coefficients are estimated at 0, a small value of \\(\\lambda\\) means the penalty will not be factored in at all and the model will return the OLS coefficient estimates. Figure 4 shows a contour plot of the lasso penalty, RSS function, and lasso error term for a two variable model. The animation shows the contours of the lasso regression look more like the contour plot of the penalty term as \\(\\lambda\\) increases. In turn we can see the minimum value of the error function (and thus the estimated coefficients) moves from the OLS estimates (the minimum of the RSS) to 0 (the minimum of the penalty).\n\n\n\n\n\n\n \nWhat may not be clear from this animation, but does simplify our ability to visualise how this adjusted error works, is that for every value of \\(\\lambda\\) there is some value of \\(s\\) such that we are minimising the RSS subject to the constraint \\(\\sum_{j=1}^{p}|\\beta_j| \\leq s\\) in the case of lasso and \\(\\sum_{j=1}^{p}\\beta_j^2 \\leq s\\) in the case of ridge. This means that instead of trying to think about a complicated constantly changing error function, we picture our restraints as shown in the illustration below. Here I have drawn a contour plot of a hypothetical RSS for a two variable model. The plot on the left has the ridge regression constraints drawn over it, while the plot on the right has the lasso constraint. The size of the circle/diamond is related to the tuning parameter \\(\\lambda\\). When \\(\\lambda=0\\) the area of the circle/diamond is infinite, and when \\(\\lambda \\rightarrow \\infty\\) the circle/diamond becomes so constrained it forces every coefficient to 0. This allows us to see how the constraint impacts the selected coefficient estimates.\n\n\n\nAn illustration of the difference between the ridge and laso regression constraints\n\n\nSomething that is important to note is that lasso regression is more likely to set coefficients to 0 (and thus more likely perform feature selection) than ridge due to the diamond shape of the constraint. The minimum RSS value in figure 4 showed this in practice, as the minimum coefficient estimate quickly set \\(\\beta_1\\) to 0 before further restricting \\(\\beta_2\\). Most commonly we will visualise the way the coefficients change as \\(\\lambda\\) increases with a plot of the coefficient vs \\(\\lambda\\) values, as drawn below.\n\n\n\nAn illustration of how the coefficients change as lambda increases.\n\n\nThere is one final question we need to answer before we move on. How do we decide whether to use ridge or lasso regression? Well, if you think all the variables are relevant, use ridge regression, if you suspect some variables to just be noise, use lasso. Now, with an understanding of how shrinkage methods work, we can go back to our example."
  },
  {
    "objectID": "posts/regularisation/index.html#predicting-the-grade",
    "href": "posts/regularisation/index.html#predicting-the-grade",
    "title": "Trying to Save My Housements (Specifically Tom) from Alcohol Poisoning with Regularisation",
    "section": "Predicting the Grade",
    "text": "Predicting the Grade\nLets apply this theory to our grades model to see if we can improve our predictions. Some of the variables are linear combinations of others, so there is absolutely no need to keep all the predictors. This means we should opt for lasso over ridge regression, although this does have one downfall. This example has a large number of interaction terms, and when we include interaction terms, we typically need to maintain a hierarchy so our variables are interpretable, e.g. we need to keep the Harriet and the Online dummy variables in the model if we want to include the Harriet:Online interaction term. Ridge and lasso regression do not follow this hierarchy when they shrink variables. Usually this would make predictability worse, however since every single predictor in this data set is a dummy variable, it isn’t going to cause (too) much of an issue. The main problem will be having almost no idea what the base line model is. From this point forward we will mostly focus on the improvements in test error, and continue with the lasso regression.\nTo find our lasso model, we need a \\(\\lambda\\) value. The best way to find this value is with cross validation, and thankfully the glmnet package does this for us. Figure 5, below, shows the mean test MSE and 95% confidence interval of the lasso regression for several values of \\(\\lambda\\). The vertical dotted line indicates the \\(\\lambda\\) value that minimises the model error.\n\n\n\n\n\n\nSelecting our lambda value with the glmnet package’s cross validation method.\n\n\nWe can also visualise how our coefficients change as \\(\\lambda\\) increases. Figure 6 shows the change in the model coefficients as we allow \\(\\lambda\\) to approach 0 (or our L1 Norm to get infinitely large as shown on the x axis). The dashed line indicates the model associated with the \\(\\lambda\\) value found from cross validation. This allows us to better understand how some coefficients interact with each other. For example the Harriet:online interaction is the largest coefficient in every model, regardless of the \\(\\lambda\\) value, which indicates it is a consistently useful variable.\n\n\n\n\n\n\nThis plot shows the impact on our variables of a decreasing lambda (and thus increasing L1 norm).\n\n\nThe model that contains every variables as well as every student, unit level, department and online interaction term has 54 variables, the regularised model has only 20 variables, so there has been some serious culling. Figure 7 shows the predictors that made it into the final model. Since the baseline model (the one that we compare each dummy variable to) is now a mess, these coefficients are almost impossible to interpret.\n\n\n\n\n\n\nThe lasso model coefficients.\n\n\nFinally, we can compare the lasso model to the basic, simple, and complex models from figure 3. Figure 8 compares the cross validated RMSE of the three old models and the new lasso model. We can see that the simple model (that was just the student and online variables as well as all their interaction terms) may slightly outperform the lasso model, however there is so much overlap in the confidence intervals it is hard to say. In this example, the lasso model did not select variables that were better than my general intuition. Lasso can help you regularise to some degree, but even regularisation techniques can be given too many predictors to choose from, and it seems my intuition was enough to beat it this time.\n\n\n\n\n\n\nThe RSME of the final lasso model when compared to the previous models over several resamples."
  },
  {
    "objectID": "posts/regularisation/index.html#conclusion",
    "href": "posts/regularisation/index.html#conclusion",
    "title": "Trying to Save My Housements (Specifically Tom) from Alcohol Poisoning with Regularisation",
    "section": "Conclusion",
    "text": "Conclusion\nRegularisation can be used to reduce the variance and improve the interpretability of our model, but human intuition can still outperform it if we know enough about our data. That being said the models for our grade predictions turned out to be useless. Results for this semester have been released and Tom was 32 off, while the simple model was 55 off. Really, the lesson here is that no model, no matter how fine tuned, can predict a response variable that has an inexplicably high variance."
  },
  {
    "objectID": "talks/rladies/index.html#how-do-scagnostics-work",
    "href": "talks/rladies/index.html#how-do-scagnostics-work",
    "title": "R-Ladies Meet-up",
    "section": "How Do Scagnostics Work?",
    "text": "How Do Scagnostics Work?\nTake this “Ring” scatter plot…\n\n\n\nSo, lets see how the scagnostics are caulculated by looking at this “ring” shaped scatter plot."
  },
  {
    "objectID": "talks/rladies/index.html#how-do-scagnostics-work-1",
    "href": "talks/rladies/index.html#how-do-scagnostics-work-1",
    "title": "R-Ladies Meet-up",
    "section": "How Do Scagnostics Work?",
    "text": "How Do Scagnostics Work?\n… and strip away everything but it’s shape\n\n\n\nThe first thing we do is remove the numbers and just look at the points in relation to each other\nfrom here we want to make several objects that represent the scatter plot’s shape,"
  },
  {
    "objectID": "talks/rladies/index.html#how-do-scagnostics-work-2",
    "href": "talks/rladies/index.html#how-do-scagnostics-work-2",
    "title": "R-Ladies Meet-up",
    "section": "How Do Scagnostics Work?",
    "text": "How Do Scagnostics Work?\n… then build the graph based objects\n\n\n\nthe convex hull which is the shape we would get if we stretched a rubber band around the outside of the ring\nthe alpha hull which is made by outlining the shape\nand the MST which is made by connecting every point up using as little edges as possible\nwith these three objects we can define our scagnostics"
  },
  {
    "objectID": "talks/rladies/index.html#the-scagnostics",
    "href": "talks/rladies/index.html#the-scagnostics",
    "title": "R-Ladies Meet-up",
    "section": "The Scagnostics",
    "text": "The Scagnostics\n\n\n\nConvex and Alpha Hull Measures\n\nConvex\nSkinny\n\nAssociation Measures\n\nMonotonic\nSplines\nDcor\n\n\n\n\nMST Measures\n\nOutlying\nClumpy*\nStriated*\nSparse\nSkewed\nStringy\n\n\n\n\n\n\nThese are the scagnostics that are in the cassowaryR package, all of which have previously been defined in scagnostic liteature\nThey are sorted into their three groups depending on which graph based object they use\nThose with an astrix have two versions, a calculation that was defined in “Scagnostic Distributions”, a paper by Leland Wilkson and Graham Wills, as well as a new adjusted version that was created by us to solve some issues with binning that I will get to later.\nTo understand how we convert those graph based objects to scagnostics are calculated it helps to break it down\nso I will go through three of the scagnostics in a bit more detail"
  },
  {
    "objectID": "talks/rladies/index.html#the-scagnostics-1",
    "href": "talks/rladies/index.html#the-scagnostics-1",
    "title": "R-Ladies Meet-up",
    "section": "The Scagnostics",
    "text": "The Scagnostics\nConvex uses the alpha hull and convex hull\n\n\n\nFirst up we have the scagnostic convex. This is a hull based measure, that is the ratio between the alpha hull and the convex hull"
  },
  {
    "objectID": "talks/rladies/index.html#the-scagnostics-2",
    "href": "talks/rladies/index.html#the-scagnostics-2",
    "title": "R-Ladies Meet-up",
    "section": "The Scagnostics",
    "text": "The Scagnostics\nOutlying uses the MST\n\n\n\nSecond is outlying. This is an example of a measure that uses the minimum spanning tree\nFirst it identifies the outlying points and the length of their edges, then it calculates how much of the total MST length is due to these outling edges"
  },
  {
    "objectID": "talks/rladies/index.html#the-scagnostics-3",
    "href": "talks/rladies/index.html#the-scagnostics-3",
    "title": "R-Ladies Meet-up",
    "section": "The Scagnostics",
    "text": "The Scagnostics\nSplines using the original data\n\n\n\nFinally we have splines. This is an example of an association measure, so it take in the raw data\nit calculates two splines models, one with x as a dependent variable and one with y as the dependent variable\nif either of these splines models have very low variance in their residuals, the splines scagnostic will be high"
  },
  {
    "objectID": "talks/rladies/index.html#assessing-the-scagnostics",
    "href": "talks/rladies/index.html#assessing-the-scagnostics",
    "title": "R-Ladies Meet-up",
    "section": "Assessing the Scagnostics",
    "text": "Assessing the Scagnostics\n\nAll are on a uniform scale: 0-1, where 0 is low and 1 is high\nThe ordering by scagnostic value hopefully matches how we perceive the structure, but it doesn’t always\nScagnostics hopefully identify different features but some are correlated with each other\n\n\n\nThere are a couple rules for the scagnostics, they aren’t just a free for all\nso, as well as defining several scagnostics, the scagnostic distributions paper I mentioned earlier also specifies three main rules these measures must follow.\nFirst they should all be on a uniform scale so they are directly comparable\nSecond, each scagnostic should order a set of scatter plots in a way that lines up with human intuition\nFinally the scagnostics should be mostly uncorrelated. If two measures are highly correlated they are probably identifying a similar visual structure and we could do without one of the scagnostics.\nOf course these are not all 100% achievable, but these assessments should be kept in mind when adjusting and creating scagnostics."
  },
  {
    "objectID": "talks/rladies/index.html#structure",
    "href": "talks/rladies/index.html#structure",
    "title": "R-Ladies Meet-up",
    "section": "Structure",
    "text": "Structure\n\nUnder the Hood Scagnostic Functions\n\nScree object\nScagnostic calculations\n\nSummary Functions\n\nWide and long data scagnostic summary\nTop Summary\n\nDraw Functions\n\nMST, Convex Hull and Alpha hull\n\nData\n\nDatasaurus dozen/ Anscombe Quartet\nFeatures\n\n\n\n\nThe packages has been written so you can easily incorperate calculating scagnostics into a tidy data workflow\nThe functions that calculate the scagnostics themselves are accessible and can be used in isolation, but\nThe summary functions are how most people will use the cassowaryr package.\nThere are the two main scagnostic summary functions, one for long data and one for wide data.\nand also two further summary functions of that summary\none that finds the highest value scatter plot for each scagnostic and another that finds the highest scoring scagnostic for each scatter plot.\nThe use of these will all be shown in the example.\nThe draw functions are mostly a debugging tool, they are designed to help you see the graph based objects so you can better understand the outputs of the package.\ncassowaryr also comes with some data that you can use to test the scagnostics, the most important of which is the features data."
  },
  {
    "objectID": "talks/rladies/index.html#the-features-dataset",
    "href": "talks/rladies/index.html#the-features-dataset",
    "title": "R-Ladies Meet-up",
    "section": "The Features Dataset",
    "text": "The Features Dataset\n\n\n\nThe features dataset is a set of scatter plots, each with a distinct feature that we want to identify.\nIt is not only important that the scagnostics can identify the features of the scatter plots but also differentiate between them\nFor example, the Ring is a hollow version of the disk scatter plot, and we want the scagnostics to be able to see that"
  },
  {
    "objectID": "talks/rladies/index.html#features-scagnostics",
    "href": "talks/rladies/index.html#features-scagnostics",
    "title": "R-Ladies Meet-up",
    "section": "Features Scagnostics",
    "text": "Features Scagnostics\n\nfeatures_scagnostics_wide <- features %>%\n  group_by(feature) %>%\n  summarise(calc_scags(x,y))\n\n# Look at the output\nfeatures_scagnostics_wide[1:5, 1:6]\n\n# A tibble: 5 × 6\n  feature  outlying stringy striated striated2 clumpy\n  <chr>       <dbl>   <dbl>    <dbl>     <dbl>  <dbl>\n1 barrier    0        0.756    0.231    0.0588  0.810\n2 clusters   0.0551   0.703    0.272    0.0828  0.802\n3 discrete   0        0.796    0.326    0.108   0.949\n4 disk       0        0.711    0.26     0.108   0.913\n5 gaps       0        0.714    0.287    0.075   0.908\n\n\n\n\nBecause the features data set is a long data set, we use group by with the calc_scags() function to get a scagnostic summary of the data set\nthe scagnostic summary can be quite large so this is a glimpse of what you would usually get."
  },
  {
    "objectID": "talks/rladies/index.html#a-scagnostic-visual-table",
    "href": "talks/rladies/index.html#a-scagnostic-visual-table",
    "title": "R-Ladies Meet-up",
    "section": "A Scagnostic Visual Table",
    "text": "A Scagnostic Visual Table\n\n\n\nTaking the full scagnostic summary we can make a visual table and have a look at what each scagnostic sees\nOn the x axis is the scagnostic value\nOn the y axis are the scagnostics\nThe points are scatter plots from the features data, each scagnostic has an example of a low value, a high value, and a moderate value, if it fits.\nif you are paying close attention you may have noticed that the scagnostics based on the MST are the ones that most freuently only have two plots, this is because distributions are very condensed\nthis occurs because all previous work in scagnostics had binning as a pre processing step, we want binning to be optional in the cassowaryr package\nwhen we removed binning and allowed for infinitely small edges in the MST, it warped a few of the scagnostics\nand so to try and fix this, we have designed some adjusted scagnostics"
  },
  {
    "objectID": "talks/rladies/index.html#clumpy-adjusted",
    "href": "talks/rladies/index.html#clumpy-adjusted",
    "title": "R-Ladies Meet-up",
    "section": "Clumpy Adjusted",
    "text": "Clumpy Adjusted\n\n\n\nHere is the same visual table but we are only plotting clumpy with clumpy2 where clumpy2, an adjusted measure that does not require binning.\nyou can see clumpy2 both a better job of identifying the clusters plot as it appears relatively higher on the measure and also is more uniform from 0 to 1.\nThis measure is still being adjusted as it is quite slow, but even in its current state it performs better than the original measure without binning."
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-the-data",
    "href": "talks/rladies/index.html#aflw-example-the-data",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: The Data",
    "text": "AFLW Example: The Data\n\nAustralian Football League Women’s\nData from the 2020 Season\n68 variables, 33 of which are numeric\n528 Scatter Plots\nWhat are we expecting the scagnostics to find?\n\n\n\nWhile it is nice to know that the scagnostics work, correctly ordering scatter plots is not what they are used for, the measures need to be able to pick out interesting scatter plots from a large selection of scatter plots\nIn order to do show that they do, in fact do this, I’m going to give an example using data from the AFLW 2020 season\nThis data set is large, and has more pairs of variables than we could plot ourselves\nHopefully the scagnostics will pick out some interesting pairs of variables"
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-the-data-1",
    "href": "talks/rladies/index.html#aflw-example-the-data-1",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: The Data",
    "text": "AFLW Example: The Data\n\n# Get AFLW data\naflw <- fitzroy::fetch_player_stats(2020, comp = \"AFLW\")\n\n# Scagnostics only work on numeric measures\naflw_num <- aflw %>%\n  select_if(is.numeric)\n\n# Calculate average all games in the season\naflw_num <- aggregate(aflw_num[,5:37], \n  list(aflw$player.player.player.surname),\n  mean)\n\n# Calculate scagnostics\nAFLW_scags <- calc_scags_wide(aflw_num[,c(2:34)])\n\n\n\nSo we fetch the data using the fitzroy package, and we have to make sure we are only using numeric variables since scagnostics only work on numeric variable\nThen we use the calc_scags_wide function to calculate every scagnostic on every possible pair of variables. This one can be a bit computationally heavy so you do have to leave it for a bit.\nOnce you have this data the best way to analyse it is to look at a SPLOM of the scagnostics."
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-splom",
    "href": "talks/rladies/index.html#aflw-example-splom",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: SPLOM",
    "text": "AFLW Example: SPLOM\n\n\n\n\n\n\n\n\nUnfortunately a SPLOM of all the scagnostics won’t fit on the slides so for this example I’ve made one with only a subset\nThe best way to find interesting scatter plots is to find plots that are away from the main mass of scatter plots in this splom."
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-quiz",
    "href": "talks/rladies/index.html#aflw-example-quiz",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: Quiz!",
    "text": "AFLW Example: Quiz!\n\nWhich of these scatter plots was not identified (have a high value) by scagnostics?\n\n\nOk time for a bit of fun\nSo, we run the scagnostics on this data and it tells us how interesting each scatter plot is\n5 of these scatter plots had a high value, or a strange combination on one or more of the 11 scagnostics\none of them were just two variables I plotted against each other not really knowing what they looked like\nIll give you 10 seconds to try and guess which plot number you think was the one I picked at random before I change the slide"
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-plot-6",
    "href": "talks/rladies/index.html#aflw-example-plot-6",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: Plot 6!",
    "text": "AFLW Example: Plot 6!\n\nPlot 6!\n\n\n\n\nIt was plot 6! I picked the two variables that were the easiest to spell.\nHere is plot 6 alongside two other randomly chosen plots\nThese plots don’t have structure that is as interesting as the plots chosen with scagnostics\nI’ll show you how a couple of them were selected."
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-plot-1",
    "href": "talks/rladies/index.html#aflw-example-plot-1",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: Plot 1",
    "text": "AFLW Example: Plot 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot 1 was high on outlying and skewed\nThis means that even after removing outliers, the data was still really spread out\nThis structure is clearly visible in the scatter plot"
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-plot-2",
    "href": "talks/rladies/index.html#aflw-example-plot-2",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: Plot 2",
    "text": "AFLW Example: Plot 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis plot is really high on the association measures\nusually a plot that deviates from this big mass, in the middle has a non-linear relationship in the scatter plot\nwe dont have that here, so total posessions and disposals just have a strong linear relationships"
  },
  {
    "objectID": "talks/rladies/index.html#aflw-example-plot-5",
    "href": "talks/rladies/index.html#aflw-example-plot-5",
    "title": "R-Ladies Meet-up",
    "section": "AFLW Example: Plot 5",
    "text": "AFLW Example: Plot 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis plot is the last we will look at, and its my favourite\nit was identifiable because it was high on striated adjsuted and low on outlying\nThis plot clearly shows that almost no players do both bounces and hitouts\nhitouts are when you punch the ball when the ref throws it back in and they are done by your tall players, bounces have to be done while running so they are done by your fast players\nIn AFL these two categories seem to have no overlap, tall and fast are mutually exclusive\nThis is a fun example of what we can learn from our data"
  },
  {
    "objectID": "talks/rladies/index.html#top-summaries",
    "href": "talks/rladies/index.html#top-summaries",
    "title": "R-Ladies Meet-up",
    "section": "Top Summaries",
    "text": "Top Summaries\n\ntop_pairs()\n\n\nAFLW_pairs <- top_pairs(AFLW_scags)\nhead(AFLW_pairs)\n\n# A tibble: 6 × 4\n  Var1      Var2    scag    value\n  <fct>     <fct>   <chr>   <dbl>\n1 behinds   goals   clumpy  0.833\n2 kicks     goals   stringy 0.861\n3 kicks     behinds stringy 0.888\n4 handballs goals   stringy 0.836\n5 handballs behinds stringy 0.842\n6 handballs kicks   clumpy  0.922\n\n\n\n\nThe cassowaryr package also has two functions that summarise the scagnostic information, they are top_scags() and top_pairs().\nthe top_pairs() function gives the top scagnostic for each scatter plot,"
  },
  {
    "objectID": "talks/rladies/index.html#top-summaries-1",
    "href": "talks/rladies/index.html#top-summaries-1",
    "title": "R-Ladies Meet-up",
    "section": "Top Summaries",
    "text": "Top Summaries\n\ntop_pairs()\n\n\ntable(AFLW_pairs$scag)\n\n\n   clumpy    convex      dcor monotonic    skewed    skinny   stringy \n      330         5         2         7        12        50       122 \n\n\n\n\nsince the scagnostics are supposed to be directly comparable on a 0 to 1 scale, if one scagnostic appears a lot, it is likely identifying an underlying structure through the whole data set"
  },
  {
    "objectID": "talks/rladies/index.html#top-summaries-2",
    "href": "talks/rladies/index.html#top-summaries-2",
    "title": "R-Ladies Meet-up",
    "section": "Top Summaries",
    "text": "Top Summaries\n\ntop_scags() and top_pairs()\n\n\nAFLW_tscag <- top_scags(AFLW_scags)\nhead(AFLW_tscag)\n\n# A tibble: 6 × 4\n  Var1                       Var2                          scag       value\n  <fct>                      <fct>                         <chr>      <dbl>\n1 goalAccuracy               bounces                       clumpy     0.999\n2 metresGained               totalPossessions              clumpy2    0.767\n3 intercepts                 rebound50s                    convex    36.5  \n4 clearances.totalClearances clearances.stoppageClearances dcor       0.979\n5 clearances.totalClearances clearances.stoppageClearances monotonic  0.988\n6 disposalEfficiency         hitouts                       outlying   0.840\n\n\n\n\ntop_scags() gives you the scatter plot that had the highest value on each scagnostic\na reccuring scatter plot that is high on a lot of measures is likely a scatter plot with an interesting structure"
  },
  {
    "objectID": "talks/rladies/index.html#projection-pursuit-index",
    "href": "talks/rladies/index.html#projection-pursuit-index",
    "title": "R-Ladies Meet-up",
    "section": "Projection Pursuit Index",
    "text": "Projection Pursuit Index\n\n\n\n\n\n\n\n\nAll the methods I have been through so far have been how you will typically use scagnostics as an exploratory data method\nThis is an example of a guided tour using the convex scagnostic as a projection pursuit index.\nThe data has the features L shape on the x1 and x4 biplot, and noise on all other variables\nThe use for scagnostics as a projection pursuit index with the tour package is not in the cassowaryr package and some of the measures are not well suited to be used as projection pursuit indexes, either due to being too slow or noisey, but this is an area of future development for scagnostics."
  },
  {
    "objectID": "talks/rladies/index.html#the-future-of-the-package",
    "href": "talks/rladies/index.html#the-future-of-the-package",
    "title": "R-Ladies Meet-up",
    "section": "The Future of the Package",
    "text": "The Future of the Package\nNear\n\nGet new version on cran\nPublish paper\n\nFar\n\nHexagonal binning\nProjection pursuit indexes\n\n\n\nWhat is next for for cassowaryR package\nwe currently have a paper in the works, and now that our package is no longer completely broken, the examples work again we might be able to get it published.\nWe also want to make make some changes to the package later down the line, like I want to try and introduce hexagonal binning so the original scagnostics aren’t essentially useless.\nI also want to continue to test the scagnostics as projection pursuit indexes and maybe also have those be easy to implement in another package. I didn’t show the code for implementing that projection pursuit in this presentation, but it is very long and tedious.\neven as the author of the package is took me several weeks to get it working, so something that would be easier to impleent would be nice."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "R-Ladies Meet-up\n\n\nTeaching Computers to See Scatterplots with Scagnostics\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nHarriet Mason\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Harriet, and if you are here it is likely because I have directed you here to look at some of my work.\nThis is Public website that I use to share stuff that is for public consumption. You should find the thing you are looking for, but feel free to look at other stuff while you are here.\nThis video is layperson summary of the work I do:"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "test",
    "section": "",
    "text": "\\[\\begin{align*}\n{\\mathbf X}^\\top = \\left[\\begin{array}{rrrr}\n2 & 6 & 5 & 1\\\\\n-2 & 6 & 4 & -7\\\\\n-8 & -4 & 3 & 6 \\\\\n6 & 9 & -7 & 7 \\\\\n-7 & 6 & 8 & -1\n\\end{array}\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial9.html",
    "href": "teaching/helpsheets/etc3250tutorial9.html",
    "title": "ETC3250: Tutorial 9 Help Sheet",
    "section": "",
    "text": "Join this weeks flux using this link"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial9.html#question-1",
    "href": "teaching/helpsheets/etc3250tutorial9.html#question-1",
    "title": "ETC3250: Tutorial 9 Help Sheet",
    "section": "Question 1",
    "text": "Question 1\nShow that \\(\\mathcal{K}(\\mathbf{x_1}, \\mathbf{x_2}) = (1 + \\langle \\mathbf{x_1}, \\mathbf{x_2}\\rangle) ^2\\) is equivalent to an inner product of transformations of the original variables defined as \\(\\mathbf{y} \\in \\mathbb{R}^6\\). That is, prove:\n\\[ \\mathcal{K}(\\mathbf{x_1}, \\mathbf{x_2}) = \\langle \\psi(\\mathbf{x_1}), \\psi(\\mathbf{x_2}) \\rangle\\]\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 8 slide 9 for all the equalities you will need to answer this question.\nRemember, you are trying to show that the kernel applied to \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) should be the dot product of \\(\\mathbf{y_1}\\) and \\(\\mathbf{y_2}\\) where \\(\\mathbf{y_1}\\) and \\(\\mathbf{y_2}\\) are transformations of \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\). e.g. \\(\\mathbf{x_1}^2 = \\mathbf{y_1}\\).\n\n\n\n\n\n\n\n\n\nHint 2: How to start\n\n\n\n\n\nStarting from \\(\\mathcal{K}(\\mathbf{x_1}, \\mathbf{x_2}) = (1 + \\langle \\mathbf{x_1}, \\mathbf{x_2}\\rangle) ^2\\) you should expand the inner product using the definition on lecture 8 slide 9.\n\n\n\n\n\n\n\n\n\nHint 3: A little more help\n\n\n\n\n\nYou want to expand the inner product of \\((1 + \\langle \\mathbf{x_1}, \\mathbf{x_2}\\rangle) ^2\\) and then expand the square.\nYou want to rearrange the expression above until you get it in terms of \\(\\mathbf{y_1}\\) and \\(\\mathbf{y_2}\\). To do this, you need to recognize the pattern of the inner product applied to a function of \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\). The end of your solution should look something like this: \\[\n\\begin{align*}\n\\sum_{j=1}^p \\psi(\\mathbf{x_1})_{1j} \\psi(\\mathbf{x_2})_{2j} & = \\langle \\psi(\\mathbf{x_1}), \\psi(\\mathbf{x_2}) \\rangle\\\\\n                                     & =  \\langle\\mathbf{y_1}, \\mathbf{y_2} \\rangle\n\\end{align*}\n\\] Where you finish by defining what \\(\\mathbf{y_1}\\) and \\(\\mathbf{y_2}\\) are. Remember, \\(\\mathbf{y} \\in \\mathbb{R}^6\\) means \\(\\mathbf{y_1}\\) and \\(\\mathbf{y_2}\\) will each be a vector with six elements."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial9.html#question-2",
    "href": "teaching/helpsheets/etc3250tutorial9.html#question-2",
    "title": "ETC3250: Tutorial 9 Help Sheet",
    "section": "Question 2",
    "text": "Question 2\n\nPart A\nMake plots of each data set.\n\n\n\n\n\n\nHint 1: Plot to make\n\n\n\n\n\nA scatter plot is typically what you use when you want to display data, only have two variables, and no other information about the structure. Use geom_point and make sure to colour by the class variable.\n\n\n\n\n\nPart B\nWhat type of kernel would be appropriate for each? How many support vectors would you expect are needed to define the boundary in each case?\n\n\n\n\n\n\nHint 1: Where to go for information (type of kernal)\n\n\n\n\n\nCheck lecture 8 slide 7 has an example of a linear SVM, and lecture 8 slide 9 has an example of a non-linear classier. Make sure to to differentiate between polynomial and radial kernels for this type of question (i.e. don’t just answer linear or non-linear).\n\n\n\n\n\n\n\n\n\nHint 2: Number of support vectors\n\n\n\n\n\nThere isn’t an obvious section of the lecture slides I can direct you to for this question, although the answer is really in the fact that we are using a separating hyperplane, so you should use lecture 8 slide 4. Instead of thinking about an exact number of support vectors to start, think about the number of support vectors each data set would need relative to each other. To answer this question, I would advise you to think about your results from Question 1. How many dimensions in the original data are you splitting on in the non-linear kernal vs in the linear kernal? Is it different? How does the dimensionality of the separating hyper plane in the non-linear kernal impact the number of support vectors needed?\n\n\n\n\n\n\n\n\n\nHint 3: Getting to an exact number of points\n\n\n\n\n\nTo draw a line you only need at least two points. To draw a surface you need at least three points. Otherwise do not have a well defined line or surface. What is the minimum number of points needed to draw a separating hyper plane in the space defined by the non-linear kernal? How about the linear kernal? Of course, this does not mean you would need 2 points for a linear deicison boundary, but the logic is similar.\n\n\n\n\n\nPart C\nBreak the data into training and test.\n\n\n\n\n\n\nHint 1: Code tip\n\n\n\n\n\nWe have done this many times. Check any previous tutorial. You need to use the initial_split function and remember to use the strata option because this is a classification task.\n\n\n\n\n\nPart D\nFit the svm model. Try changing the cost parameter to explore the number of support vectors used. Choose the value that gives you the smallest number.\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nThe code to make the model is provided, so you only need to get information on the impact of the cost function. Check lecture 8 slide 10 for information on the cost parameter. Using your results from Part B, you should know the theoretical minimum for each kernel. You can try and fiddle around with cost until you get close to that threshold (you might not be able to hit it).\n\n\n\n\n\nPart E\nCan you use the parameter estimates to write out the equation of the separating hyperplane for the linear SVM model?\n\n\n\n\n\n\nHint 1: Extra help\n\n\n\n\n\nDi told you to use svm_fit1$fit@coef and svm_fit1$fit@SVindex in the formula on lecture 8 slide 6. Remember that you are trying to compute all the \\(\\beta\\) values for the SVM model (which will give you the equation for the hyperplane) and keep in mind your data was scaled.\nConsider using the apply function if you need to do a calculation multiple times.\n\n\n\n\n\nPart F\nCompute the confusion table, and the test error for each model.\n\n\n\n\n\n\nHarriet Note\n\n\n\nWe have done this many times. Check any previous tutorial or your assignments\n\n\n\n\nPart G\nWhich observations would you expect to be the support vectors? Overlay indications of the support vectors from each model to check whether the model thinks the same as you.\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lectrue 8 slide 5 has a visualisation of the typical support vectors. This question only requires you to compare this to what you expected.\n\n\n\n\n\nPart H\nWould a neural network might be able to fit the second data set? Think about what would need to be done to make a neural network architecture.\n\n\n\n\n\n\nHint 1: Tip on thinking about this question.\n\n\n\n\n\nOne possible answer to this question can be given without knowing much about neural networks. After all, the simplest boundary you can give is linear. Is there anything you can do to your data that would make the modelling problem simple enough that any model could do the classification?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial8.html",
    "href": "teaching/helpsheets/etc3250tutorial8.html",
    "title": "ETC3250: Tutorial 8 Help Sheet",
    "section": "",
    "text": "Join this weeks flux using this link"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial8.html#challenge-question",
    "href": "teaching/helpsheets/etc3250tutorial8.html#challenge-question",
    "title": "ETC3250: Tutorial 8 Help Sheet",
    "section": "Challenge Question",
    "text": "Challenge Question\nIn the penguins data, find an observation where you think various models might differ in their prediction. Try to base your choice on the structure of the various models, not from that observation being in an overlap area between class clusters. (The code like that below will help to identify observations by their row number.)\n\n\n\n\n\n\nHint 1: How to think about this question\n\n\n\n\n\nQuestions like this want you to think about what kind of structures a model can easily capture and how that structure conflicts with the structure of your data. Different models capture some structures and miss others.\nFor this question you should consider the structure of the models we have covered, i.e. tree/forest, LDA, Logistic, and Neural networks. There are many images in the lecture notes that show what the boundaries look like, e.g. this lecture slide. Think about what creates these boundaries and what quirks in your data can cause them to fail.\n\n\n\n\n\n\n\n\n\nHint 2: Assumption vs Data driven\n\n\n\n\n\nIf your model is built upon assumptions (i.e. LDA or Logistic model) you want to think about when these assumptions will fail and what it will do to your data. You can simulate some toy data to see how failled assumptions can influence your model.\nIf your model is data driven, the types of failures it might experience are a little more complicated. Here you often need to consider the algorithm that made the model. For example, trees (and to a lesser extent forests and boosted trees depending on the tuning parameters) make recursive splits on individual variables, so it can sometimes struggle with boundaries that are combinations of variables.\n\n\n\n\n\n\n\n\n\nHint 3: Extra considerations\n\n\n\n\n\nThe typical things we look for when we do exploratory data analysis are also the kind of data features that can mess with your model. When trying to understand how a model might fail you should consider things like outliers, influential observations, non-linearity, etc."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial8.html#question-1",
    "href": "teaching/helpsheets/etc3250tutorial8.html#question-1",
    "title": "ETC3250: Tutorial 8 Help Sheet",
    "section": "Question 1",
    "text": "Question 1\nFit a random forest model to a subset of the penguins data containing only Adelie and Chinstrap. Report the summaries, and which variable(s) are globally important.\n\n\n\n\n\n\nHint 1: Where to go for information (random forest model)\n\n\n\n\n\nFor a reminder on how to fit a random forest model, check lecture 5 slide 19.\n\n\n\n\n\n\n\n\n\nHint 2: Where to go for information (Random forest variable importance)\n\n\n\n\n\nVariable specifically for random forests are discussed in lecture 5 slide 23-24 while general variable importance is discussed in lecture 7 slide 4-8.\nIf you are confused about the theory behind random forest variables importance, this blog post has a quick explanation."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial8.html#question-2",
    "href": "teaching/helpsheets/etc3250tutorial8.html#question-2",
    "title": "ETC3250: Tutorial 8 Help Sheet",
    "section": "Question 2",
    "text": "Question 2\nCompute LIME, counterfactuals and SHAP for these cases: 19, 28, 37, 111, 122, 129, 281, 292, 295, 305. Report these values.\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 7 slides 11 to 21 for an explanation and example of LIME, counterfactuals, and SHAP.\n\n\n\n\n\n\n\n\n\nHint 2: Local vs global interpretability\n\n\n\n\n\nIf you are confused about local vs global interpretability (and by extension variable importance), this blog post section has a short explanation for why a model becomes less interpretable as it becomes more flexible. The post as a whole is an explanation of how LIME works.\n\n\n\nOnce you have run the code blocks in the tutorial, make sure you can read the code and output well enough to be able to identify which variables are the most important in classifying each observation."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial8.html#question-3",
    "href": "teaching/helpsheets/etc3250tutorial8.html#question-3",
    "title": "ETC3250: Tutorial 8 Help Sheet",
    "section": "Question 3",
    "text": "Question 3\nExplain what you learn about the fitted model by studying the local explainers for the selected cases. (You will want to compare the suggested variable importance of the local explainers, for an observation, and then make plots of those variables with the observation of interest marked.)\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nIt might be helpful to start by making a summary table similar to the one in lecture 7 slide 19. This will help you identify trends in\n\n\n\n\n\n\n\n\n\nHint 2: Helpful questions for interpreting these results\n\n\n\n\n\nWhen interpreting these local importances, you should consider things like: - Which of these observations misclassified? Can you work out why? - Which variables are typically the most important? Is this the same for all observations? - Are there differences between the results from LIME, Counterfactuals, and SHAP? What do you think caused this? - Where does this point appear if we visualise the model in the data space? - Is this observation in the training or test set? If it is in the text set, Where is the nearest observation from the training set?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial6.html#question-1",
    "href": "teaching/helpsheets/etc3250tutorial6.html#question-1",
    "title": "ETC3250: Tutorial 6 Help Sheet",
    "section": "Question 1",
    "text": "Question 1\n\nPart A\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 5 slide 5 to 7 for the formula for the Gini index and an example where it is used. That formula is for \\(K\\) groups and \\(m\\) buckets (i.e. areas after the split). Each bucket (one of m) has \\(\\hat{p_{mk}}\\) of the total observations in group \\(k\\). To answer this question you need to write a function that expresses the Gini index in terms of a single parameter, \\(p\\), for a single bucket (ignore the concept of a split)\n\n\n\n\n\n\n\n\n\nHint 2: Ignoring m in the formula\n\n\n\n\n\nSince we are only working in one bucket, we can ignore \\(m\\) so our Gini index for two groups in a single bucket is: \\[G = \\hat{p_{1}}(1-\\hat{p_{1}}) + \\hat{p_{2}}(1-\\hat{p_{2}})\\]\n\n\n\n\n\n\n\n\n\nHint 3: Ignoring K in the formula\n\n\n\n\n\nSince we only have two groups we can have a single parameter for \\(p\\) which we set as: \\[p= \\hat{p_{1}} = 1-\\hat{p_{2}}\\] Which gives us the Gini index \\[G = p(1-p) + (1-p)p\\] \\[G = 2p(1-p)\\]\n\n\n\n\n\n\n\n\n\nHint 4: The domain of the function\n\n\n\n\n\n\\(p\\) is a proporition or a probability, which means it is bound by the same maximum and minimum of a probability.\n\n\n\n\n\n\n\n\n\nHint 5: How to find the maximium or minimum\n\n\n\n\n\nThere are two ways you can find the maximum and minimum 1. Algebraically using the first order condition (\\(\\frac{dG}{dp}=0\\)) 2. Draw it as a function in R (generate x as a set of values over the domain and y as \\(G\\)) and look at it. You can use whichever method you feel most comfortable with.\n\n\n\n\n\nPart B\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nThe example in lecture 5 slide 7 goes through an example with a split. The main difference is that we can no longer ignore \\(m\\) and you need to combine the weighted sum of the Gini index for each bucket.\n\n\n\n\n\n\n\n\n\nHint 2: Adding M back into the formula\n\n\n\n\n\nA Gini index can only be computed for each bucket, so for buckets \\(m=L\\) and \\(m=R\\) you will have \\(G_L\\) and \\(G_R\\) that have \\(p_{L}\\) and \\(p_{R}\\) of the total data in each bucket respectively. We end up with \\[G=p_L*G_L + p_R*G_R\\]\n\n\n\n\n\n\n\n\n\nHint 3: Part A in terms of M\n\n\n\n\n\nA single bucket in the previous formula from part A would look like: \\[G_L = 2p_{L1}(1-p_{L1})\\] Where we have only included the group index to differentiate \\(p_L\\) from \\(p_{L1}\\) (they are differnt variables).\n\n\n\n\n\nPart C\n\n\n\n\n\n\nHarriet Note\n\n\n\nI would really suggest taking the time to work through this code line by line and work out what it does. A good way to test if you know what is happening is to guess the output of each line before running it and see if you are right or wrong. You have already made this as a function yourself so it should be straight forward to understand what it is doing.\n\n\n\nHint 1: Where to go for information\nCheck lecture 5 slide 10 to see what minsplit usually does. :::\n\n\n\nPart D\n\n\n\n\n\n\nHint 1: Breakdown the function\n\n\n\n\n\nThe mysplit function takes inputs x (your x variable you are splitting on), spl (a series of values in the domain of x that you will perform a split on) and cl (the class associated with that particular x value). You will set minsplit=1 so it is not of particular importance. You need to be able to write this question in terms of those inputs.\n\n\n\n\n\n\n\n\n\nHint 2: Psudo code for the function\n\n\n\n\n\nThe question broken down into psudo code essentially asks you to:\n\nLet s=c() be a vector of all possible splits.\nFor each i in s, calculate mysplit(x, spl=i, cl, minsplit)\n\nWe know that x, cl and minsplit will be exactly the same every time we calculate this function. The only thing that changes is the split.\nRemember, if you use a for loop, you need to keep the values you calculate.\n\n\n\n\n\n\n\n\n\nHint 3: How to get the split\n\n\n\n\n\nYou dont actually need to split halfway between any two values on the x variable, you can just split at each unique value to get the same outcome. Try using the function unique().\n\n\n\n\n\n\n\n\n\nHint 4: Plot\n\n\n\n\n\nYou can use a simple ggplot with x=split_values and y=mysplit_output.\n\n\n\n\n\nPart E\n\n\n\n\n\n\nHarriet Note\n\n\n\nQuite frankly, this question is very complicated. You are welcome to use the hints and attempt this yourself, however I will not blame you if you just want to look at the code in the solution. PLEASE try and rewrite the solution code in pseudo-code (i.e. plain English) to make sure you understand what it is doing.\n\n\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 5 slide 9 to 13 to see how a random forest works with more than one split.\n\n\n\n\n\n\n\n\n\nHint 2: Explaining what to do in English\n\n\n\n\n\nBasically, you are going to need to manually do what a random forest does, which is:\n\nTo find the first split you need to:\n\n\nFor each variable (individually) calculate the Gini index for every unique split of the data.\nThe best split is the one that minimises the gini index. This is the first split.\n\n\nYour data is now split into two bins (based off the first split), to get your second split you need to:\n\n\nWithin each bin, for each variable calculate the Gini index for every unique split of the data.\nThe best split is the one that minimises the gini index. This is the second split."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial6.html#question-2",
    "href": "teaching/helpsheets/etc3250tutorial6.html#question-2",
    "title": "ETC3250: Tutorial 6 Help Sheet",
    "section": "Question 2",
    "text": "Question 2\n\nPart A\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 5 slide 16 to 19 to see how the model works with an example of how to fit it on slide 19.\nDO NOT forget to do a training and test split, and example can be found on slide 9\n\n\n\n\n\nPart B\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 5 slide 19 for an example that displays the confusion matrix."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial6.html#question-3",
    "href": "teaching/helpsheets/etc3250tutorial6.html#question-3",
    "title": "ETC3250: Tutorial 6 Help Sheet",
    "section": "Question 3",
    "text": "Question 3\n\nPart A\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 5 slides 20 to 22 to understand what a vote matrix is.\n\n\n\n\n\n\n\n\n\nHint 1: Hints on what to Consider\n\n\n\n\n\nWhenever you look at a tour you should describe the shape of the data. For this particular question you should discuss: - the overall shape - if there is clustering and where it is - How this shape translates to meaning in terms of your variables and model - Why this shape might have occured\n\n\n\n\n\nPart B\n\n\n\n\n\n\nHint 2: Where to go for information\n\n\n\n\n\nCheck lecture 5 slides 23 to 24 to understand what variable importance is (and see an example where it is used).\n\n\n\n\n\n\n\n\n\nHint 2: Explaining the results\n\n\n\n\n\nTry and find meaning in what you find by explaining the data in terms of real world phenomena. You want to make a plot x=important variable and y=cause and look to see if you can see why that variable is important. What would the plot look like for a variable you think would be important?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial6.html#question-4",
    "href": "teaching/helpsheets/etc3250tutorial6.html#question-4",
    "title": "ETC3250: Tutorial 6 Help Sheet",
    "section": "Question 4",
    "text": "Question 4\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nCheck lecture 5 slide 29 to 31 to see an example that uses boosted trees."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial7.html",
    "href": "teaching/helpsheets/etc3250tutorial7.html",
    "title": "ETC3250: Tutorial 7 Help Sheet",
    "section": "",
    "text": "No flux this week! Some questions below are missing, this is because there isn’t really anything to work out for those questions."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial7.html#question-1",
    "href": "teaching/helpsheets/etc3250tutorial7.html#question-1",
    "title": "ETC3250: Tutorial 7 Help Sheet",
    "section": "Question 1",
    "text": "Question 1\n\n\n\n\n\n\nHint 1: Where to go for details on the data\n\n\n\n\n\nBefore doing an analysis you should always get a good idea of the data you are working with. There are some details on the data set here and you can also call ?dataset_fashion_mnist to get some details."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial7.html#question-2",
    "href": "teaching/helpsheets/etc3250tutorial7.html#question-2",
    "title": "ETC3250: Tutorial 7 Help Sheet",
    "section": "Question 2",
    "text": "Question 2\n\n\n\n\n\n\nHint 1: Where to go for information\n\n\n\n\n\nThere are several functions for checking the details of a data set, they include dim, glimpse, head, summarise, etc. To plot the images, you can just use ggplot. The images are 28x28 pixel\n\n\n\n\n\n\n\n\n\nHint 2: Why this is important\n\n\n\n\n\nIt is important to know what your data looks like after every transformation too. That way you understand the input and output of every function. After running those code fashion_mnist$train is a list that contrains two datasets, x and y. fashion_mnist$train$x is an 60000 x 28 x 28 dimension array. That means"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial7.html#question-4",
    "href": "teaching/helpsheets/etc3250tutorial7.html#question-4",
    "title": "ETC3250: Tutorial 7 Help Sheet",
    "section": "Question 4",
    "text": "Question 4\n\n\n\n\n\n\nHint 1: Where to go for layer information\n\n\n\n\n\nCheck lecture 6 slide 5-6 for a simple neural network structure, and check lecture 6 slide 11-12 for a look at how this structure fits into the model process\n\n\n\n\n\n\n\n\n\nHint 2: Extra hint\n\n\n\n\n\nYour final layer is related to what you are trying to predict, and your hidden layers are related to model flexibility.\n\n\n\n\n\n\n\n\n\nHint 3: Where to go for loss and optimiser information\n\n\n\n\n\nRead the help sheet for the compile function to understand these parameters. ?keras::compile.keras.engine.training.Model should take you there (there are a lot of functions called compile so using the full name is easier). Lecture 6 slide 11-12 also have some information related to this question. You can also"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial7.html#question-6",
    "href": "teaching/helpsheets/etc3250tutorial7.html#question-6",
    "title": "ETC3250: Tutorial 7 Help Sheet",
    "section": "Question 6",
    "text": "Question 6\n\n\n\n\n\n\nHint 1: Why you might get a different result\n\n\n\n\n\nTypically if you get a different result to someone else despite running identical code on an identical data set, it means the function uses randomness somewhere in the function. Think about where a neural network might use randomness and think about what function could be used to prevent this from happening."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial7.html#question-7",
    "href": "teaching/helpsheets/etc3250tutorial7.html#question-7",
    "title": "ETC3250: Tutorial 7 Help Sheet",
    "section": "Question 7",
    "text": "Question 7\n\n\n\n\n\n\nHint 1: How to read\n\n\n\n\n\nThis code will provide you with a confusion matrix. If a particular class if frequently predicted as another class, this will appear in the confusion matrix as a large number off the diagonal."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial7.html#question-9",
    "href": "teaching/helpsheets/etc3250tutorial7.html#question-9",
    "title": "ETC3250: Tutorial 7 Help Sheet",
    "section": "Question 9",
    "text": "Question 9\n\n\n\n\n\n\nHint 1: Re-explain question\n\n\n\n\n\nBy “Explain how the class structure matches any clustering” Di means you should explain if the classes are visibly separated in the PCA and UMAP plots.\nIf one of these methods can separate the classes and another cannot, what does that say about our method and/or input?\n\n\n\n\n\n\n\n\n\nHint 2: Interpreting the 9D Simplex\n\n\n\n\n\nLecture 6 slide 55 has an example of a simple case where you need to interpret a 3D simplex (three classes). This question requires"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial5.html#question-1-lda",
    "href": "teaching/helpsheets/etc3250tutorial5.html#question-1-lda",
    "title": "ETC3250: Tutorial 5 Help Sheet",
    "section": "Question 1: LDA",
    "text": "Question 1: LDA\n\nPart A\nIs the assumption of equal variance-covariance reasonable to make for this data?\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 4 slide 18 and slides 40 to 43.\n\n\n\n\n\nPart B\nFit the LDA model to the training data\n(No hint, the code is provided)\n\n\nPart C\nCompute the confusion matrices for training and test sets, and thus the error for the test set.\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nYou have computed a confusion matrix multiple times before. Check lecture 4 slide 15 for an example of using a model to get the confusion matrix for the logistic model. Think about how to get the predicted class for a LDA model.\n\n\n\n\n\n\n\n\n\nHint: Extra code hint\n\n\n\n\n\nYou should use the predict(lda_fit$fit, p_tr)$class should give you the predicted class for an LDA model. You will need to do this twice, once on your training and once on your test set.\n\n\n\n\n\nPart D\nPlot the training and test data in the discriminant space, using symbols to indicate which set. See if you can mark the misclassified cases, too.\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 4 slide 42. The code that generated the plot is not there, but you can download the lecture QMD file, find the code that made those plots, and adjust them for this data.\n\n\n\n\n\nPart E\nRe-do the plot of the discriminant space, to examine the boundary between groups. You’ll need to generate a set of random points in the domain of the data, predict their class, and projection into the discriminant space. The explore() in the classifly package can help you generate the box of random points.\n\nHint: Extra code hint\nYou should use the explore(lda_fit$fit, p_tidy_std) to generate the data and get predictions for those values. Once you have the predictions you should plot them. :::\n\n\n\nPart F\nWhat happens to the boundary, if you change the prior probabilities? And why does this happen? Change the prior probabilities to be 1.999/3, 0.001/3, 1/3 for Adelie, Chinstrap, Gentoo, respectively. Re-do the plot of the boundaries in the discriminant space.\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 4 slide 27 for a hint on how to change the prior for probability for the code. Describe what happens think about why."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial5.html#question-2-logistic-regression",
    "href": "teaching/helpsheets/etc3250tutorial5.html#question-2-logistic-regression",
    "title": "ETC3250: Tutorial 5 Help Sheet",
    "section": "Question 2: Logistic Regression",
    "text": "Question 2: Logistic Regression\n\nPart A\nFit a logistic discriminant model to the training set. (No hint, the code is in the tutorial)\n\n\nPart B\nCompute the confusion matrices for training and test sets, and thus the error for the test set.\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nYou have computed a confusion matrix multiple times before. Check lecture 4 slide 15 for an example of using a model to get the confusion matrix for the logistic model.\n\n\n\n\n\nPart C\nCheck the boundaries produced by logistic regression, and how they differ from those of LDA. Using the 2D projection produced by the LDA rule (using equal priors) predict the your set of random points using the logistic model.\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 4 slides 6 to 9 for an understanding on how logistic regression works, check lecture 4 slides 18 to 22 to understand how LDA works. The answer is in the theoretical working of these two models.\n\n\n\n\n\n\n\n\n\nHint: Extra hint\n\n\n\n\n\nThink about the dimensional of this problem. You are looking at the data in 2D space. What dimension is the model rule drawn in?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial5.html#question-3-misclassifications",
    "href": "teaching/helpsheets/etc3250tutorial5.html#question-3-misclassifications",
    "title": "ETC3250: Tutorial 5 Help Sheet",
    "section": "Question 3: Misclassifications",
    "text": "Question 3: Misclassifications\nThis question is an exploratory question so there isnt really a right or wrong question."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial5.html#question-4-math",
    "href": "teaching/helpsheets/etc3250tutorial5.html#question-4-math",
    "title": "ETC3250: Tutorial 5 Help Sheet",
    "section": "Question 4: Math",
    "text": "Question 4: Math\nWe will go through this question in the last 20 mins of class"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial4.html#question-1",
    "href": "teaching/helpsheets/etc3250tutorial4.html#question-1",
    "title": "ETC3250: Tutorial 4 Help Sheet",
    "section": "Question 1",
    "text": "Question 1\nIn the lecture, we used bootstrap to examine the significance of the coefficients for the second principal component from the womens’ track PCA. Do this computation for PC1. The question for you to answer is: Can we consider all of the coefficients to be equal?\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 3 slides 20 to 24. The code for this question is very similar to the code on slide 24.\n\n\n\n\n\n\n\n\n\nHint: Interpreting the plot\n\n\n\n\n\nWhat does the red dashed line on the plot mean?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial4.html#question-2",
    "href": "teaching/helpsheets/etc3250tutorial4.html#question-2",
    "title": "ETC3250: Tutorial 4 Help Sheet",
    "section": "Question 2",
    "text": "Question 2\nThe ggscree function in the mulgar package computes PCA on multivariate standard normal samples, to learn what the largest eigenvalue might be when there the covariance between variables is 0.\n\nPart A\nWhat is the mean and covariance matrix of a multivariate standard normal distribution?\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nThis is a pretty basic question about a multivariate normal distribution, I can only suggest checking the wiki page for the standard normal distribution and the multivariate normal distribution\n\n\n\n\n\nPart B\nSimulate a sample of 55 observations from a 7D standard multivariate normal distribution. Compute the sample mean and covariance. (Question: Why 55 observations? Why 7D?)\n\n\n\n\n\n\nHint: Functions for simulation\n\n\n\n\n\nTo simulate the data you will need to use set.seed and rmvnorm. The variables should be independent so you might find the function diag useful in generating the variance-covariance matrix.\n\n\n\n\n\n\n\n\n\nHint: Functions for computation\n\n\n\n\n\nThere are many ways to calcualte the mean and covariance, I suggest using apply to calculate the mean, where x is your data, MARGIN indicates the calculation will be done over columns, and FUN can just be mean (you dont need the brackets on the function mean). You can use the cov for the variance-covariance matrix.\n\n\n\n\n\n\n\n\n\nHint: Nudge for 55 observations and 7D\n\n\n\n\n\nWe are going to compare this simulation to our results from the track data. Consider the dimensionality of the data (for numeric variables).\n\n\n\n\n\nPart C\nCompute PCA on your sample, and note the variance of the first PC. How does this compare with variance of the first PC of the women’s track data?\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 2 slide 45 for code that computed a PCA.\n\n\n\n\n\n\n\n\n\nHint: The function options\n\n\n\n\n\nRemember, your data is simulated from a standard normal distribution, so you shouldn’t scale or center the data.\n\n\n\n\n\n\n\n\n\nHint: Comparison considerations\n\n\n\n\n\nLook at how much of the total variance of the track data is covered by PC1 and then look at how much of the total variance of your simulated data is covered by PC1. Is it a valid check to just compare these two numbers? What would be required to make sure PC1 of the track data does not just happen to be different to the PC1 of our simulated data for this particular draw?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial4.html#question-3",
    "href": "teaching/helpsheets/etc3250tutorial4.html#question-3",
    "title": "ETC3250: Tutorial 4 Help Sheet",
    "section": "Question 3",
    "text": "Question 3\nPermutation samples is used to significance assess relationships and importance of variables. Here we will use it to assess the strength of a non-linear relationship.\n\nPart A\nGenerate a sample of data that has a strong non-linear relationship but no correlation, as follows:\n\nset.seed(908)\nn <- 205\ndf <- tibble(x1 = runif(n)-0.5, x2 = x1^2 + rnorm(n)*0.01)\n\nand then use permutation to generate another 19 plots where x1 is permuted. You can do this with the nullabor package as follows:\n\nset.seed(912)\ndf_l <- lineup(null_permute('x1'), df)\n\nand make all 20 plots as follows:\n\nggplot(df_l, aes(x=x1, y=x2)) + \n  geom_point() + \n  facet_wrap(~.sample)\n\nIs the data plot recognisably different from the plots of permuted data?\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 3 slide 26 for information on permutation.\n\n\n\n\n\n\n\n\n\nHint: Things to consider\n\n\n\n\n\nConsider how permutation works and how the other 19 plots were generated. What kind of relationship should become invisible when variables are permuted. How does that translate to being able to identify your data in this lineup plot? What does that mean about your data?\n\n\n\n\n\nPart B\nRepeat this with a sample simulated with no relationship between the two variables. Can the data be distinguished from the permuted data?\n\n\n\n\n\n\nHint: Functions for simulation\n\n\n\n\n\nTo simulate the data you can basically use the code above except instead of setting x2 = x1^2 + rnorm(n)*0.01 you can remove the dependence by setting x2 = rnorm(n)*0.1.\n\n\n\n\n\n\n\n\n\nHint: Things to consider\n\n\n\n\n\nConsiderations are identical to those provided in the hint for Part A."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial4.html#question-4",
    "href": "teaching/helpsheets/etc3250tutorial4.html#question-4",
    "title": "ETC3250: Tutorial 4 Help Sheet",
    "section": "Question 4",
    "text": "Question 4\nFor the penguins data, compute 5-fold cross-validation sets, stratified by species.\n\nPart A\nList the observations in each sample, so that you can see there is no overlap.\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 3 slide 13 to 16 for information about k-fold cross validation (in this case, k=5). You can find very similar code for this section on slide 14\n\n\n\n\n\nPart B\nMake a scatterplot matrix for each fold, coloured by species. Do the samples look similar?\n\n\n\n\n\n\nHint: Consideration for making data\n\n\n\n\n\nEach of the lines in Part A should line up with the index of one segmentation of the data. Look at the code and work out what it is doing.\n\n\n\n\n\n\n\n\n\nHint: More consideration for making data\n\n\n\n\n\nWhat is the code required for p_tidy[row_index,] to work? Remember, the training set is the remaining data after the test set is removed, and you can remove data through an index by using p_tidy[-test_index,]\n\n\n\n\n\n\n\n\n\nHint: Functions and code for scatterplot matrix\n\n\n\n\n\nLook at ggscatmat for the scatterplot matrix , you can use columns to specify the numeric variables and color (American spelling) for the colouring of the points.\nYou can just copy and paste this for each scatterplot matrix you need to make.\n\n\n\n\n\n\n\n\n\nHint: More consideration for answering questions\n\n\n\n\n\nDoes your data vary between subsets? Is this variation natural?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial4.html#question-5",
    "href": "teaching/helpsheets/etc3250tutorial4.html#question-5",
    "title": "ETC3250: Tutorial 4 Help Sheet",
    "section": "Question 5",
    "text": "Question 5\nWhat was the easiest part of this tutorial to understand, and what was the hardest?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial1.html",
    "href": "teaching/helpsheets/etc3250tutorial1.html",
    "title": "ETC3250: Tutorial 1 Help Sheet",
    "section": "",
    "text": "Join this weeks flux using this link"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial1.html#exercise-1",
    "href": "teaching/helpsheets/etc3250tutorial1.html#exercise-1",
    "title": "ETC3250: Tutorial 1 Help Sheet",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nPart A\nThe following code generates an error, can you work out why?\n\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(palmerpenguins)\np_sub <- penguins |>\n  select(species, flipper_length_mm) |>\n  filter(species == \"Adelie\")\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nI suggest googling “how to debug code” if you dont know where to start. A good first step is to run the code and google your error, or run the code line by line to work out where the error is coming from. Getting good at debugging is getting good at coding, its 90% of the pain.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nRun the code ?select which should produce the help sheet for the function causing the error.\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nYou can specify which package a function should come from using :: , i.e. ggplot2::ggplot() will specify that you want to use the ggplot() function that comes from the ggplot2 package. An example of where this is useful is when you are only using one function and don’t want to load the entire package, but there are other uses for this…\n\n\n\n\n\nPart B\nUse the reprex package to create a text where the code and error are visible (for the code block in part A), and can be shared with someone that might be able to help.\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nMake sure you have the reprex package installed. If you are not familiar with the package, Google it. The first thing that should come up is the package vignette. Read the example on the first page and try to copy it.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nRun the code ?reprex to get the help sheet on this function. The examples at the bottom usually help."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial1.html#exercise-2",
    "href": "teaching/helpsheets/etc3250tutorial1.html#exercise-2",
    "title": "ETC3250: Tutorial 1 Help Sheet",
    "section": "Exercise 2",
    "text": "Exercise 2\nYour turn to write some code that generates an error. Create a reprex, and share with your tutor or neighbour, to see if they can fix the error.\n\n\n\n\n\n\nHarriet’s Comment\n\n\n\nNot every question will have hints. For this question it is because this is a creative exercise that follows naturally from the previous question. In these cases I might have a comment (or a heads up) which I will put here."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial1.html#exercise-3",
    "href": "teaching/helpsheets/etc3250tutorial1.html#exercise-3",
    "title": "ETC3250: Tutorial 1 Help Sheet",
    "section": "Exercise 3",
    "text": "Exercise 3\nFollow the guidelines here to setup python and tensorflow on your computer. Then test your installation by following the beginner tutorial.\n\n\n\n\n\n\nHarriet’s Comment\n\n\n\nThe other tutorials have found this to be a nightmare. Try with those guidelines but try to avoid other student’s pitfalls.\nTo start, make sure you have the latest versions of R and r studio.\nMac users typically have a lot of issues with installing tensorflow, especially if you have previously installed Python. If you have a Mac you should follow the Apple specific instructions here. After successfully installing it, select the correct Python environment or interpreter in RStudio. You will do this by selecting the correct interpreter in RStudio under the “Tools -> Global Options”. Also install R packages “tensorflow” and “keras” using install.packages(\"keras\", \"tensorflow\"), don’t run install_tensorflow or install_keras. Hopefully this works. If this works for you we can all thank patrick for these instructions.\nAdditionally, the labs section of the ED discussions might also already have your question, you can find it here"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial1.html#exercise-4",
    "href": "teaching/helpsheets/etc3250tutorial1.html#exercise-4",
    "title": "ETC3250: Tutorial 1 Help Sheet",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nPart A\nDownload the slides.qmd file for week 1 lecture. Use knitr::purl() to extract the R code for the class.\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nThe qmd link from the week 1 section of the iml website should let you download it\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nRun ?knitr::purl() and check what you need for the function to run.\nAs a rule of thumb, it is generally easier to move a file to your project’s working directory than to change your working directory to a specific file location. If you don’t know what an R project is, please wave me down.\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nI would also set the output option because otherwise that slides.R file will be sent somewhere you might not want.\n\n\n\n\n\nPart B\nOpen the resulting slides.R file in your RStudio file browser. What code is in the setup.R file that is sourced at the top?\n\n\nPart C\nRun the rest of the code in small chunks. Does it all work for you? Do you get any errors? Do you have any suggestions on making it easier to run or understand the code?\n\n\n\n\n\n\nHarriet’s Comment\n\n\n\nI think this is a genuine question from Di. If you have any suggestions let me know (it could even be something that another one of your units did). If your suggestions is feasible for Di to do time wise she might implement it."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial3.html#question-1",
    "href": "teaching/helpsheets/etc3250tutorial3.html#question-1",
    "title": "ETC3250: Tutorial 3 Help Sheet",
    "section": "Question 1",
    "text": "Question 1\nRandomly generate data points that are uniformly distributed in a hyper-cube of 3, 5 and 10 dimensions, with 500 points in each sample, using the cube.solid.random function of the geozoo package. What differences do we expect to see?\n\n\n\n\n\n\nCode to Generate Data\n\n\n\n\n\n\nlibrary(tourr)\nlibrary(geozoo)\nset.seed(1234)\ncube3 <- cube.solid.random(3, 500)$points\ncube5 <- cube.solid.random(5, 500)$points\ncube10 <- cube.solid.random(10, 500)$points\n\n\n\n\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 2 slide 18 for a visual of hypercubes in 2D space.\n\n\n\n\n\n\n\n\n\nHint: Questions to consider\n\n\n\n\n\n\nHow “cube like” do you expect the data to be when projected into 2D space?\nDo you expect the increasing dimensionality to affect that?\nThe number of observations remains constant as we increase the sample size, what impact will that have?\n\n\n\n\nNow visualise each set in a grand tour and describe how they differ, and whether this matched your expectations?\n\n\n\n\n\n\nCode to Generate Animation\n\n\n\n\n\nIf you are running this code inside a QMD file, please make sure you have the chunk output option set to “in console”.\n\nanimate_xy(cube3, axes=\"bottomleft\")\nanimate_xy(cube5, axes=\"bottomleft\")\nanimate_xy(cube10, axes=\"bottomleft\")"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial3.html#question-2",
    "href": "teaching/helpsheets/etc3250tutorial3.html#question-2",
    "title": "ETC3250: Tutorial 3 Help Sheet",
    "section": "Question 2",
    "text": "Question 2\nFor the data sets, c1, c3 from the mulgar package, use the grand tour to view and try to identify structure (outliers, clusters, non-linear relationships).\n\n\n\n\n\n\nCode to Generate Animation\n\n\n\n\n\n\nanimate_xy(c1)\nanimate_xy(c3)\n\n\n\n\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 2 slide 21 and 22 to get an idea of how to comment on the structures you see in tours.\n\n\n\n\n\n\n\n\n\nHint: Questions to consider\n\n\n\n\n\n\nHow many clusters are there?\nHow big are the clusters?\nHow would you describe the shape of the clusters?\nHow would you describe the overall shape of the data?\nHow does the shape change as you move through different projections?\nIs there any statistical importance to some of the shapes you notice? (e.g. if the data can be projected into a straight line, what does that mean?)"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial3.html#question-3",
    "href": "teaching/helpsheets/etc3250tutorial3.html#question-3",
    "title": "ETC3250: Tutorial 3 Help Sheet",
    "section": "Question 3",
    "text": "Question 3\nExamine 5D multivariate normal samples drawn from populations with a range of variance-covariance matrices. (You can use the mvtnorm package to do the sampling, for example.) Examine the data using a grand tour. What changes when you change the correlation from close to zero to close to 1? Can you see a difference between strong positive correlation and strong negative correlation?\n\n\n\n\n\n\nCode to Generate Animation\n\n\n\n\n\n\nlibrary(mvtnorm)\nset.seed(501)\n\ns1 <- diag(5)\ns2 <- diag(5)\ns2[3,4] <- 0.7\ns2[4,3] <- 0.7\ns3 <- s2\ns3[1,2] <- -0.7\ns3[2,1] <- -0.7\n\ns1\ns2\ns3\n\nset.seed(1234)\nd1 <- as.data.frame(rmvnorm(500, sigma = s1))\nd2 <- as.data.frame(rmvnorm(500, sigma = s2))\nd3 <- as.data.frame(rmvnorm(500, sigma = s3))\n\nanimate_xy(d1)\nanimate_xy(d2)\nanimate_xy(d3)\n\n\n\n\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 2 slide 21 and 22 to get an idea of how to comment on the structures you see in tours.\n\n\n\n\n\n\n\n\n\nHint: Questions to consider\n\n\n\n\n\n\nHow does the shape of the data change as the correlation between the variables changes?\nHow does this shape relate to the variance-covariance matrix?\nLook at the three variance-covariance matrices. Which variables have correlation in s2 and s3? What would you expect to happen when these variables are contributing to the projection?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial3.html#question-4",
    "href": "teaching/helpsheets/etc3250tutorial3.html#question-4",
    "title": "ETC3250: Tutorial 3 Help Sheet",
    "section": "Question 4",
    "text": "Question 4\nFor data sets d2 and d3 what would you expect would be the number of PCs suggested by PCA?\n\n\n\n\n\n\nHarriet’s Comment\n\n\n\nThe d2 and d3 data for this question is the same data used in the previous question (q3)\n\n\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nCheck lecture 2 slide 29 to 34 to get an idea of how PCA works.\n\n\n\n\n\n\n\n\n\nHint: Compare to d1\n\n\n\n\n\nAnswering these questions might be easier if we think about what will happen with d1, then compare d2 to d1, and then compare d3 to d2.\nWhat would the PCA on a dataset with 5 uncorrelated variables look like? How many dimensions are needed to capture the variance of these 5 variables? Does capturing the variance get easier or harder when you correlate the variables?\n\n\n\n\n\n\n\n\n\nHint: Number of PCs\n\n\n\n\n\nWe know that the PCA for d1, d2, and d3 will have 5 PCs simply because that is how many variables, but the question is moreso asking how many PCs will have “useful” information.\nFor d1, because there is no correlation, each variable needs to be captured separataly. You need one PC dimension to capture the variance in each of the 5 dimensions.\nWhen you correlate two variables, you establish that those variables contain a lot of the same information and can be summarised together. That is, correlation should reduce the number of PCs requires to summarise the information in the data set.\n\n\n\nConduct the PCA. Report the variances (eigenvalues), and cumulative proportions of total variance, make a scree plot, and the PC coefficients.\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nTo conduct the PCA, check lecture 2 slide 45 which calculates the PCA for an example. REMEMBER: All the columns in d2 and d3 are numeric with a mean of 0.\nTo report the variances, calculate how much of the variance is in the # of PCs you suggested in the previous part of this question. Lecture 2 slide 39 will give you the formula for proportion of total variance and cumulative variance.\nLecture 2 slide 47 has a scree plot but the code is not shown. Try downloading the lecture qmd file to find the code that made the scree plot.\nThe PC coeffieicnets are shown in lecture 2 slide 45. They are what is provided when you calculate the PCA.\n\n\n\n\n\n\n\n\n\nHint: Suggestions for the code you need\n\n\n\n\n\nCompute the PCA with prcomp(data)\nIf you name your prcomp, pca, you can get the proportion of variance explained for each PC using pca$sdev. Remember you total variance should be 5 due to standardisation.\nUse mulgar::ggscree(pca, q=#variables) to make your scree plot\n\n\n\nOften, the selected number of PCs are used in future work. For both d3 and d4, think about the pros and cons of using 4 PCs and 3 PCs, respectively.\n\n\n\n\n\n\nHint: An ideal solution\n\n\n\n\n\nBased on the response you gave at the begining of this question, what do you think an ideal PC for d1, d2 and d3 would look like? How do the actual PCs differ from that solution? How does this impact the number of PCs you should use in your work?"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial3.html#question-5",
    "href": "teaching/helpsheets/etc3250tutorial3.html#question-5",
    "title": "ETC3250: Tutorial 3 Help Sheet",
    "section": "Question 5",
    "text": "Question 5\nThe rates.csv data has 152 currencies relative to the USD for the period of Nov 1, 2019 through to Mar 31, 2020. Treating the dates as variables, conduct a PCA to examine how the cross-currencies vary, focusing on this subset: ARS, AUD, BRL, CAD, CHF, CNY, EUR, FJD, GBP, IDR, INR, ISK, JPY, KRW, KZT, MXN, MYR, NZD, QAR, RUB, SEK, SGD, UYU, ZAR.\n\nPart A\nStandardise the currency columns to each have mean 0 and variance 1. Explain why this is necessary prior to doing the PCA or is it? Use this data to make a time series plot overlaying all of the cross-currencies.\n\n\n\n\n\n\nCode to get data\n\n\n\n\nrates <- read_csv(\"https://raw.githubusercontent.com/numbats/iml/master/data/rates_Nov19_Mar20.csv\") |>\n  select(date, ARS, AUD, BRL, CAD, CHF, CNY, EUR, FJD, GBP, IDR, INR, ISK, JPY, KRW, KZT, MXN, MYR, NZD, QAR, RUB, SEK, SGD, UYU, ZAR)\n\n\n\n\n\n\n\n\n\nCode to standardise currency\n\n\n\n\n\n\nlibrary(plotly)\nrates_std <- rates |>\n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\nrownames(rates_std) <- rates_std$date\np <- rates_std |>\n  pivot_longer(cols=ARS:ZAR, \n               names_to = \"currency\", \n               values_to = \"rate\") |>\n  ggplot(aes(x=date, y=rate, \n             group=currency, label=currency)) +\n    geom_line() \nggplotly(p, width=400, height=300)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint: Where to go for information (about whether we need to standardise).\n\n\n\n\n\nCheck lecture 2 slide 45. Were the variables in that data set standardised before the PCA was calculated?\n\n\n\n\n\n\n\n\n\nHint: A little extra push\n\n\n\n\n\nSetting scale=TRUE means we don’t need to standarise the variables for the computation of the PCA, however you should consider what will happen to the time series plot if we don’t standardise the variables beforehand.\n\n\n\n\n\n\n\n\n\nHint: Making the time series plot\n\n\n\n\n\nA time series plot would require a ggplot() with x=date, y=rate, and group=currency. Consider using pivot_longer() to get your data in a form where it is easy to plot.\n\n\n\n\n\nPart B\nConduct a PCA. Make a scree plot, and summarise proportion of the total variance. Summarise these values and the coefficients for the first five PCs, nicely.\n\n\n\n\n\n\nNote\n\n\n\nThe following hints are identical to those in question 4 that are for the computation.\n\n\n\n\n\n\n\n\nHint: Where to go for information\n\n\n\n\n\nTo conduct the PCA, check lecture 2 slide 45 which calculates the PCA for an example. REMEMBER: All the columns in d2 and d3 are numeric with a mean of 0.\nTo report the variances, calculate how much of the variance is in the # of PCs you suggested in the previous part of this question. Lecture 2 slide 39 will give you the formula for proportion of total variance and cumulative variance.\nLecture 2 slide 47 has a scree plot but the code is not shown. Try downloading the lecture qmd file to find the code that made the scree plot.\nThe PC coeffieicnets are shown in lecture 2 slide 45. They are what is provided when you calculate the PCA.\n\n\n\n\n\n\n\n\n\nHint: Suggested functions to use\n\n\n\n\n\nCompute the PCA with prcomp(data)\nIf you name your prcomp, pca, you can get the proportion of variance explained for each PC using pca$sdev. Remember you total variance should be 5 due to standardisation.\nUse mulgar::ggscree(pca, q=#variables) to make your scree plot\n\n\n\n\n\n\n\n\n\nCode: Explicit code to do PCA and scree plot\n\n\n\n\n\n\nrates_pca <- prcomp(rates_std[,-1], scale=FALSE)\nmulgar::ggscree(rates_pca, q=24)\n\n\n\n\n\n\n\noptions(digits=2)\nsummary(rates_pca)\n\nImportance of components:\n                         PC1   PC2    PC3    PC4    PC5    PC6     PC7     PC8\nStandard deviation     4.193 1.679 1.0932 0.9531 0.7358 0.5460 0.38600 0.33484\nProportion of Variance 0.733 0.118 0.0498 0.0379 0.0226 0.0124 0.00621 0.00467\nCumulative Proportion  0.733 0.850 0.8999 0.9377 0.9603 0.9727 0.97893 0.98360\n                           PC9    PC10    PC11    PC12    PC13    PC14    PC15\nStandard deviation     0.30254 0.25669 0.25391 0.17893 0.16189 0.15184 0.14260\nProportion of Variance 0.00381 0.00275 0.00269 0.00133 0.00109 0.00096 0.00085\nCumulative Proportion  0.98741 0.99016 0.99284 0.99418 0.99527 0.99623 0.99708\n                          PC16    PC17    PC18    PC19    PC20    PC21    PC22\nStandard deviation     0.11649 0.10691 0.09923 0.09519 0.08928 0.07987 0.07222\nProportion of Variance 0.00057 0.00048 0.00041 0.00038 0.00033 0.00027 0.00022\nCumulative Proportion  0.99764 0.99812 0.99853 0.99891 0.99924 0.99950 0.99972\n                          PC23    PC24\nStandard deviation     0.05985 0.05588\nProportion of Variance 0.00015 0.00013\nCumulative Proportion  0.99987 1.00000\n\n\n\n\n\n\n\n\n\n\n\nCode: Explicit code to do the Summary\n\n\n\n\n\n\n# Summarise the coefficients nicely\nrates_pca_smry <- tibble(evl=rates_pca$sdev^2) |>\n  mutate(p = evl/sum(evl), \n         cum_p = cumsum(evl/sum(evl))) |> \n  t() |>\n  as.data.frame()\ncolnames(rates_pca_smry) <- colnames(rates_pca$rotation)\nrates_pca_smry <- bind_rows(as.data.frame(rates_pca$rotation),\n                            rates_pca_smry)\nrownames(rates_pca_smry) <- c(rownames(rates_pca$rotation),\n                              \"Variance\", \"Proportion\", \n                              \"Cum. prop\")\nrates_pca_smry[,1:5]\n\n              PC1    PC2      PC3    PC4     PC5\nARS         0.215 -0.121  0.19832  0.181 -0.2010\nAUD         0.234  0.013  0.11466  0.018  0.0346\nBRL         0.229 -0.108  0.10513  0.093 -0.0526\nCAD         0.235 -0.025 -0.02659 -0.037  0.0337\nCHF        -0.065  0.505 -0.33521 -0.188 -0.0047\nCNY         0.144  0.237 -0.45337 -0.238 -0.5131\nEUR         0.088  0.495  0.24474  0.245 -0.1416\nFJD         0.234  0.055  0.04470  0.028  0.0330\nGBP         0.219  0.116 -0.00915 -0.073  0.3059\nIDR         0.218 -0.022 -0.24905 -0.117  0.2362\nINR         0.223 -0.147 -0.00734 -0.014  0.0279\nISK         0.230 -0.016  0.10979  0.093  0.1295\nJPY        -0.022  0.515  0.14722  0.234  0.3388\nKRW         0.214  0.063  0.17488  0.059 -0.3404\nKZT         0.217  0.013 -0.23244 -0.119  0.3304\nMXN         0.229 -0.059 -0.13804 -0.102  0.2048\nMYR         0.227  0.040 -0.13970 -0.115 -0.2009\nNZD         0.230  0.061  0.04289 -0.056 -0.0354\nQAR        -0.013  0.111  0.55283 -0.807  0.0078\nRUB         0.233 -0.102 -0.05863 -0.042  0.0063\nSEK         0.205  0.240  0.07570  0.085  0.0982\nSGD         0.227  0.057  0.14225  0.115 -0.2424\nUYU         0.231 -0.101  0.00064 -0.053  0.0957\nZAR         0.232 -0.070 -0.00328  0.042 -0.0443\nVariance   17.582  2.820  1.19502  0.908  0.5413\nProportion  0.733  0.118  0.04979  0.038  0.0226\nCum. prop   0.733  0.850  0.89989  0.938  0.9603\n\n\n\n\n\n\n\nPart C\nMake a biplot of the first two PCs. Explain what you learn.\n\n\n\n\n\n\nHint: coding suggestions\n\n\n\n\n\nLecture 2 slide 48 has a biplot but the code is not shown. Try downloading the lecture qmd file to find the code that made the scree plot.\n\n\n\n\n\n\n\n\n\nCode: Explicit code to do the biplot\n\n\n\n\n\n\nlibrary(ggfortify)\nautoplot(rates_pca, loadings = TRUE, \n         loadings.label = TRUE) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint: Questions to consider\n\n\n\n\n\n\nIs there any noticeable shape? Does that shape make sense given what you know about biplots?\nWhich variables contribute to PC1? What about PC2?\nGiven which countries contribute to each PC, is there any real world effect that you think the data is picking up on?\n\n\n\n\n\n\nPart D\nMake a time series plot of PC1 and PC2. Explain why this is useful to do for this data.\n\n\n\n\n\n\nCode: Explicit code to do the two time series plots\n\n\n\n\n\n\nrates_pca$x |>\n  as.data.frame() |>\n  mutate(date = rates_std$date) |>\n  ggplot(aes(x=date, y=PC1)) + geom_line()\n\n\n\n\n\n\n\nrates_pca$x |>\n  as.data.frame() |>\n  mutate(date = rates_std$date) |>\n  ggplot(aes(x=date, y=PC2)) + geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint: Considerations for why you might use this plot\n\n\n\n\n\n\nIs there any inherent structure in the observation in this case? How might that structure impact your interpretation of the PCs?\nDo you notice any patterns in the data when you plot them according to your PCs?\nThese values are relative to the USD, are the patterns in the data expected given the currencies that contributed to each PC?\n\n\n\n\n\n\nPart E\nYou’ll want to drill down deeper to understand what the PCA tells us about the movement of the various currencies, relative to the USD, over the volatile period of the COVID pandemic. Plot the first two PCs again, but connect the dots in order of time. Make it interactive with plotly, where the dates are the labels. What does following the dates tell us about the variation captured in the first two principal components?\n\n\n\n\n\n\nCode: Explicit code to do the interactive biplot\n\n\n\n\n\n\nlibrary(plotly)\np2 <- rates_pca$x |>\n  as.data.frame() |>\n  mutate(date = rates_std$date) |>\n  ggplot(aes(x=PC1, y=PC2, label=date)) +\n    geom_point() +\n    geom_path()\nggplotly(p2, width=400, height=400)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint: Considerations for why you might use this plot\n\n\n\n\n\n\nAre the PCs related to time? What time periods cause changes in the pattern?\nTry to think about real world events that correlate with the dates that you have in your pattern."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial3.html#question-6",
    "href": "teaching/helpsheets/etc3250tutorial3.html#question-6",
    "title": "ETC3250: Tutorial 3 Help Sheet",
    "section": "Question 6",
    "text": "Question 6\nWrite a simple question about the week’s material."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial2.html#question-1",
    "href": "teaching/helpsheets/etc3250tutorial2.html#question-1",
    "title": "ETC3250: Tutorial 2 Help Sheet",
    "section": "Question 1",
    "text": "Question 1\n\nPart A to E\n\nWhat is \\(X_1\\) (variable 1)?\nWhat is observation 3?\nWhat is \\(n\\)?\nWhat is \\(p\\)?\nWhat is \\(X^\\top\\)?\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nCheck week 1 lecture slides 13 to 18\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nThe notation section of the matricies wiki page might also help. The wiki page should also have a definition of the transpose.\n\n\n\n\n\nPart F\nWrite a projection matrix which would generate a 2D projection where the first data projection has variables 1 and 4 combined equally, and the second data projection has one third of variable 2 and two thirds of 5.\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nCheck week 1 lecture slides 19 to 24\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nI would reccomend checking this page of the Cook and Laa textbook for the requirements of a projection matrix. Remember, matricies are just a simple way to describe a system of linear equations.\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nI would suggest writing the matrix that combines the variables correctly and then make sure it follows the gneeral requirements for a projection matrix. The projected data (\\(Y\\)) should have dimension 1 equally variable 1 and variable 4, that is to say \\(Y_1 = \\frac1 2 X_1 + \\frac1 2 X_4\\) and dimension 2 should be a third variable 2 and two thirds variable 5, that is \\(Y_2 = \\frac1 3 X_2 + \\frac 2 3 X_5\\). Write these equations in matrix form to get a starting point for your answer.\n\n\n\n\n\n\n\n\n\nHint 4\n\n\n\n\n\nThe matrix equation that represents that linear system above is:\n\\[\\begin{align*}\n{\\mathbf Y = \\mathbf{XA}} =\n\\left[\\begin{array}{rrrrr}\n2 & -2 & -8 & 6 & -7 \\\\\n6 & 6 & -4 & 9 & 6 \\\\\n5 & 4 & 3 & -7 & 8 \\\\\n1 & -7 & 6 & 7 & -1\n\\end{array}\\right]\n\\left[\\begin{array}{rr}\n1/2 & 0 \\\\\n0 & 1/3  \\\\\n0 & 0 \\\\\n1/2 & 0 \\\\\n0 & 2/3 \\\\\n\\end{array}\\right]\n\\end{align*}\\]\nThis matrix has not been adjusted to fulfill the requirements of a projection matrix. If you are unsure how to convert this into a projection matrix, check the hints for Part G.\n\n\n\n\n\nPart G\nWhy can’t the following matrix considered a projection matrix?\n\\[\\begin{align*}\n{\\mathbf A} = \\left[\\begin{array}{rr}\n-1/\\sqrt{2} & 1/\\sqrt{3} \\\\\n0 & 0  \\\\\n1/\\sqrt{2} & 0 \\\\\n0 & \\sqrt{2}/\\sqrt{3} \\\\\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nHarriet’s Comment\n\n\n\nThis is not specifically a projection matrix for \\(\\mathbf X\\), so the dimensionality does not have to line up. I included the dimensionality comments in the hints below to help with f and make sure you know what is typically needed for these matricies, but the dimensionality is not the problem with this matrix.\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\n(same as Hint 1 Part F) Check week 1 lecture slides 19 to 24\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\n(same as Hint 2 Part F) I would recommend checking this page of the Cook and Laa textbook for the requirements of a projection matrix. Remember, matricies are just a simple way to describe a system of linear equations.\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nThe requirements for any projection matrix are:\n\nThe projected data should be a 2D matrix with n observations. Therefore the projection matrix has a specific dimensionality according to the rules of matrix multiplication which can be found on the wiki page.\nThe projection matrix should be orthonormal which means each column should be orthonomal with the other columns in the matrix. Remember, a vector cannot be orthogonal to itself.\n\n\n\n\n\n\n\n\n\n\nHint 4\n\n\n\n\n\nThe requirements for any projection matrix are (more specifically):\n\nSince the data is \\(n \\times p\\) and the projected data needs to be needs to be \\(n \\times p\\) the projection matrix should have the dimension \\(p \\times d\\). In this case, \\(d=2\\)\nSince \\(d=2\\) you only need to check each column is normalised (this will be two calculations) and the two columns are orthogonal to each other (this is just one calculation). A vector is normalised if it has a length of 1, i.e. \\(\\sqrt{a_{11}^2 + a_{12}^2 + ... + a_{1n}^2}=1\\). Two vectors are orthogonal to each other if their dot product is 0. That is, for columns i and j (where \\(i \\neq j\\)) they need to satisfy: \\[a_i \\cdot a_j = \\sum_k^n(a_{ik} \\times a_{jk}) = 0 \\] If you are unfamiliar with summation notation, this is just: \\[(a_{i1} \\times a_{j1}) + (a_{i2} \\times a_{j2}) + ... + (a_{in} \\times a_{jn}) = 0\\]"
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial2.html#question-2",
    "href": "teaching/helpsheets/etc3250tutorial2.html#question-2",
    "title": "ETC3250: Tutorial 2 Help Sheet",
    "section": "Question 2",
    "text": "Question 2\nWhich of these statements is the most accurate? And which is the most precise?\nA. It is almost certain to rain in the next week.\nB. It is 90% likely to get at least 10mm of rain tomorrow.\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nCheck week 1 lecture slide 26 to see a diagram that depicts the difference between accuracy and precision\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nAccuracy tells you how likely a statement is to be true, and precision tells you how specific a statement is. For example, if I guess your weight to be somewhere between 0 and 1000kg, my statement is highly accurate (true with 100% certainty) but imprecise to the point of being meaningless. Typically a more accurate statement will be less precise, and a more precise statement will be less accurate."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial2.html#question-3",
    "href": "teaching/helpsheets/etc3250tutorial2.html#question-3",
    "title": "ETC3250: Tutorial 2 Help Sheet",
    "section": "Question 3",
    "text": "Question 3\nFor the following data, make an appropriate training test split of 60:40. The response variable is cause. Demonstrate that you have made an appropriate split.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(rsample)\n\nbushfires <- read_csv(\"https://raw.githubusercontent.com/dicook/mulgar_book/pdf/data/bushfires_2019-2020.csv\")\nbushfires |> count(cause)\n\n# A tibble: 4 × 2\n  cause           n\n  <chr>       <int>\n1 accident      138\n2 arson          37\n3 burning_off     9\n4 lightning     838\n\n\n\n\n\n\n\n\nHarriet’s Comment\n\n\n\nIf you want to get the same results as the solution (and the flux quiz) use set.seed(1156)\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nCheck week 1 lecture slides 27 and 28.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nCheck the function initial_split with ?initial_split. Also look at the functions training and count to check your split and see how many are in each group.\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nYour function should use the options initial_split(data, prop=??, strata=??) to set an initial split. Then using testing(split) |> count(same variable you used for strata) to check the correct amount is in each group."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial2.html#question-4",
    "href": "teaching/helpsheets/etc3250tutorial2.html#question-4",
    "title": "ETC3250: Tutorial 2 Help Sheet",
    "section": "Question 4",
    "text": "Question 4\nIn the lecture slides from week 1 on bias vs variance, these four images were shown.\n(the images are in the tutorial, I’m not going to move them here)\nMark the images with the labels “true model”, “fitted model”, “bias”. Then explain in your own words why the different model shown in each has (potentially) large bias or small bias, and small variance or large variance.\n\n\n\n\n\n\nHarriet’s Comment\n\n\n\nThe data in the images is testing data, so the model was not trained on it. Additionally, there is no error in the data generating process, that is to say, generated data is perfectly categorised according to the true model. Keep this in mind when considering the source of the error.\nChapter 2 of the ISLR textbook is of particular use for questions that require an understanding of the bias/variance trade off, such as this one.\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nCheck week 1 lecture slides 33 to 42\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nThe black line is the true boundary (that generates the data), the colored dots show what the model has predicted. There is no noise so the wave function should perfectly categorise the observations.\nLook for areas where there is error in the model (i.e. the coloured dots diverge from the from the true model) and try to work out what is causing them. Is the error bias, variance or irreducible? Try to explain if there is any common themes in the errors in the images.\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nThe black Bias is caused by an inflexible model, variance is caused by a model that is too flexible. Error from bias will be predictable and consistent sources of error, error from variance will be inconsistent and hard to differentiate from irreduciable error. Error from bias is always visible in the images, error from variance only exists in repeated samples, however you should still mention it if you have used a highly flexible model."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial2.html#question-5",
    "href": "teaching/helpsheets/etc3250tutorial2.html#question-5",
    "title": "ETC3250: Tutorial 2 Help Sheet",
    "section": "Question 5",
    "text": "Question 5\nThe following data contains true class and predictive probabilities for a model fit. Answer the questions below for this data.\n\npred_data <- read_csv(\"https://raw.githubusercontent.com/numbats/iml/master/data/pred_data.csv\") |>\n  mutate(true = factor(true))\n\n\nPart A\nHow many classes?\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nTry using count() or table() or a similar function you are familiar with.\n\n\n\n\n\nPart B\nCompute the confusion table, using the maximum predictive probability to label the observation.\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nYou have been given data that contains the true class, the probability an observation is class Adelie and the probability an observation is class Chinstrap. You need to make a new variable that represents the predicted class and add it to your data set. Then you need to use the predicted class and the true class to make a confusion matrix.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nFirst use mutate() to add the predicted class to your data frame\nThe theory and code used to make a confusion matrix from a data set with the true and predicted values is shown on lecture 1 slides 30 to 31.\n\n\n\n\n\nPart C\nCompute the accuracy, and accuracy if all observations were classified as Adelie. Why is the accuracy almost as good when all observations are predicted to be the majority class?\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nCheck lecture 1 slides 30 to 31. It shows you how to calculate accuracy by hand and with code.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nTo calculate the accuracy if if all observations were classified as Adelie, you can either use the formula on slide 30, or add a new variable where all the predictions are Adelie and calculate it with the code on slide 31.\n\n\n\n\n\nPart D\nCompute the balanced accuracy when all observations were classified as Adelie, by averaging the class errors. Why is it lower than the overall accuracy? Which is the better accuracy to use to reflect the ability to classify this data?\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nAgain, check lecture 1 slides 30 to 31. It shows you how to calculate the balaced accuracy by hand and with code."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial2.html#question-6",
    "href": "teaching/helpsheets/etc3250tutorial2.html#question-6",
    "title": "ETC3250: Tutorial 2 Help Sheet",
    "section": "Question 6",
    "text": "Question 6\nThis question relates to feature engineering, creating better variables on which to build your model.\n\nlibrary(ggplot2)\nlibrary(ggbeeswarm)\nspam <- read_csv(\"http://ggobi.org/book/data/spam.csv\")\nggplot(spam, aes(x=spam, y=size.kb, colour=spam)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n  coord_flip() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nPart A\nThe following spam data has a heavily skewed distribution for the size of the email message. How would you transform this variable to better see differences between spam and ham emails?\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nCheck lecture 1 slides 43. You can replace the skewed variable with its transformed version, however it is better practice to leave your data as is and only transform it in the model (or in this case, in the visualisation).\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nTry adding scale_y_log10() to the ggplot that generated the visual.\n\n\n\n\n\nPart B\nFor the following data, how would you construct a new single variable which would capture the difference between the two classes using a linear model?\n\nolive <- read_csv(\"http://ggobi.org/book/data/olive.csv\") |>\n  dplyr::filter(region != 1) |>\n  dplyr::select(region, arachidic, linoleic) |>\n  mutate(region = factor(region))\nggplot(olive, aes(x=linoleic, \n                  y=arachidic, \n                  colour=region)) +\n  geom_point() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n   theme(legend.position=\"none\", \n        aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nTo get an idea of what we are trying to do, look at the example in lecture 2 slides 18 to 20. Simply guessing and checking your results is a fair way to attempt this question. The remaining hints are going to explain how to find a solution without using guess and check.\nYou can also look at the theory section of this blog post, to get different visuals for what we are going to do. You do not need to understand LDA for this question (that will be covered in later weeks) so the body of the blog can be ignored, but the drawing of the two group projection and the animation following it might help you understand what we are doing here.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nThere are two ways to approach this problem. You can either find a line that separates the groups into two mounds (similar to what was done in lecture 2), or you can find a line that can be used as a decision boundary to separate the two groups (so each group is entirely contained on one side of the line). The line that is perpendicular to that decision boundary will be a good projection to separate the groups. The line that separates the two groups is what you want to project your data onto.\nYou can either manually calculate this line by looking at the plot and finding the slope with the visualisation. You may need to make a small change to the visualisation before hand to make this a tad easier.\nWhen creating a new variable we only care about the proportions of the two variables, so the intercept of the line can be ignored. I would suggest using a plot that shows \\(y = arachidic-50\\) and \\(x = linoleic-1000\\) so you don’t have to constantly be adjusting the intercept of your line when you want to change the slope.\nUsing the plot of your data, there are two methods that can be used to find the slope (\\(m\\)) a line (this can be used reguardless of whether or not you are calculating the line that separates the two groups or the decision boundary). You can eyeball two points that should be on the line and use them to calculate \\(m = \\frac {y_2-y_1}{x_2-x_1}\\). Since the decision boundary and the separating projection are perpendicular, you can swap between the two slops using the relationship \\(m_2=\\frac{-1}{m_1}\\).\nAlternatively you can use geom_abline() from ggplot2 to plot the line with a slope of \\(m\\) and an intercept of \\(0\\) (assuming you shifted your data). Note, geom_abline() should only be used to check the decision boundary. Since the axis in the plot are scaled 10x:1y, the angle is severely warped and a right angle on the plane will not be a right angle in the data visualisation. Thereofre, the correct separating line will look very wrong when you draw it on the plot. By this same token, if you plot the decision boundary its associated 1D projection, they will not appear perpendicular.\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nOnce you have a nice separating line to project your data onto, you need to convert that to a projection vector. This is easy if you have your line in the form of \\(y=mx\\) (still assuming \\(intercept=0\\)). Projection vectors only require us to know the proportion of each variable we want in the projection. Our equation tells us the ratio \\(y:x\\) is just \\(1:m\\). If we normalise this to get a projection matrix, it will be \\(A=\\frac1{1+m}(1, m)\\). Then our new data can be expressed using the projection \\(\\mathbf Y= \\mathbf {XA} = (Linoleic, Arachidic) \\cdot (\\frac1{1+ m}, \\frac m{1+ m})\\).\nYou can add new variable in your data frame using the mutate function. To check if your projection is correct, you should plot it on a ggplot using geom_jitter() where x=new_variable and y=region. If you have a good projection, you should be able to mark draw a line on the x-axis that perfectly separates the data."
  },
  {
    "objectID": "teaching/helpsheets/etc3250tutorial2.html#question-7",
    "href": "teaching/helpsheets/etc3250tutorial2.html#question-7",
    "title": "ETC3250: Tutorial 2 Help Sheet",
    "section": "Question 7",
    "text": "Question 7\nDiscuss with your neighbour, what you found the most difficult part of last week’s content. Find some material (from resources or googling) together that gives alternative explanations that make it clearer."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Tutorial Help Sheets",
    "section": "",
    "text": "ETC3250 Tutorial Help Sheet\nI have found that students struggle to know where to start with tutorial questions. These files provide simple hints that should help to get started on questions without providing answers. I STRONGLY recommend doing the tutorial questions without this help sheet if possible to simulate an exam situation. This is a step between doing the question entirely by yourself and just watching me do the question.\n\n\n\n\n\n\nETC3250: Tutorial 1 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 2 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 3 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 4 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 5 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 6 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 7 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 8 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nETC3250: Tutorial 9 Help Sheet\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]